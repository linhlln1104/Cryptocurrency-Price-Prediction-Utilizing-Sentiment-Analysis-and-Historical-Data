{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils.models.time_series_transformer import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_objective(d_model, nhead, num_layers, dropout, learning_rate):\n",
    "    # Convert float parameters to int\n",
    "    d_model = int(d_model)\n",
    "    nhead = int(nhead)\n",
    "    num_layers = int(num_layers)\n",
    "    \n",
    "    # Ensure nhead is even\n",
    "    nhead = nhead if nhead % 2 == 0 else nhead + 1\n",
    "    \n",
    "    # Ensure d_model is even and divisible by nhead\n",
    "    d_model = ((d_model + 1) // 2) * 2  # Make d_model even\n",
    "    d_model = (d_model // nhead) * nhead\n",
    "    \n",
    "    model = TimeSeriesTransformer(input_dim=X.shape[2], d_model=d_model, nhead=nhead, \n",
    "                                  num_layers=num_layers, dropout=dropout, output_dim=prediction_window)\n",
    "    \n",
    "    dataset = TensorDataset(X, y)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, scaler_target, epochs=50, lr=learning_rate, patience=10)\n",
    "    \n",
    "    _, mae, _, _ = evaluate_model(model, val_loader, scaler_target, nn.HuberLoss())\n",
    "    \n",
    "    return -mae\n",
    "\n",
    "\n",
    "def optimize_hyperparameters():\n",
    "    pbounds = {\n",
    "        'd_model': (64, 512),\n",
    "        'nhead': (2, 16),\n",
    "        'num_layers': (1, 8),\n",
    "        'dropout': (0.1, 0.5),\n",
    "        'learning_rate': (1e-4, 1e-2)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=model_objective,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(\n",
    "        init_points=5,\n",
    "        n_iter=25,\n",
    "    )\n",
    "\n",
    "    return optimizer.max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  d_model  |  dropout  | learni... |   nhead   | num_la... |\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch [1/50], Train Loss: 0.4073, Val Loss: 0.3522, Val MAE: 0.7151, Val RMSE: 0.8806, Val SMAPE: 132.80%\n",
      "Epoch [2/50], Train Loss: 0.3218, Val Loss: 0.2150, Val MAE: 0.5316, Val RMSE: 0.6704, Val SMAPE: 126.76%\n",
      "Epoch [3/50], Train Loss: 0.2367, Val Loss: 0.1439, Val MAE: 0.4387, Val RMSE: 0.5385, Val SMAPE: 121.86%\n",
      "Epoch [4/50], Train Loss: 0.2165, Val Loss: 0.1345, Val MAE: 0.4251, Val RMSE: 0.5198, Val SMAPE: 118.57%\n",
      "Epoch [5/50], Train Loss: 0.1889, Val Loss: 0.0799, Val MAE: 0.3251, Val RMSE: 0.3987, Val SMAPE: 97.29%\n",
      "Epoch [6/50], Train Loss: 0.1567, Val Loss: 0.0682, Val MAE: 0.2913, Val RMSE: 0.3681, Val SMAPE: 91.75%\n",
      "Epoch [7/50], Train Loss: 0.1426, Val Loss: 0.0486, Val MAE: 0.2547, Val RMSE: 0.3107, Val SMAPE: 78.78%\n",
      "Epoch [8/50], Train Loss: 0.1227, Val Loss: 0.0421, Val MAE: 0.2340, Val RMSE: 0.2890, Val SMAPE: 73.10%\n",
      "Epoch [9/50], Train Loss: 0.1140, Val Loss: 0.0386, Val MAE: 0.2293, Val RMSE: 0.2769, Val SMAPE: 71.67%\n",
      "Epoch [10/50], Train Loss: 0.1069, Val Loss: 0.0379, Val MAE: 0.2213, Val RMSE: 0.2743, Val SMAPE: 70.09%\n",
      "Epoch [11/50], Train Loss: 0.0995, Val Loss: 0.0368, Val MAE: 0.2072, Val RMSE: 0.2698, Val SMAPE: 67.80%\n",
      "Epoch [12/50], Train Loss: 0.0971, Val Loss: 0.0350, Val MAE: 0.2113, Val RMSE: 0.2634, Val SMAPE: 68.32%\n",
      "Epoch [13/50], Train Loss: 0.0955, Val Loss: 0.0385, Val MAE: 0.2003, Val RMSE: 0.2756, Val SMAPE: 67.11%\n",
      "Epoch [14/50], Train Loss: 0.0898, Val Loss: 0.0374, Val MAE: 0.2063, Val RMSE: 0.2717, Val SMAPE: 68.23%\n",
      "Epoch [15/50], Train Loss: 0.0858, Val Loss: 0.0340, Val MAE: 0.2106, Val RMSE: 0.2597, Val SMAPE: 68.19%\n",
      "Epoch [16/50], Train Loss: 0.0824, Val Loss: 0.0333, Val MAE: 0.2119, Val RMSE: 0.2569, Val SMAPE: 68.59%\n",
      "Epoch [17/50], Train Loss: 0.0782, Val Loss: 0.0320, Val MAE: 0.1994, Val RMSE: 0.2513, Val SMAPE: 65.84%\n",
      "Epoch [18/50], Train Loss: 0.0743, Val Loss: 0.0300, Val MAE: 0.1974, Val RMSE: 0.2437, Val SMAPE: 65.29%\n",
      "Epoch [19/50], Train Loss: 0.0706, Val Loss: 0.0278, Val MAE: 0.1869, Val RMSE: 0.2344, Val SMAPE: 62.63%\n",
      "Epoch [20/50], Train Loss: 0.0661, Val Loss: 0.0255, Val MAE: 0.1837, Val RMSE: 0.2241, Val SMAPE: 62.05%\n",
      "Epoch [21/50], Train Loss: 0.0604, Val Loss: 0.0217, Val MAE: 0.1686, Val RMSE: 0.2066, Val SMAPE: 58.77%\n",
      "Epoch [22/50], Train Loss: 0.0562, Val Loss: 0.0170, Val MAE: 0.1396, Val RMSE: 0.1824, Val SMAPE: 50.38%\n",
      "Epoch [23/50], Train Loss: 0.0516, Val Loss: 0.0122, Val MAE: 0.1223, Val RMSE: 0.1542, Val SMAPE: 46.39%\n",
      "Epoch [24/50], Train Loss: 0.0424, Val Loss: 0.0083, Val MAE: 0.1013, Val RMSE: 0.1269, Val SMAPE: 41.83%\n",
      "Epoch [25/50], Train Loss: 0.0389, Val Loss: 0.0056, Val MAE: 0.0811, Val RMSE: 0.1040, Val SMAPE: 36.73%\n",
      "Epoch [26/50], Train Loss: 0.0358, Val Loss: 0.0045, Val MAE: 0.0711, Val RMSE: 0.0938, Val SMAPE: 38.73%\n",
      "Epoch [27/50], Train Loss: 0.0345, Val Loss: 0.0047, Val MAE: 0.0723, Val RMSE: 0.0956, Val SMAPE: 45.88%\n",
      "Epoch [28/50], Train Loss: 0.0323, Val Loss: 0.0042, Val MAE: 0.0690, Val RMSE: 0.0907, Val SMAPE: 40.72%\n",
      "Epoch [29/50], Train Loss: 0.0313, Val Loss: 0.0042, Val MAE: 0.0677, Val RMSE: 0.0909, Val SMAPE: 32.86%\n",
      "Epoch [30/50], Train Loss: 0.0311, Val Loss: 0.0045, Val MAE: 0.0724, Val RMSE: 0.0942, Val SMAPE: 33.42%\n",
      "Epoch [31/50], Train Loss: 0.0303, Val Loss: 0.0041, Val MAE: 0.0672, Val RMSE: 0.0891, Val SMAPE: 39.50%\n",
      "Epoch [32/50], Train Loss: 0.0281, Val Loss: 0.0042, Val MAE: 0.0691, Val RMSE: 0.0907, Val SMAPE: 36.28%\n",
      "Epoch [33/50], Train Loss: 0.0288, Val Loss: 0.0039, Val MAE: 0.0660, Val RMSE: 0.0872, Val SMAPE: 36.99%\n",
      "Epoch [34/50], Train Loss: 0.0274, Val Loss: 0.0036, Val MAE: 0.0631, Val RMSE: 0.0845, Val SMAPE: 33.90%\n",
      "Epoch [35/50], Train Loss: 0.0265, Val Loss: 0.0040, Val MAE: 0.0673, Val RMSE: 0.0887, Val SMAPE: 38.93%\n",
      "Epoch [36/50], Train Loss: 0.0257, Val Loss: 0.0040, Val MAE: 0.0668, Val RMSE: 0.0890, Val SMAPE: 32.34%\n",
      "Epoch [37/50], Train Loss: 0.0267, Val Loss: 0.0037, Val MAE: 0.0635, Val RMSE: 0.0851, Val SMAPE: 34.82%\n",
      "Epoch [38/50], Train Loss: 0.0253, Val Loss: 0.0041, Val MAE: 0.0669, Val RMSE: 0.0896, Val SMAPE: 32.80%\n",
      "Epoch [39/50], Train Loss: 0.0257, Val Loss: 0.0035, Val MAE: 0.0613, Val RMSE: 0.0833, Val SMAPE: 31.03%\n",
      "Epoch [40/50], Train Loss: 0.0258, Val Loss: 0.0039, Val MAE: 0.0655, Val RMSE: 0.0879, Val SMAPE: 29.31%\n",
      "Epoch [41/50], Train Loss: 0.0249, Val Loss: 0.0037, Val MAE: 0.0629, Val RMSE: 0.0847, Val SMAPE: 31.87%\n",
      "Epoch [42/50], Train Loss: 0.0251, Val Loss: 0.0033, Val MAE: 0.0597, Val RMSE: 0.0809, Val SMAPE: 31.77%\n",
      "Epoch [43/50], Train Loss: 0.0256, Val Loss: 0.0035, Val MAE: 0.0610, Val RMSE: 0.0826, Val SMAPE: 28.75%\n",
      "Epoch [44/50], Train Loss: 0.0244, Val Loss: 0.0036, Val MAE: 0.0625, Val RMSE: 0.0844, Val SMAPE: 28.73%\n",
      "Epoch [45/50], Train Loss: 0.0253, Val Loss: 0.0036, Val MAE: 0.0626, Val RMSE: 0.0844, Val SMAPE: 29.09%\n",
      "Epoch [46/50], Train Loss: 0.0245, Val Loss: 0.0036, Val MAE: 0.0623, Val RMSE: 0.0841, Val SMAPE: 29.36%\n",
      "Epoch [47/50], Train Loss: 0.0244, Val Loss: 0.0035, Val MAE: 0.0612, Val RMSE: 0.0826, Val SMAPE: 29.83%\n",
      "Epoch [48/50], Train Loss: 0.0233, Val Loss: 0.0034, Val MAE: 0.0607, Val RMSE: 0.0821, Val SMAPE: 30.01%\n",
      "Epoch [49/50], Train Loss: 0.0239, Val Loss: 0.0034, Val MAE: 0.0606, Val RMSE: 0.0819, Val SMAPE: 30.02%\n",
      "Epoch [50/50], Train Loss: 0.0244, Val Loss: 0.0034, Val MAE: 0.0605, Val RMSE: 0.0819, Val SMAPE: 30.01%\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-0.06055 \u001b[39m | \u001b[39m250.8    \u001b[39m | \u001b[39m0.3881   \u001b[39m | \u001b[39m0.0001011\u001b[39m | \u001b[39m6.233    \u001b[39m | \u001b[39m2.027    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1052, Val Loss: 0.0502, Val MAE: 0.2510, Val RMSE: 0.3170, Val SMAPE: 82.68%\n",
      "Epoch [2/50], Train Loss: 0.0564, Val Loss: 0.0464, Val MAE: 0.2410, Val RMSE: 0.3042, Val SMAPE: 77.84%\n",
      "Epoch [3/50], Train Loss: 0.0497, Val Loss: 0.0441, Val MAE: 0.2444, Val RMSE: 0.2961, Val SMAPE: 78.23%\n",
      "Epoch [4/50], Train Loss: 0.0459, Val Loss: 0.0398, Val MAE: 0.2274, Val RMSE: 0.2816, Val SMAPE: 75.61%\n",
      "Epoch [5/50], Train Loss: 0.0437, Val Loss: 0.0403, Val MAE: 0.2302, Val RMSE: 0.2835, Val SMAPE: 75.81%\n",
      "Epoch [6/50], Train Loss: 0.0421, Val Loss: 0.0410, Val MAE: 0.2484, Val RMSE: 0.2852, Val SMAPE: 78.77%\n",
      "Epoch [7/50], Train Loss: 0.0390, Val Loss: 0.0267, Val MAE: 0.1669, Val RMSE: 0.2327, Val SMAPE: 75.23%\n",
      "Epoch [8/50], Train Loss: 0.0155, Val Loss: 0.0061, Val MAE: 0.0892, Val RMSE: 0.1104, Val SMAPE: 42.71%\n",
      "Epoch [9/50], Train Loss: 0.0098, Val Loss: 0.0036, Val MAE: 0.0676, Val RMSE: 0.0857, Val SMAPE: 34.98%\n",
      "Epoch [10/50], Train Loss: 0.0069, Val Loss: 0.0031, Val MAE: 0.0597, Val RMSE: 0.0790, Val SMAPE: 31.41%\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0020, Val MAE: 0.0490, Val RMSE: 0.0640, Val SMAPE: 27.14%\n",
      "Epoch [12/50], Train Loss: 0.0063, Val Loss: 0.0065, Val MAE: 0.0971, Val RMSE: 0.1152, Val SMAPE: 74.30%\n",
      "Epoch [13/50], Train Loss: 0.0073, Val Loss: 0.0080, Val MAE: 0.1085, Val RMSE: 0.1275, Val SMAPE: 79.85%\n",
      "Epoch [14/50], Train Loss: 0.0062, Val Loss: 0.0025, Val MAE: 0.0541, Val RMSE: 0.0711, Val SMAPE: 31.91%\n",
      "Epoch [15/50], Train Loss: 0.0049, Val Loss: 0.0020, Val MAE: 0.0507, Val RMSE: 0.0641, Val SMAPE: 29.95%\n",
      "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0039, Val MAE: 0.0745, Val RMSE: 0.0879, Val SMAPE: 69.52%\n",
      "Epoch [17/50], Train Loss: 0.0040, Val Loss: 0.0019, Val MAE: 0.0484, Val RMSE: 0.0625, Val SMAPE: 28.34%\n",
      "Epoch [18/50], Train Loss: 0.0032, Val Loss: 0.0027, Val MAE: 0.0578, Val RMSE: 0.0741, Val SMAPE: 30.54%\n",
      "Epoch [19/50], Train Loss: 0.0033, Val Loss: 0.0040, Val MAE: 0.0745, Val RMSE: 0.0907, Val SMAPE: 52.98%\n",
      "Epoch [20/50], Train Loss: 0.0031, Val Loss: 0.0016, Val MAE: 0.0424, Val RMSE: 0.0565, Val SMAPE: 25.15%\n",
      "Epoch [21/50], Train Loss: 0.0030, Val Loss: 0.0014, Val MAE: 0.0401, Val RMSE: 0.0524, Val SMAPE: 23.31%\n",
      "Epoch [22/50], Train Loss: 0.0025, Val Loss: 0.0016, Val MAE: 0.0441, Val RMSE: 0.0572, Val SMAPE: 31.10%\n",
      "Epoch [23/50], Train Loss: 0.0027, Val Loss: 0.0014, Val MAE: 0.0409, Val RMSE: 0.0536, Val SMAPE: 23.85%\n",
      "Epoch [24/50], Train Loss: 0.0024, Val Loss: 0.0013, Val MAE: 0.0392, Val RMSE: 0.0514, Val SMAPE: 22.44%\n",
      "Epoch [25/50], Train Loss: 0.0023, Val Loss: 0.0016, Val MAE: 0.0428, Val RMSE: 0.0571, Val SMAPE: 23.58%\n",
      "Epoch [26/50], Train Loss: 0.0022, Val Loss: 0.0017, Val MAE: 0.0444, Val RMSE: 0.0590, Val SMAPE: 26.25%\n",
      "Epoch [27/50], Train Loss: 0.0023, Val Loss: 0.0014, Val MAE: 0.0397, Val RMSE: 0.0522, Val SMAPE: 23.53%\n",
      "Epoch [28/50], Train Loss: 0.0022, Val Loss: 0.0016, Val MAE: 0.0458, Val RMSE: 0.0565, Val SMAPE: 36.94%\n",
      "Epoch [29/50], Train Loss: 0.0022, Val Loss: 0.0016, Val MAE: 0.0454, Val RMSE: 0.0568, Val SMAPE: 37.77%\n",
      "Epoch [30/50], Train Loss: 0.0022, Val Loss: 0.0019, Val MAE: 0.0487, Val RMSE: 0.0612, Val SMAPE: 39.57%\n",
      "Epoch [31/50], Train Loss: 0.0020, Val Loss: 0.0014, Val MAE: 0.0393, Val RMSE: 0.0520, Val SMAPE: 24.19%\n",
      "Epoch [32/50], Train Loss: 0.0019, Val Loss: 0.0013, Val MAE: 0.0391, Val RMSE: 0.0513, Val SMAPE: 21.93%\n",
      "Epoch [33/50], Train Loss: 0.0020, Val Loss: 0.0012, Val MAE: 0.0369, Val RMSE: 0.0492, Val SMAPE: 21.60%\n",
      "Epoch [34/50], Train Loss: 0.0018, Val Loss: 0.0014, Val MAE: 0.0400, Val RMSE: 0.0521, Val SMAPE: 21.50%\n",
      "Epoch [35/50], Train Loss: 0.0018, Val Loss: 0.0012, Val MAE: 0.0368, Val RMSE: 0.0491, Val SMAPE: 21.24%\n",
      "Epoch [36/50], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0378, Val RMSE: 0.0504, Val SMAPE: 22.32%\n",
      "Epoch [37/50], Train Loss: 0.0018, Val Loss: 0.0012, Val MAE: 0.0372, Val RMSE: 0.0483, Val SMAPE: 22.97%\n",
      "Epoch [38/50], Train Loss: 0.0018, Val Loss: 0.0014, Val MAE: 0.0410, Val RMSE: 0.0534, Val SMAPE: 29.18%\n",
      "Epoch [39/50], Train Loss: 0.0018, Val Loss: 0.0012, Val MAE: 0.0357, Val RMSE: 0.0481, Val SMAPE: 20.80%\n",
      "Epoch [40/50], Train Loss: 0.0018, Val Loss: 0.0014, Val MAE: 0.0410, Val RMSE: 0.0529, Val SMAPE: 30.20%\n",
      "Epoch [41/50], Train Loss: 0.0018, Val Loss: 0.0012, Val MAE: 0.0368, Val RMSE: 0.0489, Val SMAPE: 22.62%\n",
      "Epoch [42/50], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0375, Val RMSE: 0.0502, Val SMAPE: 20.69%\n",
      "Epoch [43/50], Train Loss: 0.0017, Val Loss: 0.0014, Val MAE: 0.0407, Val RMSE: 0.0528, Val SMAPE: 28.94%\n",
      "Epoch [44/50], Train Loss: 0.0017, Val Loss: 0.0011, Val MAE: 0.0358, Val RMSE: 0.0476, Val SMAPE: 20.19%\n",
      "Epoch [45/50], Train Loss: 0.0017, Val Loss: 0.0012, Val MAE: 0.0371, Val RMSE: 0.0494, Val SMAPE: 23.14%\n",
      "Epoch [46/50], Train Loss: 0.0016, Val Loss: 0.0012, Val MAE: 0.0375, Val RMSE: 0.0495, Val SMAPE: 23.91%\n",
      "Epoch [47/50], Train Loss: 0.0017, Val Loss: 0.0012, Val MAE: 0.0367, Val RMSE: 0.0487, Val SMAPE: 22.20%\n",
      "Epoch [48/50], Train Loss: 0.0016, Val Loss: 0.0012, Val MAE: 0.0367, Val RMSE: 0.0487, Val SMAPE: 22.33%\n",
      "Epoch [49/50], Train Loss: 0.0016, Val Loss: 0.0012, Val MAE: 0.0367, Val RMSE: 0.0487, Val SMAPE: 22.39%\n",
      "Epoch [50/50], Train Loss: 0.0016, Val Loss: 0.0012, Val MAE: 0.0367, Val RMSE: 0.0487, Val SMAPE: 22.38%\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m-0.03673 \u001b[39m | \u001b[35m105.4    \u001b[39m | \u001b[35m0.1745   \u001b[39m | \u001b[35m0.003521 \u001b[39m | \u001b[35m7.555    \u001b[39m | \u001b[35m4.772    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.2335, Val Loss: 0.0872, Val MAE: 0.3275, Val RMSE: 0.4192, Val SMAPE: 94.29%\n",
      "Epoch [2/50], Train Loss: 0.1269, Val Loss: 0.0564, Val MAE: 0.2658, Val RMSE: 0.3371, Val SMAPE: 90.99%\n",
      "Epoch [3/50], Train Loss: 0.0993, Val Loss: 0.0403, Val MAE: 0.2387, Val RMSE: 0.2845, Val SMAPE: 77.01%\n",
      "Epoch [4/50], Train Loss: 0.0773, Val Loss: 0.0463, Val MAE: 0.2565, Val RMSE: 0.3049, Val SMAPE: 77.83%\n",
      "Epoch [5/50], Train Loss: 0.0639, Val Loss: 0.0190, Val MAE: 0.1509, Val RMSE: 0.1951, Val SMAPE: 60.32%\n",
      "Epoch [6/50], Train Loss: 0.0352, Val Loss: 0.0102, Val MAE: 0.1124, Val RMSE: 0.1431, Val SMAPE: 66.63%\n",
      "Epoch [7/50], Train Loss: 0.0231, Val Loss: 0.0058, Val MAE: 0.0837, Val RMSE: 0.1077, Val SMAPE: 46.14%\n",
      "Epoch [8/50], Train Loss: 0.0176, Val Loss: 0.0053, Val MAE: 0.0830, Val RMSE: 0.1025, Val SMAPE: 52.08%\n",
      "Epoch [9/50], Train Loss: 0.0160, Val Loss: 0.0067, Val MAE: 0.0915, Val RMSE: 0.1160, Val SMAPE: 53.84%\n",
      "Epoch [10/50], Train Loss: 0.0124, Val Loss: 0.0040, Val MAE: 0.0689, Val RMSE: 0.0889, Val SMAPE: 47.73%\n",
      "Epoch [11/50], Train Loss: 0.0090, Val Loss: 0.0050, Val MAE: 0.0798, Val RMSE: 0.1001, Val SMAPE: 45.67%\n",
      "Epoch [12/50], Train Loss: 0.0100, Val Loss: 0.0058, Val MAE: 0.0894, Val RMSE: 0.1073, Val SMAPE: 57.92%\n",
      "Epoch [13/50], Train Loss: 0.0089, Val Loss: 0.0051, Val MAE: 0.0822, Val RMSE: 0.1012, Val SMAPE: 49.23%\n",
      "Epoch [14/50], Train Loss: 0.0068, Val Loss: 0.0052, Val MAE: 0.0814, Val RMSE: 0.1016, Val SMAPE: 43.05%\n",
      "Epoch [15/50], Train Loss: 0.0057, Val Loss: 0.0033, Val MAE: 0.0662, Val RMSE: 0.0816, Val SMAPE: 46.98%\n",
      "Epoch [16/50], Train Loss: 0.0070, Val Loss: 0.0047, Val MAE: 0.0743, Val RMSE: 0.0969, Val SMAPE: 49.05%\n",
      "Epoch [17/50], Train Loss: 0.0058, Val Loss: 0.0028, Val MAE: 0.0597, Val RMSE: 0.0750, Val SMAPE: 41.17%\n",
      "Epoch [18/50], Train Loss: 0.0053, Val Loss: 0.0037, Val MAE: 0.0675, Val RMSE: 0.0863, Val SMAPE: 38.61%\n",
      "Epoch [19/50], Train Loss: 0.0057, Val Loss: 0.0030, Val MAE: 0.0601, Val RMSE: 0.0774, Val SMAPE: 36.29%\n",
      "Epoch [20/50], Train Loss: 0.0046, Val Loss: 0.0029, Val MAE: 0.0598, Val RMSE: 0.0755, Val SMAPE: 33.74%\n",
      "Epoch [21/50], Train Loss: 0.0043, Val Loss: 0.0029, Val MAE: 0.0599, Val RMSE: 0.0763, Val SMAPE: 47.50%\n",
      "Epoch [22/50], Train Loss: 0.0038, Val Loss: 0.0023, Val MAE: 0.0548, Val RMSE: 0.0683, Val SMAPE: 33.11%\n",
      "Epoch [23/50], Train Loss: 0.0038, Val Loss: 0.0022, Val MAE: 0.0520, Val RMSE: 0.0659, Val SMAPE: 37.25%\n",
      "Epoch [24/50], Train Loss: 0.0035, Val Loss: 0.0031, Val MAE: 0.0617, Val RMSE: 0.0786, Val SMAPE: 45.67%\n",
      "Epoch [25/50], Train Loss: 0.0034, Val Loss: 0.0022, Val MAE: 0.0534, Val RMSE: 0.0670, Val SMAPE: 29.84%\n",
      "Epoch [26/50], Train Loss: 0.0030, Val Loss: 0.0018, Val MAE: 0.0463, Val RMSE: 0.0596, Val SMAPE: 26.98%\n",
      "Epoch [27/50], Train Loss: 0.0026, Val Loss: 0.0018, Val MAE: 0.0461, Val RMSE: 0.0593, Val SMAPE: 28.85%\n",
      "Epoch [28/50], Train Loss: 0.0027, Val Loss: 0.0021, Val MAE: 0.0510, Val RMSE: 0.0645, Val SMAPE: 37.70%\n",
      "Epoch [29/50], Train Loss: 0.0028, Val Loss: 0.0018, Val MAE: 0.0482, Val RMSE: 0.0604, Val SMAPE: 33.68%\n",
      "Epoch [30/50], Train Loss: 0.0025, Val Loss: 0.0017, Val MAE: 0.0454, Val RMSE: 0.0587, Val SMAPE: 31.17%\n",
      "Epoch [31/50], Train Loss: 0.0025, Val Loss: 0.0015, Val MAE: 0.0435, Val RMSE: 0.0552, Val SMAPE: 29.17%\n",
      "Epoch [32/50], Train Loss: 0.0022, Val Loss: 0.0018, Val MAE: 0.0466, Val RMSE: 0.0597, Val SMAPE: 25.53%\n",
      "Epoch [33/50], Train Loss: 0.0024, Val Loss: 0.0024, Val MAE: 0.0550, Val RMSE: 0.0693, Val SMAPE: 28.49%\n",
      "Epoch [34/50], Train Loss: 0.0026, Val Loss: 0.0017, Val MAE: 0.0448, Val RMSE: 0.0577, Val SMAPE: 30.03%\n",
      "Epoch [35/50], Train Loss: 0.0022, Val Loss: 0.0014, Val MAE: 0.0423, Val RMSE: 0.0538, Val SMAPE: 26.62%\n",
      "Epoch [36/50], Train Loss: 0.0021, Val Loss: 0.0015, Val MAE: 0.0433, Val RMSE: 0.0553, Val SMAPE: 24.86%\n",
      "Epoch [37/50], Train Loss: 0.0021, Val Loss: 0.0014, Val MAE: 0.0412, Val RMSE: 0.0530, Val SMAPE: 25.11%\n",
      "Epoch [38/50], Train Loss: 0.0021, Val Loss: 0.0015, Val MAE: 0.0425, Val RMSE: 0.0544, Val SMAPE: 28.52%\n",
      "Epoch [39/50], Train Loss: 0.0021, Val Loss: 0.0016, Val MAE: 0.0441, Val RMSE: 0.0573, Val SMAPE: 30.86%\n",
      "Epoch [40/50], Train Loss: 0.0022, Val Loss: 0.0014, Val MAE: 0.0409, Val RMSE: 0.0538, Val SMAPE: 27.53%\n",
      "Epoch [41/50], Train Loss: 0.0021, Val Loss: 0.0013, Val MAE: 0.0401, Val RMSE: 0.0519, Val SMAPE: 24.11%\n",
      "Epoch [42/50], Train Loss: 0.0019, Val Loss: 0.0014, Val MAE: 0.0407, Val RMSE: 0.0527, Val SMAPE: 23.02%\n",
      "Epoch [43/50], Train Loss: 0.0019, Val Loss: 0.0014, Val MAE: 0.0413, Val RMSE: 0.0532, Val SMAPE: 23.75%\n",
      "Epoch [44/50], Train Loss: 0.0018, Val Loss: 0.0014, Val MAE: 0.0404, Val RMSE: 0.0521, Val SMAPE: 23.67%\n",
      "Epoch [45/50], Train Loss: 0.0018, Val Loss: 0.0013, Val MAE: 0.0400, Val RMSE: 0.0517, Val SMAPE: 23.41%\n",
      "Epoch [46/50], Train Loss: 0.0018, Val Loss: 0.0013, Val MAE: 0.0399, Val RMSE: 0.0517, Val SMAPE: 23.25%\n",
      "Epoch [47/50], Train Loss: 0.0018, Val Loss: 0.0013, Val MAE: 0.0399, Val RMSE: 0.0517, Val SMAPE: 23.16%\n",
      "Epoch [48/50], Train Loss: 0.0018, Val Loss: 0.0013, Val MAE: 0.0401, Val RMSE: 0.0518, Val SMAPE: 23.04%\n",
      "Epoch [49/50], Train Loss: 0.0019, Val Loss: 0.0013, Val MAE: 0.0400, Val RMSE: 0.0518, Val SMAPE: 22.95%\n",
      "Epoch [50/50], Train Loss: 0.0018, Val Loss: 0.0013, Val MAE: 0.0400, Val RMSE: 0.0517, Val SMAPE: 22.95%\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-0.04001 \u001b[39m | \u001b[39m251.8    \u001b[39m | \u001b[39m0.3741   \u001b[39m | \u001b[39m0.002124 \u001b[39m | \u001b[39m14.29    \u001b[39m | \u001b[39m1.192    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.3736, Val Loss: 0.2312, Val MAE: 0.5805, Val RMSE: 0.6949, Val SMAPE: 118.27%\n",
      "Epoch [2/50], Train Loss: 0.1705, Val Loss: 0.0474, Val MAE: 0.2405, Val RMSE: 0.3041, Val SMAPE: 79.79%\n",
      "Epoch [3/50], Train Loss: 0.1229, Val Loss: 0.0844, Val MAE: 0.3206, Val RMSE: 0.4092, Val SMAPE: 93.09%\n",
      "Epoch [4/50], Train Loss: 0.1002, Val Loss: 0.0561, Val MAE: 0.2537, Val RMSE: 0.3316, Val SMAPE: 97.71%\n",
      "Epoch [5/50], Train Loss: 0.0773, Val Loss: 0.0276, Val MAE: 0.1919, Val RMSE: 0.2346, Val SMAPE: 65.55%\n",
      "Epoch [6/50], Train Loss: 0.0524, Val Loss: 0.0212, Val MAE: 0.1778, Val RMSE: 0.2062, Val SMAPE: 77.91%\n",
      "Epoch [7/50], Train Loss: 0.0380, Val Loss: 0.0113, Val MAE: 0.1217, Val RMSE: 0.1496, Val SMAPE: 66.79%\n",
      "Epoch [8/50], Train Loss: 0.0368, Val Loss: 0.0269, Val MAE: 0.1787, Val RMSE: 0.2311, Val SMAPE: 86.27%\n",
      "Epoch [9/50], Train Loss: 0.0311, Val Loss: 0.0322, Val MAE: 0.2162, Val RMSE: 0.2531, Val SMAPE: 109.05%\n",
      "Epoch [10/50], Train Loss: 0.0314, Val Loss: 0.0113, Val MAE: 0.1257, Val RMSE: 0.1498, Val SMAPE: 59.66%\n",
      "Epoch [11/50], Train Loss: 0.0220, Val Loss: 0.0056, Val MAE: 0.0870, Val RMSE: 0.1056, Val SMAPE: 54.65%\n",
      "Epoch [12/50], Train Loss: 0.0164, Val Loss: 0.0195, Val MAE: 0.1565, Val RMSE: 0.1963, Val SMAPE: 70.05%\n",
      "Epoch [13/50], Train Loss: 0.0188, Val Loss: 0.0071, Val MAE: 0.0995, Val RMSE: 0.1189, Val SMAPE: 60.23%\n",
      "Epoch [14/50], Train Loss: 0.0127, Val Loss: 0.0084, Val MAE: 0.1002, Val RMSE: 0.1289, Val SMAPE: 42.15%\n",
      "Epoch [15/50], Train Loss: 0.0088, Val Loss: 0.0042, Val MAE: 0.0724, Val RMSE: 0.0912, Val SMAPE: 45.40%\n",
      "Epoch [16/50], Train Loss: 0.0192, Val Loss: 0.0078, Val MAE: 0.0907, Val RMSE: 0.1230, Val SMAPE: 41.08%\n",
      "Epoch [17/50], Train Loss: 0.0120, Val Loss: 0.0054, Val MAE: 0.0816, Val RMSE: 0.1032, Val SMAPE: 47.05%\n",
      "Epoch [18/50], Train Loss: 0.0067, Val Loss: 0.0055, Val MAE: 0.0819, Val RMSE: 0.1038, Val SMAPE: 35.74%\n",
      "Epoch [19/50], Train Loss: 0.0067, Val Loss: 0.0032, Val MAE: 0.0626, Val RMSE: 0.0792, Val SMAPE: 36.54%\n",
      "Epoch [20/50], Train Loss: 0.0056, Val Loss: 0.0037, Val MAE: 0.0694, Val RMSE: 0.0857, Val SMAPE: 57.23%\n",
      "Epoch [21/50], Train Loss: 0.0051, Val Loss: 0.0043, Val MAE: 0.0689, Val RMSE: 0.0915, Val SMAPE: 40.46%\n",
      "Epoch [22/50], Train Loss: 0.0046, Val Loss: 0.0054, Val MAE: 0.0889, Val RMSE: 0.1037, Val SMAPE: 79.91%\n",
      "Epoch [23/50], Train Loss: 0.0045, Val Loss: 0.0024, Val MAE: 0.0543, Val RMSE: 0.0686, Val SMAPE: 39.68%\n",
      "Epoch [24/50], Train Loss: 0.0040, Val Loss: 0.0024, Val MAE: 0.0525, Val RMSE: 0.0689, Val SMAPE: 26.92%\n",
      "Epoch [25/50], Train Loss: 0.0039, Val Loss: 0.0023, Val MAE: 0.0517, Val RMSE: 0.0670, Val SMAPE: 28.42%\n",
      "Epoch [26/50], Train Loss: 0.0042, Val Loss: 0.0036, Val MAE: 0.0648, Val RMSE: 0.0839, Val SMAPE: 30.07%\n",
      "Epoch [27/50], Train Loss: 0.0039, Val Loss: 0.0020, Val MAE: 0.0489, Val RMSE: 0.0620, Val SMAPE: 33.05%\n",
      "Epoch [28/50], Train Loss: 0.0034, Val Loss: 0.0032, Val MAE: 0.0596, Val RMSE: 0.0791, Val SMAPE: 28.66%\n",
      "Epoch [29/50], Train Loss: 0.0030, Val Loss: 0.0026, Val MAE: 0.0544, Val RMSE: 0.0708, Val SMAPE: 32.28%\n",
      "Epoch [30/50], Train Loss: 0.0031, Val Loss: 0.0032, Val MAE: 0.0584, Val RMSE: 0.0781, Val SMAPE: 35.49%\n",
      "Epoch [31/50], Train Loss: 0.0028, Val Loss: 0.0023, Val MAE: 0.0519, Val RMSE: 0.0670, Val SMAPE: 35.57%\n",
      "Epoch [32/50], Train Loss: 0.0035, Val Loss: 0.0024, Val MAE: 0.0515, Val RMSE: 0.0677, Val SMAPE: 32.27%\n",
      "Epoch [33/50], Train Loss: 0.0029, Val Loss: 0.0016, Val MAE: 0.0423, Val RMSE: 0.0561, Val SMAPE: 22.85%\n",
      "Epoch [34/50], Train Loss: 0.0029, Val Loss: 0.0036, Val MAE: 0.0641, Val RMSE: 0.0839, Val SMAPE: 27.22%\n",
      "Epoch [35/50], Train Loss: 0.0027, Val Loss: 0.0022, Val MAE: 0.0529, Val RMSE: 0.0654, Val SMAPE: 45.19%\n",
      "Epoch [36/50], Train Loss: 0.0028, Val Loss: 0.0022, Val MAE: 0.0490, Val RMSE: 0.0650, Val SMAPE: 23.91%\n",
      "Epoch [37/50], Train Loss: 0.0025, Val Loss: 0.0028, Val MAE: 0.0545, Val RMSE: 0.0742, Val SMAPE: 23.68%\n",
      "Epoch [38/50], Train Loss: 0.0023, Val Loss: 0.0019, Val MAE: 0.0462, Val RMSE: 0.0616, Val SMAPE: 23.60%\n",
      "Epoch [39/50], Train Loss: 0.0023, Val Loss: 0.0024, Val MAE: 0.0533, Val RMSE: 0.0685, Val SMAPE: 38.41%\n",
      "Epoch [40/50], Train Loss: 0.0023, Val Loss: 0.0021, Val MAE: 0.0467, Val RMSE: 0.0631, Val SMAPE: 22.56%\n",
      "Epoch [41/50], Train Loss: 0.0022, Val Loss: 0.0023, Val MAE: 0.0498, Val RMSE: 0.0662, Val SMAPE: 30.04%\n",
      "Epoch [42/50], Train Loss: 0.0022, Val Loss: 0.0020, Val MAE: 0.0479, Val RMSE: 0.0629, Val SMAPE: 32.05%\n",
      "Epoch [43/50], Train Loss: 0.0021, Val Loss: 0.0020, Val MAE: 0.0457, Val RMSE: 0.0625, Val SMAPE: 25.23%\n",
      "Early stopping triggered after 43 epochs\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-0.04568 \u001b[39m | \u001b[39m364.4    \u001b[39m | \u001b[39m0.2669   \u001b[39m | \u001b[39m0.005631 \u001b[39m | \u001b[39m3.965    \u001b[39m | \u001b[39m2.387    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.5866, Val Loss: 0.1456, Val MAE: 0.4337, Val RMSE: 0.5458, Val SMAPE: 117.56%\n",
      "Epoch [2/50], Train Loss: 0.2930, Val Loss: 0.1578, Val MAE: 0.4178, Val RMSE: 0.5771, Val SMAPE: 94.04%\n",
      "Epoch [3/50], Train Loss: 0.2299, Val Loss: 0.0746, Val MAE: 0.3139, Val RMSE: 0.3867, Val SMAPE: 85.15%\n",
      "Epoch [4/50], Train Loss: 0.2070, Val Loss: 0.0616, Val MAE: 0.3059, Val RMSE: 0.3513, Val SMAPE: 86.15%\n",
      "Epoch [5/50], Train Loss: 0.1758, Val Loss: 0.0710, Val MAE: 0.3180, Val RMSE: 0.3771, Val SMAPE: 110.68%\n",
      "Epoch [6/50], Train Loss: 0.1436, Val Loss: 0.0661, Val MAE: 0.2788, Val RMSE: 0.3636, Val SMAPE: 104.29%\n",
      "Epoch [7/50], Train Loss: 0.1346, Val Loss: 0.1038, Val MAE: 0.4002, Val RMSE: 0.4563, Val SMAPE: 95.44%\n",
      "Epoch [8/50], Train Loss: 0.1233, Val Loss: 0.1047, Val MAE: 0.4089, Val RMSE: 0.4583, Val SMAPE: 97.27%\n",
      "Epoch [9/50], Train Loss: 0.1029, Val Loss: 0.0459, Val MAE: 0.2460, Val RMSE: 0.3033, Val SMAPE: 78.26%\n",
      "Epoch [10/50], Train Loss: 0.0874, Val Loss: 0.0683, Val MAE: 0.3244, Val RMSE: 0.3701, Val SMAPE: 88.45%\n",
      "Epoch [11/50], Train Loss: 0.0864, Val Loss: 0.0386, Val MAE: 0.2240, Val RMSE: 0.2779, Val SMAPE: 75.79%\n",
      "Epoch [12/50], Train Loss: 0.0725, Val Loss: 0.0361, Val MAE: 0.2089, Val RMSE: 0.2686, Val SMAPE: 80.23%\n",
      "Epoch [13/50], Train Loss: 0.0645, Val Loss: 0.0383, Val MAE: 0.2301, Val RMSE: 0.2771, Val SMAPE: 76.38%\n",
      "Epoch [14/50], Train Loss: 0.0649, Val Loss: 0.0326, Val MAE: 0.2050, Val RMSE: 0.2555, Val SMAPE: 72.44%\n",
      "Epoch [15/50], Train Loss: 0.0560, Val Loss: 0.0332, Val MAE: 0.2203, Val RMSE: 0.2580, Val SMAPE: 75.65%\n",
      "Epoch [16/50], Train Loss: 0.0513, Val Loss: 0.0380, Val MAE: 0.2407, Val RMSE: 0.2762, Val SMAPE: 78.65%\n",
      "Epoch [17/50], Train Loss: 0.0489, Val Loss: 0.0338, Val MAE: 0.2151, Val RMSE: 0.2604, Val SMAPE: 74.35%\n",
      "Epoch [18/50], Train Loss: 0.0476, Val Loss: 0.0471, Val MAE: 0.2730, Val RMSE: 0.3073, Val SMAPE: 83.09%\n",
      "Epoch [19/50], Train Loss: 0.0470, Val Loss: 0.0379, Val MAE: 0.2359, Val RMSE: 0.2755, Val SMAPE: 77.65%\n",
      "Epoch [20/50], Train Loss: 0.0462, Val Loss: 0.0305, Val MAE: 0.1940, Val RMSE: 0.2469, Val SMAPE: 70.66%\n",
      "Epoch [21/50], Train Loss: 0.0458, Val Loss: 0.0416, Val MAE: 0.2508, Val RMSE: 0.2888, Val SMAPE: 79.83%\n",
      "Epoch [22/50], Train Loss: 0.0456, Val Loss: 0.0343, Val MAE: 0.2238, Val RMSE: 0.2621, Val SMAPE: 76.09%\n",
      "Epoch [23/50], Train Loss: 0.0442, Val Loss: 0.0342, Val MAE: 0.2248, Val RMSE: 0.2617, Val SMAPE: 76.34%\n",
      "Epoch [24/50], Train Loss: 0.0434, Val Loss: 0.0416, Val MAE: 0.2538, Val RMSE: 0.2888, Val SMAPE: 80.51%\n",
      "Epoch [25/50], Train Loss: 0.0426, Val Loss: 0.0349, Val MAE: 0.2282, Val RMSE: 0.2645, Val SMAPE: 76.84%\n",
      "Epoch [26/50], Train Loss: 0.0420, Val Loss: 0.0365, Val MAE: 0.2352, Val RMSE: 0.2706, Val SMAPE: 77.91%\n",
      "Epoch [27/50], Train Loss: 0.0416, Val Loss: 0.0360, Val MAE: 0.2334, Val RMSE: 0.2688, Val SMAPE: 77.63%\n",
      "Epoch [28/50], Train Loss: 0.0416, Val Loss: 0.0368, Val MAE: 0.2361, Val RMSE: 0.2717, Val SMAPE: 78.01%\n",
      "Epoch [29/50], Train Loss: 0.0417, Val Loss: 0.0376, Val MAE: 0.2396, Val RMSE: 0.2747, Val SMAPE: 78.54%\n",
      "Epoch [30/50], Train Loss: 0.0406, Val Loss: 0.0346, Val MAE: 0.2270, Val RMSE: 0.2634, Val SMAPE: 76.66%\n",
      "Early stopping triggered after 30 epochs\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-0.227   \u001b[39m | \u001b[39m422.7    \u001b[39m | \u001b[39m0.4873   \u001b[39m | \u001b[39m0.003203 \u001b[39m | \u001b[39m11.69    \u001b[39m | \u001b[39m7.135    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1252, Val Loss: 0.0479, Val MAE: 0.2473, Val RMSE: 0.3109, Val SMAPE: 78.25%\n",
      "Epoch [2/50], Train Loss: 0.0722, Val Loss: 0.0442, Val MAE: 0.2481, Val RMSE: 0.2983, Val SMAPE: 76.16%\n",
      "Epoch [3/50], Train Loss: 0.0627, Val Loss: 0.0413, Val MAE: 0.2377, Val RMSE: 0.2886, Val SMAPE: 75.08%\n",
      "Epoch [4/50], Train Loss: 0.0551, Val Loss: 0.0406, Val MAE: 0.2425, Val RMSE: 0.2857, Val SMAPE: 75.79%\n",
      "Epoch [5/50], Train Loss: 0.0492, Val Loss: 0.0403, Val MAE: 0.2368, Val RMSE: 0.2845, Val SMAPE: 74.26%\n",
      "Epoch [6/50], Train Loss: 0.0333, Val Loss: 0.0128, Val MAE: 0.1286, Val RMSE: 0.1600, Val SMAPE: 77.64%\n",
      "Epoch [7/50], Train Loss: 0.0149, Val Loss: 0.0063, Val MAE: 0.0824, Val RMSE: 0.1123, Val SMAPE: 38.51%\n",
      "Epoch [8/50], Train Loss: 0.0096, Val Loss: 0.0038, Val MAE: 0.0653, Val RMSE: 0.0883, Val SMAPE: 30.90%\n",
      "Epoch [9/50], Train Loss: 0.0068, Val Loss: 0.0063, Val MAE: 0.0937, Val RMSE: 0.1125, Val SMAPE: 67.13%\n",
      "Epoch [10/50], Train Loss: 0.0074, Val Loss: 0.0038, Val MAE: 0.0726, Val RMSE: 0.0874, Val SMAPE: 57.18%\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0041, Val MAE: 0.0726, Val RMSE: 0.0905, Val SMAPE: 40.19%\n",
      "Epoch [12/50], Train Loss: 0.0057, Val Loss: 0.0022, Val MAE: 0.0518, Val RMSE: 0.0668, Val SMAPE: 27.81%\n",
      "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0042, Val MAE: 0.0745, Val RMSE: 0.0922, Val SMAPE: 61.41%\n",
      "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0020, Val MAE: 0.0500, Val RMSE: 0.0625, Val SMAPE: 26.73%\n",
      "Epoch [15/50], Train Loss: 0.0039, Val Loss: 0.0038, Val MAE: 0.0709, Val RMSE: 0.0878, Val SMAPE: 31.85%\n",
      "Epoch [16/50], Train Loss: 0.0039, Val Loss: 0.0059, Val MAE: 0.0951, Val RMSE: 0.1084, Val SMAPE: 43.67%\n",
      "Epoch [17/50], Train Loss: 0.0041, Val Loss: 0.0036, Val MAE: 0.0667, Val RMSE: 0.0849, Val SMAPE: 29.07%\n",
      "Epoch [18/50], Train Loss: 0.0039, Val Loss: 0.0021, Val MAE: 0.0510, Val RMSE: 0.0649, Val SMAPE: 27.04%\n",
      "Epoch [19/50], Train Loss: 0.0028, Val Loss: 0.0018, Val MAE: 0.0448, Val RMSE: 0.0596, Val SMAPE: 24.09%\n",
      "Epoch [20/50], Train Loss: 0.0025, Val Loss: 0.0015, Val MAE: 0.0424, Val RMSE: 0.0543, Val SMAPE: 22.83%\n",
      "Epoch [21/50], Train Loss: 0.0027, Val Loss: 0.0024, Val MAE: 0.0554, Val RMSE: 0.0696, Val SMAPE: 27.06%\n",
      "Epoch [22/50], Train Loss: 0.0027, Val Loss: 0.0017, Val MAE: 0.0442, Val RMSE: 0.0578, Val SMAPE: 23.18%\n",
      "Epoch [23/50], Train Loss: 0.0023, Val Loss: 0.0022, Val MAE: 0.0529, Val RMSE: 0.0672, Val SMAPE: 25.90%\n",
      "Epoch [24/50], Train Loss: 0.0022, Val Loss: 0.0014, Val MAE: 0.0419, Val RMSE: 0.0529, Val SMAPE: 23.23%\n",
      "Epoch [25/50], Train Loss: 0.0020, Val Loss: 0.0017, Val MAE: 0.0453, Val RMSE: 0.0589, Val SMAPE: 22.70%\n",
      "Epoch [26/50], Train Loss: 0.0021, Val Loss: 0.0020, Val MAE: 0.0525, Val RMSE: 0.0632, Val SMAPE: 28.71%\n",
      "Epoch [27/50], Train Loss: 0.0021, Val Loss: 0.0017, Val MAE: 0.0455, Val RMSE: 0.0585, Val SMAPE: 22.71%\n",
      "Epoch [28/50], Train Loss: 0.0024, Val Loss: 0.0028, Val MAE: 0.0618, Val RMSE: 0.0753, Val SMAPE: 55.33%\n",
      "Epoch [29/50], Train Loss: 0.0029, Val Loss: 0.0029, Val MAE: 0.0579, Val RMSE: 0.0761, Val SMAPE: 28.96%\n",
      "Epoch [30/50], Train Loss: 0.0031, Val Loss: 0.0019, Val MAE: 0.0470, Val RMSE: 0.0622, Val SMAPE: 25.49%\n",
      "Epoch [31/50], Train Loss: 0.0025, Val Loss: 0.0031, Val MAE: 0.0669, Val RMSE: 0.0793, Val SMAPE: 33.26%\n",
      "Epoch [32/50], Train Loss: 0.0023, Val Loss: 0.0014, Val MAE: 0.0410, Val RMSE: 0.0538, Val SMAPE: 23.07%\n",
      "Epoch [33/50], Train Loss: 0.0020, Val Loss: 0.0021, Val MAE: 0.0515, Val RMSE: 0.0657, Val SMAPE: 24.74%\n",
      "Epoch [34/50], Train Loss: 0.0019, Val Loss: 0.0014, Val MAE: 0.0408, Val RMSE: 0.0524, Val SMAPE: 22.41%\n",
      "Epoch [35/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0393, Val RMSE: 0.0513, Val SMAPE: 20.91%\n",
      "Epoch [36/50], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0393, Val RMSE: 0.0506, Val SMAPE: 20.93%\n",
      "Epoch [37/50], Train Loss: 0.0016, Val Loss: 0.0014, Val MAE: 0.0413, Val RMSE: 0.0539, Val SMAPE: 21.71%\n",
      "Epoch [38/50], Train Loss: 0.0015, Val Loss: 0.0012, Val MAE: 0.0382, Val RMSE: 0.0491, Val SMAPE: 20.57%\n",
      "Epoch [39/50], Train Loss: 0.0015, Val Loss: 0.0016, Val MAE: 0.0439, Val RMSE: 0.0575, Val SMAPE: 21.11%\n",
      "Epoch [40/50], Train Loss: 0.0015, Val Loss: 0.0012, Val MAE: 0.0386, Val RMSE: 0.0496, Val SMAPE: 20.90%\n",
      "Epoch [41/50], Train Loss: 0.0015, Val Loss: 0.0013, Val MAE: 0.0387, Val RMSE: 0.0512, Val SMAPE: 20.09%\n",
      "Epoch [42/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0376, Val RMSE: 0.0504, Val SMAPE: 19.52%\n",
      "Epoch [43/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0377, Val RMSE: 0.0496, Val SMAPE: 20.28%\n",
      "Epoch [44/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0374, Val RMSE: 0.0494, Val SMAPE: 19.47%\n",
      "Epoch [45/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0373, Val RMSE: 0.0489, Val SMAPE: 19.56%\n",
      "Epoch [46/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0370, Val RMSE: 0.0484, Val SMAPE: 19.43%\n",
      "Epoch [47/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0376, Val RMSE: 0.0491, Val SMAPE: 19.62%\n",
      "Epoch [48/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0373, Val RMSE: 0.0491, Val SMAPE: 19.47%\n",
      "Epoch [49/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0371, Val RMSE: 0.0489, Val SMAPE: 19.38%\n",
      "Epoch [50/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0371, Val RMSE: 0.0488, Val SMAPE: 19.36%\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-0.03707 \u001b[39m | \u001b[39m106.1    \u001b[39m | \u001b[39m0.3701   \u001b[39m | \u001b[39m0.006919 \u001b[39m | \u001b[39m7.43     \u001b[39m | \u001b[39m3.343    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.2703, Val Loss: 0.1802, Val MAE: 0.4714, Val RMSE: 0.6107, Val SMAPE: 99.06%\n",
      "Epoch [2/50], Train Loss: 0.1182, Val Loss: 0.0359, Val MAE: 0.1953, Val RMSE: 0.2645, Val SMAPE: 71.94%\n",
      "Epoch [3/50], Train Loss: 0.0753, Val Loss: 0.0596, Val MAE: 0.2602, Val RMSE: 0.3432, Val SMAPE: 98.99%\n",
      "Epoch [4/50], Train Loss: 0.0469, Val Loss: 0.0331, Val MAE: 0.2083, Val RMSE: 0.2556, Val SMAPE: 105.61%\n",
      "Epoch [5/50], Train Loss: 0.0283, Val Loss: 0.0105, Val MAE: 0.1107, Val RMSE: 0.1441, Val SMAPE: 55.51%\n",
      "Epoch [6/50], Train Loss: 0.0179, Val Loss: 0.0078, Val MAE: 0.0967, Val RMSE: 0.1240, Val SMAPE: 55.85%\n",
      "Epoch [7/50], Train Loss: 0.0139, Val Loss: 0.0133, Val MAE: 0.1326, Val RMSE: 0.1630, Val SMAPE: 71.07%\n",
      "Epoch [8/50], Train Loss: 0.0180, Val Loss: 0.0157, Val MAE: 0.1284, Val RMSE: 0.1761, Val SMAPE: 72.42%\n",
      "Epoch [9/50], Train Loss: 0.0195, Val Loss: 0.0154, Val MAE: 0.1383, Val RMSE: 0.1747, Val SMAPE: 71.26%\n",
      "Epoch [10/50], Train Loss: 0.0213, Val Loss: 0.0277, Val MAE: 0.1852, Val RMSE: 0.2354, Val SMAPE: 67.43%\n",
      "Epoch [11/50], Train Loss: 0.1818, Val Loss: 0.1514, Val MAE: 0.4673, Val RMSE: 0.5484, Val SMAPE: 147.78%\n",
      "Epoch [12/50], Train Loss: 0.0841, Val Loss: 0.0328, Val MAE: 0.2106, Val RMSE: 0.2568, Val SMAPE: 67.11%\n",
      "Epoch [13/50], Train Loss: 0.0362, Val Loss: 0.0153, Val MAE: 0.1418, Val RMSE: 0.1739, Val SMAPE: 53.41%\n",
      "Epoch [14/50], Train Loss: 0.0213, Val Loss: 0.0080, Val MAE: 0.1005, Val RMSE: 0.1269, Val SMAPE: 51.30%\n",
      "Epoch [15/50], Train Loss: 0.0099, Val Loss: 0.0046, Val MAE: 0.0731, Val RMSE: 0.0965, Val SMAPE: 34.48%\n",
      "Epoch [16/50], Train Loss: 0.0073, Val Loss: 0.0031, Val MAE: 0.0579, Val RMSE: 0.0792, Val SMAPE: 29.74%\n",
      "Epoch [17/50], Train Loss: 0.0067, Val Loss: 0.0079, Val MAE: 0.1001, Val RMSE: 0.1256, Val SMAPE: 41.97%\n",
      "Epoch [18/50], Train Loss: 0.0110, Val Loss: 0.0033, Val MAE: 0.0622, Val RMSE: 0.0812, Val SMAPE: 32.88%\n",
      "Epoch [19/50], Train Loss: 0.0052, Val Loss: 0.0026, Val MAE: 0.0539, Val RMSE: 0.0726, Val SMAPE: 28.22%\n",
      "Epoch [20/50], Train Loss: 0.0045, Val Loss: 0.0039, Val MAE: 0.0723, Val RMSE: 0.0874, Val SMAPE: 56.87%\n",
      "Epoch [21/50], Train Loss: 0.0046, Val Loss: 0.0031, Val MAE: 0.0606, Val RMSE: 0.0785, Val SMAPE: 37.42%\n",
      "Epoch [22/50], Train Loss: 0.0039, Val Loss: 0.0030, Val MAE: 0.0596, Val RMSE: 0.0780, Val SMAPE: 30.14%\n",
      "Epoch [23/50], Train Loss: 0.0040, Val Loss: 0.0024, Val MAE: 0.0508, Val RMSE: 0.0687, Val SMAPE: 26.22%\n",
      "Epoch [24/50], Train Loss: 0.0035, Val Loss: 0.0020, Val MAE: 0.0479, Val RMSE: 0.0632, Val SMAPE: 27.36%\n",
      "Epoch [25/50], Train Loss: 0.0028, Val Loss: 0.0023, Val MAE: 0.0519, Val RMSE: 0.0675, Val SMAPE: 34.56%\n",
      "Epoch [26/50], Train Loss: 0.0028, Val Loss: 0.0030, Val MAE: 0.0569, Val RMSE: 0.0770, Val SMAPE: 30.12%\n",
      "Epoch [27/50], Train Loss: 0.0032, Val Loss: 0.0025, Val MAE: 0.0513, Val RMSE: 0.0708, Val SMAPE: 28.00%\n",
      "Epoch [28/50], Train Loss: 0.0027, Val Loss: 0.0027, Val MAE: 0.0549, Val RMSE: 0.0743, Val SMAPE: 30.52%\n",
      "Epoch [29/50], Train Loss: 0.0028, Val Loss: 0.0022, Val MAE: 0.0508, Val RMSE: 0.0671, Val SMAPE: 30.84%\n",
      "Epoch [30/50], Train Loss: 0.0025, Val Loss: 0.0018, Val MAE: 0.0442, Val RMSE: 0.0602, Val SMAPE: 24.05%\n",
      "Epoch [31/50], Train Loss: 0.0024, Val Loss: 0.0017, Val MAE: 0.0429, Val RMSE: 0.0580, Val SMAPE: 23.95%\n",
      "Epoch [32/50], Train Loss: 0.0024, Val Loss: 0.0019, Val MAE: 0.0457, Val RMSE: 0.0611, Val SMAPE: 32.82%\n",
      "Epoch [33/50], Train Loss: 0.0023, Val Loss: 0.0020, Val MAE: 0.0461, Val RMSE: 0.0627, Val SMAPE: 26.98%\n",
      "Epoch [34/50], Train Loss: 0.0024, Val Loss: 0.0024, Val MAE: 0.0517, Val RMSE: 0.0694, Val SMAPE: 29.13%\n",
      "Epoch [35/50], Train Loss: 0.0021, Val Loss: 0.0017, Val MAE: 0.0439, Val RMSE: 0.0588, Val SMAPE: 27.08%\n",
      "Epoch [36/50], Train Loss: 0.0022, Val Loss: 0.0019, Val MAE: 0.0451, Val RMSE: 0.0609, Val SMAPE: 27.19%\n",
      "Epoch [37/50], Train Loss: 0.0024, Val Loss: 0.0021, Val MAE: 0.0483, Val RMSE: 0.0652, Val SMAPE: 24.78%\n",
      "Epoch [38/50], Train Loss: 0.0022, Val Loss: 0.0021, Val MAE: 0.0488, Val RMSE: 0.0655, Val SMAPE: 25.33%\n",
      "Epoch [39/50], Train Loss: 0.0024, Val Loss: 0.0021, Val MAE: 0.0484, Val RMSE: 0.0653, Val SMAPE: 24.83%\n",
      "Epoch [40/50], Train Loss: 0.0023, Val Loss: 0.0018, Val MAE: 0.0435, Val RMSE: 0.0595, Val SMAPE: 25.17%\n",
      "Epoch [41/50], Train Loss: 0.0021, Val Loss: 0.0020, Val MAE: 0.0471, Val RMSE: 0.0641, Val SMAPE: 25.55%\n",
      "Early stopping triggered after 41 epochs\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-0.04711 \u001b[39m | \u001b[39m311.0    \u001b[39m | \u001b[39m0.1897   \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.4317, Val Loss: 0.3616, Val MAE: 0.7323, Val RMSE: 0.8836, Val SMAPE: 169.61%\n",
      "Epoch [2/50], Train Loss: 0.2759, Val Loss: 0.1882, Val MAE: 0.4906, Val RMSE: 0.6216, Val SMAPE: 142.49%\n",
      "Epoch [3/50], Train Loss: 0.1371, Val Loss: 0.0802, Val MAE: 0.3180, Val RMSE: 0.4007, Val SMAPE: 97.10%\n",
      "Epoch [4/50], Train Loss: 0.0864, Val Loss: 0.0677, Val MAE: 0.3091, Val RMSE: 0.3681, Val SMAPE: 80.86%\n",
      "Epoch [5/50], Train Loss: 0.0726, Val Loss: 0.0513, Val MAE: 0.2594, Val RMSE: 0.3197, Val SMAPE: 75.87%\n",
      "Epoch [6/50], Train Loss: 0.0551, Val Loss: 0.0408, Val MAE: 0.2109, Val RMSE: 0.2845, Val SMAPE: 66.37%\n",
      "Epoch [7/50], Train Loss: 0.0516, Val Loss: 0.0378, Val MAE: 0.2113, Val RMSE: 0.2741, Val SMAPE: 65.25%\n",
      "Epoch [8/50], Train Loss: 0.0470, Val Loss: 0.0373, Val MAE: 0.2177, Val RMSE: 0.2725, Val SMAPE: 66.64%\n",
      "Epoch [9/50], Train Loss: 0.0466, Val Loss: 0.0365, Val MAE: 0.2102, Val RMSE: 0.2692, Val SMAPE: 65.27%\n",
      "Epoch [10/50], Train Loss: 0.0461, Val Loss: 0.0362, Val MAE: 0.2105, Val RMSE: 0.2681, Val SMAPE: 65.40%\n",
      "Epoch [11/50], Train Loss: 0.0460, Val Loss: 0.0361, Val MAE: 0.2155, Val RMSE: 0.2680, Val SMAPE: 66.54%\n",
      "Epoch [12/50], Train Loss: 0.0455, Val Loss: 0.0362, Val MAE: 0.2074, Val RMSE: 0.2682, Val SMAPE: 64.78%\n",
      "Epoch [13/50], Train Loss: 0.0450, Val Loss: 0.0358, Val MAE: 0.2096, Val RMSE: 0.2669, Val SMAPE: 65.23%\n",
      "Epoch [14/50], Train Loss: 0.0452, Val Loss: 0.0355, Val MAE: 0.2118, Val RMSE: 0.2657, Val SMAPE: 65.71%\n",
      "Epoch [15/50], Train Loss: 0.0450, Val Loss: 0.0359, Val MAE: 0.2073, Val RMSE: 0.2669, Val SMAPE: 64.86%\n",
      "Epoch [16/50], Train Loss: 0.0443, Val Loss: 0.0360, Val MAE: 0.2221, Val RMSE: 0.2676, Val SMAPE: 67.94%\n",
      "Epoch [17/50], Train Loss: 0.0436, Val Loss: 0.0355, Val MAE: 0.2127, Val RMSE: 0.2656, Val SMAPE: 66.19%\n",
      "Epoch [18/50], Train Loss: 0.0439, Val Loss: 0.0353, Val MAE: 0.2051, Val RMSE: 0.2648, Val SMAPE: 64.49%\n",
      "Epoch [19/50], Train Loss: 0.0433, Val Loss: 0.0342, Val MAE: 0.2076, Val RMSE: 0.2607, Val SMAPE: 64.99%\n",
      "Epoch [20/50], Train Loss: 0.0416, Val Loss: 0.0334, Val MAE: 0.2109, Val RMSE: 0.2578, Val SMAPE: 65.51%\n",
      "Epoch [21/50], Train Loss: 0.0409, Val Loss: 0.0324, Val MAE: 0.2140, Val RMSE: 0.2542, Val SMAPE: 66.18%\n",
      "Epoch [22/50], Train Loss: 0.0409, Val Loss: 0.0317, Val MAE: 0.2154, Val RMSE: 0.2522, Val SMAPE: 66.10%\n",
      "Epoch [23/50], Train Loss: 0.0327, Val Loss: 0.0215, Val MAE: 0.1643, Val RMSE: 0.2069, Val SMAPE: 53.95%\n",
      "Epoch [24/50], Train Loss: 0.0226, Val Loss: 0.0093, Val MAE: 0.1070, Val RMSE: 0.1360, Val SMAPE: 51.66%\n",
      "Epoch [25/50], Train Loss: 0.0146, Val Loss: 0.0055, Val MAE: 0.0846, Val RMSE: 0.1043, Val SMAPE: 52.29%\n",
      "Epoch [26/50], Train Loss: 0.0115, Val Loss: 0.0038, Val MAE: 0.0663, Val RMSE: 0.0866, Val SMAPE: 28.69%\n",
      "Epoch [27/50], Train Loss: 0.0110, Val Loss: 0.0036, Val MAE: 0.0648, Val RMSE: 0.0844, Val SMAPE: 29.20%\n",
      "Epoch [28/50], Train Loss: 0.0104, Val Loss: 0.0037, Val MAE: 0.0664, Val RMSE: 0.0860, Val SMAPE: 31.05%\n",
      "Epoch [29/50], Train Loss: 0.0103, Val Loss: 0.0036, Val MAE: 0.0664, Val RMSE: 0.0844, Val SMAPE: 31.85%\n",
      "Epoch [30/50], Train Loss: 0.0100, Val Loss: 0.0036, Val MAE: 0.0657, Val RMSE: 0.0843, Val SMAPE: 28.57%\n",
      "Epoch [31/50], Train Loss: 0.0102, Val Loss: 0.0031, Val MAE: 0.0610, Val RMSE: 0.0785, Val SMAPE: 27.76%\n",
      "Epoch [32/50], Train Loss: 0.0094, Val Loss: 0.0030, Val MAE: 0.0590, Val RMSE: 0.0770, Val SMAPE: 26.63%\n",
      "Epoch [33/50], Train Loss: 0.0094, Val Loss: 0.0034, Val MAE: 0.0622, Val RMSE: 0.0817, Val SMAPE: 27.08%\n",
      "Epoch [34/50], Train Loss: 0.0090, Val Loss: 0.0029, Val MAE: 0.0585, Val RMSE: 0.0753, Val SMAPE: 26.35%\n",
      "Epoch [35/50], Train Loss: 0.0090, Val Loss: 0.0028, Val MAE: 0.0583, Val RMSE: 0.0747, Val SMAPE: 26.36%\n",
      "Epoch [36/50], Train Loss: 0.0085, Val Loss: 0.0028, Val MAE: 0.0584, Val RMSE: 0.0745, Val SMAPE: 27.47%\n",
      "Epoch [37/50], Train Loss: 0.0091, Val Loss: 0.0028, Val MAE: 0.0573, Val RMSE: 0.0740, Val SMAPE: 25.53%\n",
      "Epoch [38/50], Train Loss: 0.0091, Val Loss: 0.0029, Val MAE: 0.0587, Val RMSE: 0.0764, Val SMAPE: 25.69%\n",
      "Epoch [39/50], Train Loss: 0.0088, Val Loss: 0.0030, Val MAE: 0.0609, Val RMSE: 0.0775, Val SMAPE: 29.73%\n",
      "Epoch [40/50], Train Loss: 0.0087, Val Loss: 0.0026, Val MAE: 0.0560, Val RMSE: 0.0722, Val SMAPE: 25.22%\n",
      "Epoch [41/50], Train Loss: 0.0084, Val Loss: 0.0026, Val MAE: 0.0561, Val RMSE: 0.0720, Val SMAPE: 25.54%\n",
      "Epoch [42/50], Train Loss: 0.0089, Val Loss: 0.0026, Val MAE: 0.0561, Val RMSE: 0.0720, Val SMAPE: 25.63%\n",
      "Epoch [43/50], Train Loss: 0.0088, Val Loss: 0.0028, Val MAE: 0.0579, Val RMSE: 0.0740, Val SMAPE: 27.46%\n",
      "Epoch [44/50], Train Loss: 0.0085, Val Loss: 0.0026, Val MAE: 0.0559, Val RMSE: 0.0723, Val SMAPE: 25.07%\n",
      "Epoch [45/50], Train Loss: 0.0084, Val Loss: 0.0026, Val MAE: 0.0555, Val RMSE: 0.0716, Val SMAPE: 25.10%\n",
      "Epoch [46/50], Train Loss: 0.0084, Val Loss: 0.0026, Val MAE: 0.0557, Val RMSE: 0.0717, Val SMAPE: 25.57%\n",
      "Epoch [47/50], Train Loss: 0.0083, Val Loss: 0.0026, Val MAE: 0.0560, Val RMSE: 0.0720, Val SMAPE: 25.87%\n",
      "Epoch [48/50], Train Loss: 0.0081, Val Loss: 0.0026, Val MAE: 0.0558, Val RMSE: 0.0717, Val SMAPE: 25.65%\n",
      "Epoch [49/50], Train Loss: 0.0084, Val Loss: 0.0026, Val MAE: 0.0556, Val RMSE: 0.0716, Val SMAPE: 25.53%\n",
      "Epoch [50/50], Train Loss: 0.0082, Val Loss: 0.0026, Val MAE: 0.0556, Val RMSE: 0.0716, Val SMAPE: 25.53%\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-0.05563 \u001b[39m | \u001b[39m171.4    \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m8.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1043, Val Loss: 0.0787, Val MAE: 0.3364, Val RMSE: 0.3981, Val SMAPE: 89.34%\n",
      "Epoch [2/50], Train Loss: 0.0749, Val Loss: 0.0349, Val MAE: 0.2173, Val RMSE: 0.2649, Val SMAPE: 73.58%\n",
      "Epoch [3/50], Train Loss: 0.0583, Val Loss: 0.0314, Val MAE: 0.1984, Val RMSE: 0.2511, Val SMAPE: 70.17%\n",
      "Epoch [4/50], Train Loss: 0.0481, Val Loss: 0.0315, Val MAE: 0.2167, Val RMSE: 0.2520, Val SMAPE: 72.34%\n",
      "Epoch [5/50], Train Loss: 0.0263, Val Loss: 0.0089, Val MAE: 0.1060, Val RMSE: 0.1337, Val SMAPE: 61.77%\n",
      "Epoch [6/50], Train Loss: 0.0116, Val Loss: 0.0061, Val MAE: 0.0873, Val RMSE: 0.1120, Val SMAPE: 45.05%\n",
      "Epoch [7/50], Train Loss: 0.0069, Val Loss: 0.0034, Val MAE: 0.0655, Val RMSE: 0.0841, Val SMAPE: 45.61%\n",
      "Epoch [8/50], Train Loss: 0.0046, Val Loss: 0.0035, Val MAE: 0.0643, Val RMSE: 0.0853, Val SMAPE: 33.73%\n",
      "Epoch [9/50], Train Loss: 0.0035, Val Loss: 0.0023, Val MAE: 0.0534, Val RMSE: 0.0689, Val SMAPE: 36.49%\n",
      "Epoch [10/50], Train Loss: 0.0035, Val Loss: 0.0031, Val MAE: 0.0645, Val RMSE: 0.0804, Val SMAPE: 33.87%\n",
      "Epoch [11/50], Train Loss: 0.0028, Val Loss: 0.0027, Val MAE: 0.0576, Val RMSE: 0.0741, Val SMAPE: 31.17%\n",
      "Epoch [12/50], Train Loss: 0.0033, Val Loss: 0.0020, Val MAE: 0.0470, Val RMSE: 0.0636, Val SMAPE: 27.78%\n",
      "Epoch [13/50], Train Loss: 0.0027, Val Loss: 0.0017, Val MAE: 0.0456, Val RMSE: 0.0596, Val SMAPE: 26.09%\n",
      "Epoch [14/50], Train Loss: 0.0026, Val Loss: 0.0025, Val MAE: 0.0580, Val RMSE: 0.0714, Val SMAPE: 31.96%\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0019, Val MAE: 0.0466, Val RMSE: 0.0617, Val SMAPE: 29.39%\n",
      "Epoch [16/50], Train Loss: 0.0020, Val Loss: 0.0020, Val MAE: 0.0507, Val RMSE: 0.0637, Val SMAPE: 27.78%\n",
      "Epoch [17/50], Train Loss: 0.0020, Val Loss: 0.0020, Val MAE: 0.0467, Val RMSE: 0.0636, Val SMAPE: 26.14%\n",
      "Epoch [18/50], Train Loss: 0.0021, Val Loss: 0.0021, Val MAE: 0.0489, Val RMSE: 0.0661, Val SMAPE: 27.41%\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0024, Val MAE: 0.0566, Val RMSE: 0.0693, Val SMAPE: 31.17%\n",
      "Epoch [20/50], Train Loss: 0.0019, Val Loss: 0.0015, Val MAE: 0.0444, Val RMSE: 0.0558, Val SMAPE: 26.11%\n",
      "Epoch [21/50], Train Loss: 0.0016, Val Loss: 0.0015, Val MAE: 0.0428, Val RMSE: 0.0546, Val SMAPE: 26.42%\n",
      "Epoch [22/50], Train Loss: 0.0018, Val Loss: 0.0020, Val MAE: 0.0464, Val RMSE: 0.0628, Val SMAPE: 26.41%\n",
      "Epoch [23/50], Train Loss: 0.0017, Val Loss: 0.0016, Val MAE: 0.0449, Val RMSE: 0.0568, Val SMAPE: 25.70%\n",
      "Epoch [24/50], Train Loss: 0.0016, Val Loss: 0.0015, Val MAE: 0.0420, Val RMSE: 0.0553, Val SMAPE: 24.75%\n",
      "Epoch [25/50], Train Loss: 0.0017, Val Loss: 0.0016, Val MAE: 0.0457, Val RMSE: 0.0571, Val SMAPE: 26.59%\n",
      "Epoch [26/50], Train Loss: 0.0016, Val Loss: 0.0017, Val MAE: 0.0426, Val RMSE: 0.0584, Val SMAPE: 24.92%\n",
      "Epoch [27/50], Train Loss: 0.0017, Val Loss: 0.0014, Val MAE: 0.0432, Val RMSE: 0.0541, Val SMAPE: 25.47%\n",
      "Epoch [28/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0395, Val RMSE: 0.0520, Val SMAPE: 23.41%\n",
      "Epoch [29/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0406, Val RMSE: 0.0519, Val SMAPE: 23.80%\n",
      "Epoch [30/50], Train Loss: 0.0014, Val Loss: 0.0019, Val MAE: 0.0443, Val RMSE: 0.0612, Val SMAPE: 25.12%\n",
      "Epoch [31/50], Train Loss: 0.0016, Val Loss: 0.0017, Val MAE: 0.0419, Val RMSE: 0.0582, Val SMAPE: 24.04%\n",
      "Epoch [32/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0409, Val RMSE: 0.0523, Val SMAPE: 24.40%\n",
      "Epoch [33/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0380, Val RMSE: 0.0491, Val SMAPE: 22.64%\n",
      "Epoch [34/50], Train Loss: 0.0012, Val Loss: 0.0014, Val MAE: 0.0410, Val RMSE: 0.0522, Val SMAPE: 23.23%\n",
      "Epoch [35/50], Train Loss: 0.0013, Val Loss: 0.0012, Val MAE: 0.0375, Val RMSE: 0.0495, Val SMAPE: 22.14%\n",
      "Epoch [36/50], Train Loss: 0.0013, Val Loss: 0.0014, Val MAE: 0.0437, Val RMSE: 0.0541, Val SMAPE: 26.05%\n",
      "Epoch [37/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0369, Val RMSE: 0.0476, Val SMAPE: 22.23%\n",
      "Epoch [38/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0387, Val RMSE: 0.0492, Val SMAPE: 22.96%\n",
      "Epoch [39/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0382, Val RMSE: 0.0487, Val SMAPE: 22.04%\n",
      "Epoch [40/50], Train Loss: 0.0011, Val Loss: 0.0012, Val MAE: 0.0395, Val RMSE: 0.0499, Val SMAPE: 23.30%\n",
      "Epoch [41/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0361, Val RMSE: 0.0469, Val SMAPE: 21.27%\n",
      "Epoch [42/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0361, Val RMSE: 0.0473, Val SMAPE: 21.16%\n",
      "Epoch [43/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0361, Val RMSE: 0.0472, Val SMAPE: 21.03%\n",
      "Epoch [44/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0361, Val RMSE: 0.0469, Val SMAPE: 21.05%\n",
      "Epoch [45/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0360, Val RMSE: 0.0468, Val SMAPE: 21.11%\n",
      "Epoch [46/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0360, Val RMSE: 0.0470, Val SMAPE: 21.17%\n",
      "Epoch [47/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0358, Val RMSE: 0.0468, Val SMAPE: 21.03%\n",
      "Epoch [48/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0359, Val RMSE: 0.0468, Val SMAPE: 21.05%\n",
      "Epoch [49/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0360, Val RMSE: 0.0468, Val SMAPE: 21.10%\n",
      "Epoch [50/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0360, Val RMSE: 0.0469, Val SMAPE: 21.11%\n",
      "| \u001b[35m9        \u001b[39m | \u001b[35m-0.03605 \u001b[39m | \u001b[35m64.0     \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m2.0      \u001b[39m | \u001b[35m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 1.2269, Val Loss: 0.4493, Val MAE: 0.7932, Val RMSE: 1.1547, Val SMAPE: 139.99%\n",
      "Epoch [2/50], Train Loss: 0.3329, Val Loss: 0.2247, Val MAE: 0.5169, Val RMSE: 0.7013, Val SMAPE: 130.79%\n",
      "Epoch [3/50], Train Loss: 0.2143, Val Loss: 0.0823, Val MAE: 0.3147, Val RMSE: 0.4080, Val SMAPE: 113.37%\n",
      "Epoch [4/50], Train Loss: 0.1148, Val Loss: 0.0627, Val MAE: 0.2953, Val RMSE: 0.3538, Val SMAPE: 85.53%\n",
      "Epoch [5/50], Train Loss: 0.0751, Val Loss: 0.0118, Val MAE: 0.1225, Val RMSE: 0.1543, Val SMAPE: 75.07%\n",
      "Epoch [6/50], Train Loss: 0.0472, Val Loss: 0.0293, Val MAE: 0.2004, Val RMSE: 0.2431, Val SMAPE: 83.05%\n",
      "Epoch [7/50], Train Loss: 0.0357, Val Loss: 0.0205, Val MAE: 0.1743, Val RMSE: 0.2029, Val SMAPE: 84.43%\n",
      "Epoch [8/50], Train Loss: 0.0312, Val Loss: 0.0225, Val MAE: 0.1825, Val RMSE: 0.2125, Val SMAPE: 100.46%\n",
      "Epoch [9/50], Train Loss: 0.0511, Val Loss: 0.0321, Val MAE: 0.1951, Val RMSE: 0.2538, Val SMAPE: 88.06%\n",
      "Epoch [10/50], Train Loss: 0.0431, Val Loss: 0.0147, Val MAE: 0.1266, Val RMSE: 0.1714, Val SMAPE: 60.08%\n",
      "Epoch [11/50], Train Loss: 0.0872, Val Loss: 0.1371, Val MAE: 0.4769, Val RMSE: 0.5232, Val SMAPE: 117.02%\n",
      "Epoch [12/50], Train Loss: 0.1038, Val Loss: 0.0322, Val MAE: 0.2140, Val RMSE: 0.2546, Val SMAPE: 95.59%\n",
      "Epoch [13/50], Train Loss: 0.0259, Val Loss: 0.0080, Val MAE: 0.0969, Val RMSE: 0.1256, Val SMAPE: 68.02%\n",
      "Epoch [14/50], Train Loss: 0.0502, Val Loss: 0.0322, Val MAE: 0.2271, Val RMSE: 0.2542, Val SMAPE: 99.83%\n",
      "Epoch [15/50], Train Loss: 0.0671, Val Loss: 0.0339, Val MAE: 0.2264, Val RMSE: 0.2599, Val SMAPE: 87.06%\n",
      "Epoch [16/50], Train Loss: 0.0534, Val Loss: 0.0282, Val MAE: 0.2029, Val RMSE: 0.2393, Val SMAPE: 73.00%\n",
      "Epoch [17/50], Train Loss: 0.0497, Val Loss: 0.0341, Val MAE: 0.2297, Val RMSE: 0.2635, Val SMAPE: 79.81%\n",
      "Epoch [18/50], Train Loss: 0.0470, Val Loss: 0.0297, Val MAE: 0.1930, Val RMSE: 0.2478, Val SMAPE: 74.92%\n",
      "Epoch [19/50], Train Loss: 0.0379, Val Loss: 0.0174, Val MAE: 0.1385, Val RMSE: 0.1905, Val SMAPE: 57.81%\n",
      "Epoch [20/50], Train Loss: 0.0208, Val Loss: 0.0037, Val MAE: 0.0673, Val RMSE: 0.0856, Val SMAPE: 51.92%\n",
      "Epoch [21/50], Train Loss: 0.0111, Val Loss: 0.0049, Val MAE: 0.0801, Val RMSE: 0.0986, Val SMAPE: 44.18%\n",
      "Epoch [22/50], Train Loss: 0.0061, Val Loss: 0.0022, Val MAE: 0.0513, Val RMSE: 0.0671, Val SMAPE: 33.44%\n",
      "Epoch [23/50], Train Loss: 0.0054, Val Loss: 0.0023, Val MAE: 0.0521, Val RMSE: 0.0677, Val SMAPE: 34.04%\n",
      "Epoch [24/50], Train Loss: 0.0047, Val Loss: 0.0024, Val MAE: 0.0531, Val RMSE: 0.0689, Val SMAPE: 34.04%\n",
      "Epoch [25/50], Train Loss: 0.0046, Val Loss: 0.0018, Val MAE: 0.0462, Val RMSE: 0.0594, Val SMAPE: 31.30%\n",
      "Epoch [26/50], Train Loss: 0.0040, Val Loss: 0.0016, Val MAE: 0.0433, Val RMSE: 0.0572, Val SMAPE: 30.98%\n",
      "Epoch [27/50], Train Loss: 0.0042, Val Loss: 0.0017, Val MAE: 0.0448, Val RMSE: 0.0591, Val SMAPE: 29.98%\n",
      "Epoch [28/50], Train Loss: 0.0034, Val Loss: 0.0021, Val MAE: 0.0524, Val RMSE: 0.0653, Val SMAPE: 34.43%\n",
      "Epoch [29/50], Train Loss: 0.0039, Val Loss: 0.0016, Val MAE: 0.0439, Val RMSE: 0.0558, Val SMAPE: 30.41%\n",
      "Epoch [30/50], Train Loss: 0.0031, Val Loss: 0.0034, Val MAE: 0.0676, Val RMSE: 0.0823, Val SMAPE: 40.17%\n",
      "Epoch [31/50], Train Loss: 0.0033, Val Loss: 0.0017, Val MAE: 0.0445, Val RMSE: 0.0577, Val SMAPE: 30.13%\n",
      "Epoch [32/50], Train Loss: 0.0033, Val Loss: 0.0016, Val MAE: 0.0441, Val RMSE: 0.0564, Val SMAPE: 30.24%\n",
      "Epoch [33/50], Train Loss: 0.0032, Val Loss: 0.0016, Val MAE: 0.0456, Val RMSE: 0.0573, Val SMAPE: 31.44%\n",
      "Epoch [34/50], Train Loss: 0.0027, Val Loss: 0.0018, Val MAE: 0.0466, Val RMSE: 0.0596, Val SMAPE: 31.09%\n",
      "Epoch [35/50], Train Loss: 0.0026, Val Loss: 0.0014, Val MAE: 0.0416, Val RMSE: 0.0531, Val SMAPE: 29.05%\n",
      "Epoch [36/50], Train Loss: 0.0034, Val Loss: 0.0016, Val MAE: 0.0440, Val RMSE: 0.0565, Val SMAPE: 29.65%\n",
      "Epoch [37/50], Train Loss: 0.0030, Val Loss: 0.0014, Val MAE: 0.0418, Val RMSE: 0.0529, Val SMAPE: 29.02%\n",
      "Epoch [38/50], Train Loss: 0.0027, Val Loss: 0.0014, Val MAE: 0.0421, Val RMSE: 0.0526, Val SMAPE: 29.52%\n",
      "Epoch [39/50], Train Loss: 0.0027, Val Loss: 0.0015, Val MAE: 0.0450, Val RMSE: 0.0550, Val SMAPE: 31.62%\n",
      "Epoch [40/50], Train Loss: 0.0028, Val Loss: 0.0014, Val MAE: 0.0415, Val RMSE: 0.0521, Val SMAPE: 29.34%\n",
      "Epoch [41/50], Train Loss: 0.0026, Val Loss: 0.0016, Val MAE: 0.0447, Val RMSE: 0.0562, Val SMAPE: 30.98%\n",
      "Epoch [42/50], Train Loss: 0.0026, Val Loss: 0.0016, Val MAE: 0.0451, Val RMSE: 0.0563, Val SMAPE: 31.27%\n",
      "Epoch [43/50], Train Loss: 0.0024, Val Loss: 0.0014, Val MAE: 0.0430, Val RMSE: 0.0537, Val SMAPE: 30.32%\n",
      "Epoch [44/50], Train Loss: 0.0026, Val Loss: 0.0015, Val MAE: 0.0442, Val RMSE: 0.0550, Val SMAPE: 30.83%\n",
      "Epoch [45/50], Train Loss: 0.0025, Val Loss: 0.0015, Val MAE: 0.0438, Val RMSE: 0.0544, Val SMAPE: 30.64%\n",
      "Epoch [46/50], Train Loss: 0.0027, Val Loss: 0.0014, Val MAE: 0.0435, Val RMSE: 0.0540, Val SMAPE: 30.50%\n",
      "Epoch [47/50], Train Loss: 0.0025, Val Loss: 0.0015, Val MAE: 0.0448, Val RMSE: 0.0557, Val SMAPE: 31.17%\n",
      "Epoch [48/50], Train Loss: 0.0027, Val Loss: 0.0015, Val MAE: 0.0444, Val RMSE: 0.0552, Val SMAPE: 30.99%\n",
      "Epoch [49/50], Train Loss: 0.0024, Val Loss: 0.0015, Val MAE: 0.0440, Val RMSE: 0.0547, Val SMAPE: 30.77%\n",
      "Epoch [50/50], Train Loss: 0.0025, Val Loss: 0.0015, Val MAE: 0.0439, Val RMSE: 0.0546, Val SMAPE: 30.73%\n",
      "Early stopping triggered after 50 epochs\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-0.04393 \u001b[39m | \u001b[39m512.0    \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1169, Val Loss: 0.0502, Val MAE: 0.2642, Val RMSE: 0.3184, Val SMAPE: 77.64%\n",
      "Epoch [2/50], Train Loss: 0.0644, Val Loss: 0.0485, Val MAE: 0.2304, Val RMSE: 0.3119, Val SMAPE: 79.52%\n",
      "Epoch [3/50], Train Loss: 0.0458, Val Loss: 0.0380, Val MAE: 0.2017, Val RMSE: 0.2755, Val SMAPE: 78.62%\n",
      "Epoch [4/50], Train Loss: 0.0255, Val Loss: 0.0154, Val MAE: 0.1413, Val RMSE: 0.1754, Val SMAPE: 75.41%\n",
      "Epoch [5/50], Train Loss: 0.0179, Val Loss: 0.0164, Val MAE: 0.1430, Val RMSE: 0.1801, Val SMAPE: 71.50%\n",
      "Epoch [6/50], Train Loss: 0.0149, Val Loss: 0.0125, Val MAE: 0.1308, Val RMSE: 0.1572, Val SMAPE: 77.69%\n",
      "Epoch [7/50], Train Loss: 0.0113, Val Loss: 0.0064, Val MAE: 0.0889, Val RMSE: 0.1139, Val SMAPE: 41.94%\n",
      "Epoch [8/50], Train Loss: 0.0088, Val Loss: 0.0078, Val MAE: 0.0926, Val RMSE: 0.1248, Val SMAPE: 48.10%\n",
      "Epoch [9/50], Train Loss: 0.0123, Val Loss: 0.0098, Val MAE: 0.1120, Val RMSE: 0.1402, Val SMAPE: 66.88%\n",
      "Epoch [10/50], Train Loss: 0.0075, Val Loss: 0.0097, Val MAE: 0.1089, Val RMSE: 0.1384, Val SMAPE: 62.08%\n",
      "Epoch [11/50], Train Loss: 0.0097, Val Loss: 0.0145, Val MAE: 0.1401, Val RMSE: 0.1697, Val SMAPE: 75.42%\n",
      "Epoch [12/50], Train Loss: 0.0084, Val Loss: 0.0044, Val MAE: 0.0763, Val RMSE: 0.0938, Val SMAPE: 47.82%\n",
      "Epoch [13/50], Train Loss: 0.0053, Val Loss: 0.0039, Val MAE: 0.0729, Val RMSE: 0.0888, Val SMAPE: 33.03%\n",
      "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0023, Val MAE: 0.0517, Val RMSE: 0.0674, Val SMAPE: 28.94%\n",
      "Epoch [15/50], Train Loss: 0.0028, Val Loss: 0.0055, Val MAE: 0.0843, Val RMSE: 0.1042, Val SMAPE: 51.65%\n",
      "Epoch [16/50], Train Loss: 0.0049, Val Loss: 0.0037, Val MAE: 0.0650, Val RMSE: 0.0850, Val SMAPE: 38.97%\n",
      "Epoch [17/50], Train Loss: 0.0043, Val Loss: 0.0023, Val MAE: 0.0526, Val RMSE: 0.0676, Val SMAPE: 27.33%\n",
      "Epoch [18/50], Train Loss: 0.0032, Val Loss: 0.0068, Val MAE: 0.0971, Val RMSE: 0.1166, Val SMAPE: 50.31%\n",
      "Epoch [19/50], Train Loss: 0.0035, Val Loss: 0.0025, Val MAE: 0.0563, Val RMSE: 0.0704, Val SMAPE: 27.84%\n",
      "Epoch [20/50], Train Loss: 0.0021, Val Loss: 0.0017, Val MAE: 0.0461, Val RMSE: 0.0587, Val SMAPE: 24.50%\n",
      "Epoch [21/50], Train Loss: 0.0017, Val Loss: 0.0030, Val MAE: 0.0663, Val RMSE: 0.0777, Val SMAPE: 32.20%\n",
      "Epoch [22/50], Train Loss: 0.0021, Val Loss: 0.0028, Val MAE: 0.0603, Val RMSE: 0.0740, Val SMAPE: 42.54%\n",
      "Epoch [23/50], Train Loss: 0.0019, Val Loss: 0.0020, Val MAE: 0.0518, Val RMSE: 0.0639, Val SMAPE: 27.15%\n",
      "Epoch [24/50], Train Loss: 0.0017, Val Loss: 0.0019, Val MAE: 0.0459, Val RMSE: 0.0617, Val SMAPE: 22.53%\n",
      "Epoch [25/50], Train Loss: 0.0015, Val Loss: 0.0013, Val MAE: 0.0383, Val RMSE: 0.0507, Val SMAPE: 21.68%\n",
      "Epoch [26/50], Train Loss: 0.0014, Val Loss: 0.0016, Val MAE: 0.0418, Val RMSE: 0.0566, Val SMAPE: 21.72%\n",
      "Epoch [27/50], Train Loss: 0.0013, Val Loss: 0.0012, Val MAE: 0.0372, Val RMSE: 0.0497, Val SMAPE: 20.20%\n",
      "Epoch [28/50], Train Loss: 0.0014, Val Loss: 0.0021, Val MAE: 0.0500, Val RMSE: 0.0641, Val SMAPE: 31.98%\n",
      "Epoch [29/50], Train Loss: 0.0016, Val Loss: 0.0017, Val MAE: 0.0467, Val RMSE: 0.0586, Val SMAPE: 23.38%\n",
      "Epoch [30/50], Train Loss: 0.0015, Val Loss: 0.0013, Val MAE: 0.0374, Val RMSE: 0.0505, Val SMAPE: 19.21%\n",
      "Epoch [31/50], Train Loss: 0.0014, Val Loss: 0.0018, Val MAE: 0.0427, Val RMSE: 0.0593, Val SMAPE: 20.05%\n",
      "Epoch [32/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0379, Val RMSE: 0.0509, Val SMAPE: 19.51%\n",
      "Epoch [33/50], Train Loss: 0.0012, Val Loss: 0.0016, Val MAE: 0.0456, Val RMSE: 0.0570, Val SMAPE: 23.64%\n",
      "Epoch [34/50], Train Loss: 0.0013, Val Loss: 0.0011, Val MAE: 0.0351, Val RMSE: 0.0470, Val SMAPE: 18.13%\n",
      "Epoch [35/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0368, Val RMSE: 0.0512, Val SMAPE: 18.35%\n",
      "Epoch [36/50], Train Loss: 0.0011, Val Loss: 0.0013, Val MAE: 0.0394, Val RMSE: 0.0506, Val SMAPE: 20.77%\n",
      "Epoch [37/50], Train Loss: 0.0011, Val Loss: 0.0012, Val MAE: 0.0345, Val RMSE: 0.0479, Val SMAPE: 17.55%\n",
      "Epoch [38/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0338, Val RMSE: 0.0463, Val SMAPE: 17.54%\n",
      "Epoch [39/50], Train Loss: 0.0009, Val Loss: 0.0011, Val MAE: 0.0347, Val RMSE: 0.0470, Val SMAPE: 18.07%\n",
      "Epoch [40/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0343, Val RMSE: 0.0467, Val SMAPE: 17.70%\n",
      "Epoch [41/50], Train Loss: 0.0009, Val Loss: 0.0011, Val MAE: 0.0344, Val RMSE: 0.0466, Val SMAPE: 17.84%\n",
      "Epoch [42/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0324, Val RMSE: 0.0444, Val SMAPE: 16.38%\n",
      "Epoch [43/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0319, Val RMSE: 0.0442, Val SMAPE: 16.44%\n",
      "Epoch [44/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0315, Val RMSE: 0.0437, Val SMAPE: 16.16%\n",
      "Epoch [45/50], Train Loss: 0.0009, Val Loss: 0.0009, Val MAE: 0.0310, Val RMSE: 0.0432, Val SMAPE: 15.51%\n",
      "Epoch [46/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0312, Val RMSE: 0.0438, Val SMAPE: 15.56%\n",
      "Epoch [47/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0319, Val RMSE: 0.0442, Val SMAPE: 16.18%\n",
      "Epoch [48/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0319, Val RMSE: 0.0442, Val SMAPE: 16.16%\n",
      "Epoch [49/50], Train Loss: 0.0008, Val Loss: 0.0010, Val MAE: 0.0318, Val RMSE: 0.0441, Val SMAPE: 16.11%\n",
      "Epoch [50/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0318, Val RMSE: 0.0441, Val SMAPE: 16.08%\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m-0.03176 \u001b[39m | \u001b[35m212.8    \u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m16.0     \u001b[39m | \u001b[35m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.5618, Val Loss: 0.1783, Val MAE: 0.4454, Val RMSE: 0.6177, Val SMAPE: 91.91%\n",
      "Epoch [2/50], Train Loss: 0.2214, Val Loss: 0.0476, Val MAE: 0.2636, Val RMSE: 0.3109, Val SMAPE: 77.49%\n",
      "Epoch [3/50], Train Loss: 0.1799, Val Loss: 0.0594, Val MAE: 0.2616, Val RMSE: 0.3482, Val SMAPE: 95.98%\n",
      "Epoch [4/50], Train Loss: 0.1501, Val Loss: 0.0486, Val MAE: 0.2452, Val RMSE: 0.3144, Val SMAPE: 75.90%\n",
      "Epoch [5/50], Train Loss: 0.1241, Val Loss: 0.0904, Val MAE: 0.3472, Val RMSE: 0.4273, Val SMAPE: 115.30%\n",
      "Epoch [6/50], Train Loss: 0.1016, Val Loss: 0.0494, Val MAE: 0.2277, Val RMSE: 0.3183, Val SMAPE: 88.76%\n",
      "Epoch [7/50], Train Loss: 0.0813, Val Loss: 0.0394, Val MAE: 0.2348, Val RMSE: 0.2837, Val SMAPE: 73.38%\n",
      "Epoch [8/50], Train Loss: 0.0668, Val Loss: 0.0394, Val MAE: 0.2261, Val RMSE: 0.2836, Val SMAPE: 72.01%\n",
      "Epoch [9/50], Train Loss: 0.0575, Val Loss: 0.0431, Val MAE: 0.2515, Val RMSE: 0.2962, Val SMAPE: 76.02%\n",
      "Epoch [10/50], Train Loss: 0.0538, Val Loss: 0.0370, Val MAE: 0.2287, Val RMSE: 0.2749, Val SMAPE: 72.76%\n",
      "Epoch [11/50], Train Loss: 0.0539, Val Loss: 0.0413, Val MAE: 0.2493, Val RMSE: 0.2897, Val SMAPE: 75.97%\n",
      "Epoch [12/50], Train Loss: 0.0480, Val Loss: 0.0408, Val MAE: 0.2505, Val RMSE: 0.2880, Val SMAPE: 76.37%\n",
      "Epoch [13/50], Train Loss: 0.0468, Val Loss: 0.0345, Val MAE: 0.1966, Val RMSE: 0.2666, Val SMAPE: 66.66%\n",
      "Epoch [14/50], Train Loss: 0.0503, Val Loss: 0.0423, Val MAE: 0.2565, Val RMSE: 0.2933, Val SMAPE: 77.20%\n",
      "Epoch [15/50], Train Loss: 0.0497, Val Loss: 0.0348, Val MAE: 0.2083, Val RMSE: 0.2674, Val SMAPE: 69.06%\n",
      "Epoch [16/50], Train Loss: 0.0498, Val Loss: 0.0596, Val MAE: 0.3157, Val RMSE: 0.3465, Val SMAPE: 85.13%\n",
      "Epoch [17/50], Train Loss: 0.0464, Val Loss: 0.0426, Val MAE: 0.2579, Val RMSE: 0.2941, Val SMAPE: 77.43%\n",
      "Epoch [18/50], Train Loss: 0.0423, Val Loss: 0.0405, Val MAE: 0.2503, Val RMSE: 0.2872, Val SMAPE: 76.39%\n",
      "Epoch [19/50], Train Loss: 0.0436, Val Loss: 0.0352, Val MAE: 0.2254, Val RMSE: 0.2682, Val SMAPE: 72.58%\n",
      "Epoch [20/50], Train Loss: 0.0411, Val Loss: 0.0344, Val MAE: 0.2186, Val RMSE: 0.2656, Val SMAPE: 71.41%\n",
      "Epoch [21/50], Train Loss: 0.0412, Val Loss: 0.0474, Val MAE: 0.2776, Val RMSE: 0.3099, Val SMAPE: 80.32%\n",
      "Epoch [22/50], Train Loss: 0.0412, Val Loss: 0.0385, Val MAE: 0.2420, Val RMSE: 0.2800, Val SMAPE: 75.16%\n",
      "Epoch [23/50], Train Loss: 0.0403, Val Loss: 0.0336, Val MAE: 0.2059, Val RMSE: 0.2630, Val SMAPE: 68.93%\n",
      "Epoch [24/50], Train Loss: 0.0397, Val Loss: 0.0349, Val MAE: 0.2229, Val RMSE: 0.2673, Val SMAPE: 72.17%\n",
      "Epoch [25/50], Train Loss: 0.0399, Val Loss: 0.0337, Val MAE: 0.2122, Val RMSE: 0.2629, Val SMAPE: 70.26%\n",
      "Epoch [26/50], Train Loss: 0.0382, Val Loss: 0.0383, Val MAE: 0.2411, Val RMSE: 0.2793, Val SMAPE: 75.04%\n",
      "Epoch [27/50], Train Loss: 0.0388, Val Loss: 0.0419, Val MAE: 0.2567, Val RMSE: 0.2918, Val SMAPE: 77.36%\n",
      "Epoch [28/50], Train Loss: 0.0390, Val Loss: 0.0391, Val MAE: 0.2449, Val RMSE: 0.2821, Val SMAPE: 75.62%\n",
      "Epoch [29/50], Train Loss: 0.0389, Val Loss: 0.0354, Val MAE: 0.1903, Val RMSE: 0.2701, Val SMAPE: 65.04%\n",
      "Epoch [30/50], Train Loss: 0.0402, Val Loss: 0.0342, Val MAE: 0.1958, Val RMSE: 0.2655, Val SMAPE: 66.55%\n",
      "Epoch [31/50], Train Loss: 0.0393, Val Loss: 0.0434, Val MAE: 0.2625, Val RMSE: 0.2969, Val SMAPE: 78.19%\n",
      "Epoch [32/50], Train Loss: 0.0396, Val Loss: 0.0410, Val MAE: 0.2531, Val RMSE: 0.2889, Val SMAPE: 76.83%\n",
      "Epoch [33/50], Train Loss: 0.0383, Val Loss: 0.0368, Val MAE: 0.2344, Val RMSE: 0.2740, Val SMAPE: 74.02%\n",
      "Early stopping triggered after 33 epochs\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m-0.2344  \u001b[39m | \u001b[39m336.5    \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m8.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.2251, Val Loss: 0.1137, Val MAE: 0.3939, Val RMSE: 0.4771, Val SMAPE: 107.26%\n",
      "Epoch [2/50], Train Loss: 0.1102, Val Loss: 0.0474, Val MAE: 0.2470, Val RMSE: 0.3045, Val SMAPE: 81.75%\n",
      "Epoch [3/50], Train Loss: 0.0790, Val Loss: 0.0444, Val MAE: 0.2233, Val RMSE: 0.2960, Val SMAPE: 92.48%\n",
      "Epoch [4/50], Train Loss: 0.0512, Val Loss: 0.0185, Val MAE: 0.1576, Val RMSE: 0.1928, Val SMAPE: 75.90%\n",
      "Epoch [5/50], Train Loss: 0.0293, Val Loss: 0.0173, Val MAE: 0.1519, Val RMSE: 0.1859, Val SMAPE: 67.66%\n",
      "Epoch [6/50], Train Loss: 0.0198, Val Loss: 0.0164, Val MAE: 0.1453, Val RMSE: 0.1811, Val SMAPE: 67.98%\n",
      "Epoch [7/50], Train Loss: 0.0146, Val Loss: 0.0085, Val MAE: 0.1046, Val RMSE: 0.1296, Val SMAPE: 53.91%\n",
      "Epoch [8/50], Train Loss: 0.0123, Val Loss: 0.0245, Val MAE: 0.1718, Val RMSE: 0.2210, Val SMAPE: 78.01%\n",
      "Epoch [9/50], Train Loss: 0.0171, Val Loss: 0.0140, Val MAE: 0.1415, Val RMSE: 0.1671, Val SMAPE: 70.21%\n",
      "Epoch [10/50], Train Loss: 0.0134, Val Loss: 0.0119, Val MAE: 0.1299, Val RMSE: 0.1541, Val SMAPE: 68.05%\n",
      "Epoch [11/50], Train Loss: 0.0100, Val Loss: 0.0061, Val MAE: 0.0884, Val RMSE: 0.1100, Val SMAPE: 54.78%\n",
      "Epoch [12/50], Train Loss: 0.0073, Val Loss: 0.0093, Val MAE: 0.1106, Val RMSE: 0.1360, Val SMAPE: 78.25%\n",
      "Epoch [13/50], Train Loss: 0.0068, Val Loss: 0.0061, Val MAE: 0.0932, Val RMSE: 0.1102, Val SMAPE: 73.77%\n",
      "Epoch [14/50], Train Loss: 0.0110, Val Loss: 0.0489, Val MAE: 0.2587, Val RMSE: 0.3121, Val SMAPE: 122.02%\n",
      "Epoch [15/50], Train Loss: 0.5019, Val Loss: 0.2311, Val MAE: 0.6102, Val RMSE: 0.6814, Val SMAPE: 148.91%\n",
      "Epoch [16/50], Train Loss: 0.1399, Val Loss: 0.0738, Val MAE: 0.3239, Val RMSE: 0.3818, Val SMAPE: 103.73%\n",
      "Epoch [17/50], Train Loss: 0.0682, Val Loss: 0.0466, Val MAE: 0.2657, Val RMSE: 0.3027, Val SMAPE: 82.17%\n",
      "Epoch [18/50], Train Loss: 0.0509, Val Loss: 0.0370, Val MAE: 0.2275, Val RMSE: 0.2692, Val SMAPE: 76.68%\n",
      "Epoch [19/50], Train Loss: 0.0456, Val Loss: 0.0417, Val MAE: 0.2556, Val RMSE: 0.2904, Val SMAPE: 78.97%\n",
      "Epoch [20/50], Train Loss: 0.0360, Val Loss: 0.0390, Val MAE: 0.1934, Val RMSE: 0.2757, Val SMAPE: 76.27%\n",
      "Epoch [21/50], Train Loss: 0.0192, Val Loss: 0.0190, Val MAE: 0.1430, Val RMSE: 0.1943, Val SMAPE: 58.45%\n",
      "Early stopping triggered after 21 epochs\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-0.143   \u001b[39m | \u001b[39m283.5    \u001b[39m | \u001b[39m0.2961   \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.3487, Val Loss: 0.2338, Val MAE: 0.5553, Val RMSE: 0.7049, Val SMAPE: 133.69%\n",
      "Epoch [2/50], Train Loss: 0.2508, Val Loss: 0.1017, Val MAE: 0.3601, Val RMSE: 0.4521, Val SMAPE: 105.41%\n",
      "Epoch [3/50], Train Loss: 0.2254, Val Loss: 0.1147, Val MAE: 0.3940, Val RMSE: 0.4799, Val SMAPE: 100.84%\n",
      "Epoch [4/50], Train Loss: 0.2092, Val Loss: 0.0620, Val MAE: 0.2820, Val RMSE: 0.3516, Val SMAPE: 89.67%\n",
      "Epoch [5/50], Train Loss: 0.1871, Val Loss: 0.0609, Val MAE: 0.2744, Val RMSE: 0.3477, Val SMAPE: 100.03%\n",
      "Epoch [6/50], Train Loss: 0.1605, Val Loss: 0.0407, Val MAE: 0.2248, Val RMSE: 0.2846, Val SMAPE: 69.91%\n",
      "Epoch [7/50], Train Loss: 0.1520, Val Loss: 0.0410, Val MAE: 0.2227, Val RMSE: 0.2854, Val SMAPE: 72.45%\n",
      "Epoch [8/50], Train Loss: 0.1489, Val Loss: 0.0354, Val MAE: 0.2059, Val RMSE: 0.2652, Val SMAPE: 65.57%\n",
      "Epoch [9/50], Train Loss: 0.1392, Val Loss: 0.0363, Val MAE: 0.2171, Val RMSE: 0.2689, Val SMAPE: 67.05%\n",
      "Epoch [10/50], Train Loss: 0.1335, Val Loss: 0.0376, Val MAE: 0.2260, Val RMSE: 0.2738, Val SMAPE: 68.32%\n",
      "Epoch [11/50], Train Loss: 0.1196, Val Loss: 0.0365, Val MAE: 0.1957, Val RMSE: 0.2689, Val SMAPE: 64.12%\n",
      "Epoch [12/50], Train Loss: 0.1163, Val Loss: 0.0371, Val MAE: 0.2160, Val RMSE: 0.2716, Val SMAPE: 67.27%\n",
      "Epoch [13/50], Train Loss: 0.1104, Val Loss: 0.0343, Val MAE: 0.2004, Val RMSE: 0.2609, Val SMAPE: 64.00%\n",
      "Epoch [14/50], Train Loss: 0.1041, Val Loss: 0.0333, Val MAE: 0.1961, Val RMSE: 0.2571, Val SMAPE: 63.19%\n",
      "Epoch [15/50], Train Loss: 0.1010, Val Loss: 0.0369, Val MAE: 0.2256, Val RMSE: 0.2711, Val SMAPE: 68.33%\n",
      "Epoch [16/50], Train Loss: 0.0985, Val Loss: 0.0371, Val MAE: 0.2273, Val RMSE: 0.2719, Val SMAPE: 68.55%\n",
      "Epoch [17/50], Train Loss: 0.0969, Val Loss: 0.0349, Val MAE: 0.2183, Val RMSE: 0.2638, Val SMAPE: 67.17%\n",
      "Epoch [18/50], Train Loss: 0.0871, Val Loss: 0.0333, Val MAE: 0.2070, Val RMSE: 0.2573, Val SMAPE: 65.20%\n",
      "Epoch [19/50], Train Loss: 0.0867, Val Loss: 0.0323, Val MAE: 0.1931, Val RMSE: 0.2530, Val SMAPE: 62.51%\n",
      "Epoch [20/50], Train Loss: 0.0860, Val Loss: 0.0330, Val MAE: 0.1914, Val RMSE: 0.2555, Val SMAPE: 62.08%\n",
      "Epoch [21/50], Train Loss: 0.0875, Val Loss: 0.0315, Val MAE: 0.1958, Val RMSE: 0.2501, Val SMAPE: 62.95%\n",
      "Epoch [22/50], Train Loss: 0.0816, Val Loss: 0.0327, Val MAE: 0.1883, Val RMSE: 0.2542, Val SMAPE: 61.22%\n",
      "Epoch [23/50], Train Loss: 0.0770, Val Loss: 0.0306, Val MAE: 0.1833, Val RMSE: 0.2458, Val SMAPE: 60.23%\n",
      "Epoch [24/50], Train Loss: 0.0757, Val Loss: 0.0280, Val MAE: 0.1846, Val RMSE: 0.2354, Val SMAPE: 60.26%\n",
      "Epoch [25/50], Train Loss: 0.0705, Val Loss: 0.0243, Val MAE: 0.1680, Val RMSE: 0.2189, Val SMAPE: 56.31%\n",
      "Epoch [26/50], Train Loss: 0.0660, Val Loss: 0.0201, Val MAE: 0.1685, Val RMSE: 0.2003, Val SMAPE: 57.37%\n",
      "Epoch [27/50], Train Loss: 0.0621, Val Loss: 0.0114, Val MAE: 0.1246, Val RMSE: 0.1515, Val SMAPE: 47.53%\n",
      "Epoch [28/50], Train Loss: 0.0508, Val Loss: 0.0094, Val MAE: 0.1134, Val RMSE: 0.1382, Val SMAPE: 71.30%\n",
      "Epoch [29/50], Train Loss: 0.0446, Val Loss: 0.0085, Val MAE: 0.1058, Val RMSE: 0.1307, Val SMAPE: 60.01%\n",
      "Epoch [30/50], Train Loss: 0.0423, Val Loss: 0.0073, Val MAE: 0.0926, Val RMSE: 0.1204, Val SMAPE: 41.53%\n",
      "Epoch [31/50], Train Loss: 0.0397, Val Loss: 0.0067, Val MAE: 0.0879, Val RMSE: 0.1153, Val SMAPE: 40.80%\n",
      "Epoch [32/50], Train Loss: 0.0383, Val Loss: 0.0070, Val MAE: 0.0952, Val RMSE: 0.1183, Val SMAPE: 55.46%\n",
      "Epoch [33/50], Train Loss: 0.0385, Val Loss: 0.0077, Val MAE: 0.0981, Val RMSE: 0.1241, Val SMAPE: 53.34%\n",
      "Epoch [34/50], Train Loss: 0.0370, Val Loss: 0.0069, Val MAE: 0.0927, Val RMSE: 0.1170, Val SMAPE: 51.70%\n",
      "Epoch [35/50], Train Loss: 0.0360, Val Loss: 0.0072, Val MAE: 0.0967, Val RMSE: 0.1202, Val SMAPE: 56.05%\n",
      "Epoch [36/50], Train Loss: 0.0359, Val Loss: 0.0059, Val MAE: 0.0841, Val RMSE: 0.1091, Val SMAPE: 42.02%\n",
      "Epoch [37/50], Train Loss: 0.0360, Val Loss: 0.0056, Val MAE: 0.0839, Val RMSE: 0.1064, Val SMAPE: 46.48%\n",
      "Epoch [38/50], Train Loss: 0.0344, Val Loss: 0.0065, Val MAE: 0.0891, Val RMSE: 0.1136, Val SMAPE: 47.44%\n",
      "Epoch [39/50], Train Loss: 0.0345, Val Loss: 0.0069, Val MAE: 0.0951, Val RMSE: 0.1173, Val SMAPE: 55.36%\n",
      "Epoch [40/50], Train Loss: 0.0348, Val Loss: 0.0064, Val MAE: 0.0931, Val RMSE: 0.1135, Val SMAPE: 56.55%\n",
      "Epoch [41/50], Train Loss: 0.0346, Val Loss: 0.0065, Val MAE: 0.0899, Val RMSE: 0.1142, Val SMAPE: 47.39%\n",
      "Epoch [42/50], Train Loss: 0.0338, Val Loss: 0.0066, Val MAE: 0.0901, Val RMSE: 0.1151, Val SMAPE: 47.11%\n",
      "Epoch [43/50], Train Loss: 0.0334, Val Loss: 0.0066, Val MAE: 0.0922, Val RMSE: 0.1146, Val SMAPE: 53.06%\n",
      "Epoch [44/50], Train Loss: 0.0321, Val Loss: 0.0066, Val MAE: 0.0937, Val RMSE: 0.1151, Val SMAPE: 55.26%\n",
      "Epoch [45/50], Train Loss: 0.0341, Val Loss: 0.0070, Val MAE: 0.0944, Val RMSE: 0.1179, Val SMAPE: 52.84%\n",
      "Epoch [46/50], Train Loss: 0.0338, Val Loss: 0.0070, Val MAE: 0.0950, Val RMSE: 0.1184, Val SMAPE: 53.36%\n",
      "Epoch [47/50], Train Loss: 0.0324, Val Loss: 0.0071, Val MAE: 0.0948, Val RMSE: 0.1192, Val SMAPE: 51.94%\n",
      "Early stopping triggered after 47 epochs\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m-0.09482 \u001b[39m | \u001b[39m231.6    \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m8.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.0998, Val Loss: 0.0550, Val MAE: 0.2850, Val RMSE: 0.3319, Val SMAPE: 82.49%\n",
      "Epoch [2/50], Train Loss: 0.0658, Val Loss: 0.0428, Val MAE: 0.2262, Val RMSE: 0.2916, Val SMAPE: 75.67%\n",
      "Epoch [3/50], Train Loss: 0.0520, Val Loss: 0.0375, Val MAE: 0.2430, Val RMSE: 0.2739, Val SMAPE: 75.80%\n",
      "Epoch [4/50], Train Loss: 0.0344, Val Loss: 0.0114, Val MAE: 0.1255, Val RMSE: 0.1498, Val SMAPE: 51.41%\n",
      "Epoch [5/50], Train Loss: 0.0154, Val Loss: 0.0061, Val MAE: 0.0858, Val RMSE: 0.1099, Val SMAPE: 59.98%\n",
      "Epoch [6/50], Train Loss: 0.0094, Val Loss: 0.0040, Val MAE: 0.0669, Val RMSE: 0.0880, Val SMAPE: 38.50%\n",
      "Epoch [7/50], Train Loss: 0.0059, Val Loss: 0.0039, Val MAE: 0.0662, Val RMSE: 0.0870, Val SMAPE: 42.34%\n",
      "Epoch [8/50], Train Loss: 0.0050, Val Loss: 0.0026, Val MAE: 0.0532, Val RMSE: 0.0716, Val SMAPE: 31.97%\n",
      "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0024, Val MAE: 0.0529, Val RMSE: 0.0678, Val SMAPE: 29.18%\n",
      "Epoch [10/50], Train Loss: 0.0033, Val Loss: 0.0021, Val MAE: 0.0506, Val RMSE: 0.0645, Val SMAPE: 27.59%\n",
      "Epoch [11/50], Train Loss: 0.0030, Val Loss: 0.0036, Val MAE: 0.0669, Val RMSE: 0.0847, Val SMAPE: 50.79%\n",
      "Epoch [12/50], Train Loss: 0.0046, Val Loss: 0.0019, Val MAE: 0.0464, Val RMSE: 0.0603, Val SMAPE: 26.45%\n",
      "Epoch [13/50], Train Loss: 0.0032, Val Loss: 0.0027, Val MAE: 0.0557, Val RMSE: 0.0720, Val SMAPE: 29.09%\n",
      "Epoch [14/50], Train Loss: 0.0026, Val Loss: 0.0028, Val MAE: 0.0572, Val RMSE: 0.0737, Val SMAPE: 28.40%\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0017, Val MAE: 0.0445, Val RMSE: 0.0575, Val SMAPE: 24.49%\n",
      "Epoch [16/50], Train Loss: 0.0025, Val Loss: 0.0039, Val MAE: 0.0715, Val RMSE: 0.0878, Val SMAPE: 49.68%\n",
      "Epoch [17/50], Train Loss: 0.0028, Val Loss: 0.0023, Val MAE: 0.0507, Val RMSE: 0.0674, Val SMAPE: 27.03%\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0015, Val MAE: 0.0425, Val RMSE: 0.0542, Val SMAPE: 23.28%\n",
      "Epoch [19/50], Train Loss: 0.0019, Val Loss: 0.0016, Val MAE: 0.0457, Val RMSE: 0.0563, Val SMAPE: 26.58%\n",
      "Epoch [20/50], Train Loss: 0.0017, Val Loss: 0.0014, Val MAE: 0.0407, Val RMSE: 0.0528, Val SMAPE: 24.36%\n",
      "Epoch [21/50], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0378, Val RMSE: 0.0501, Val SMAPE: 21.37%\n",
      "Epoch [22/50], Train Loss: 0.0019, Val Loss: 0.0016, Val MAE: 0.0397, Val RMSE: 0.0550, Val SMAPE: 21.31%\n",
      "Epoch [23/50], Train Loss: 0.0030, Val Loss: 0.0022, Val MAE: 0.0502, Val RMSE: 0.0666, Val SMAPE: 29.70%\n",
      "Epoch [24/50], Train Loss: 0.0020, Val Loss: 0.0013, Val MAE: 0.0373, Val RMSE: 0.0495, Val SMAPE: 20.34%\n",
      "Epoch [25/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0398, Val RMSE: 0.0510, Val SMAPE: 23.36%\n",
      "Epoch [26/50], Train Loss: 0.0016, Val Loss: 0.0014, Val MAE: 0.0402, Val RMSE: 0.0524, Val SMAPE: 21.45%\n",
      "Epoch [27/50], Train Loss: 0.0017, Val Loss: 0.0015, Val MAE: 0.0401, Val RMSE: 0.0541, Val SMAPE: 23.57%\n",
      "Epoch [28/50], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0387, Val RMSE: 0.0503, Val SMAPE: 20.91%\n",
      "Epoch [29/50], Train Loss: 0.0019, Val Loss: 0.0027, Val MAE: 0.0581, Val RMSE: 0.0727, Val SMAPE: 28.48%\n",
      "Epoch [30/50], Train Loss: 0.0023, Val Loss: 0.0014, Val MAE: 0.0391, Val RMSE: 0.0515, Val SMAPE: 21.32%\n",
      "Epoch [31/50], Train Loss: 0.0017, Val Loss: 0.0016, Val MAE: 0.0433, Val RMSE: 0.0562, Val SMAPE: 33.69%\n",
      "Epoch [32/50], Train Loss: 0.0015, Val Loss: 0.0011, Val MAE: 0.0345, Val RMSE: 0.0458, Val SMAPE: 18.63%\n",
      "Epoch [33/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0394, Val RMSE: 0.0491, Val SMAPE: 22.94%\n",
      "Epoch [34/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0346, Val RMSE: 0.0455, Val SMAPE: 18.72%\n",
      "Epoch [35/50], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0344, Val RMSE: 0.0451, Val SMAPE: 19.78%\n",
      "Epoch [36/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0390, Val RMSE: 0.0487, Val SMAPE: 22.09%\n",
      "Epoch [37/50], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0344, Val RMSE: 0.0447, Val SMAPE: 18.92%\n",
      "Epoch [38/50], Train Loss: 0.0011, Val Loss: 0.0012, Val MAE: 0.0389, Val RMSE: 0.0480, Val SMAPE: 22.87%\n",
      "Epoch [39/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0346, Val RMSE: 0.0448, Val SMAPE: 18.98%\n",
      "Epoch [40/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0330, Val RMSE: 0.0436, Val SMAPE: 18.46%\n",
      "Epoch [41/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0439, Val SMAPE: 18.41%\n",
      "Epoch [42/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0354, Val RMSE: 0.0455, Val SMAPE: 19.89%\n",
      "Epoch [43/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0342, Val RMSE: 0.0443, Val SMAPE: 19.48%\n",
      "Epoch [44/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0345, Val RMSE: 0.0448, Val SMAPE: 19.05%\n",
      "Epoch [45/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0333, Val RMSE: 0.0435, Val SMAPE: 18.39%\n",
      "Epoch [46/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0438, Val SMAPE: 18.65%\n",
      "Epoch [47/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0437, Val SMAPE: 18.70%\n",
      "Epoch [48/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0334, Val RMSE: 0.0436, Val SMAPE: 18.62%\n",
      "Epoch [49/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0437, Val SMAPE: 18.67%\n",
      "Epoch [50/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0438, Val SMAPE: 18.69%\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m-0.03358 \u001b[39m | \u001b[39m84.96    \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1433, Val Loss: 0.0532, Val MAE: 0.2405, Val RMSE: 0.3260, Val SMAPE: 93.67%\n",
      "Epoch [2/50], Train Loss: 0.0599, Val Loss: 0.0455, Val MAE: 0.2232, Val RMSE: 0.3015, Val SMAPE: 84.89%\n",
      "Epoch [3/50], Train Loss: 0.0494, Val Loss: 0.0276, Val MAE: 0.1906, Val RMSE: 0.2344, Val SMAPE: 65.97%\n",
      "Epoch [4/50], Train Loss: 0.0308, Val Loss: 0.0146, Val MAE: 0.1342, Val RMSE: 0.1704, Val SMAPE: 57.20%\n",
      "Epoch [5/50], Train Loss: 0.0161, Val Loss: 0.0096, Val MAE: 0.1125, Val RMSE: 0.1377, Val SMAPE: 56.37%\n",
      "Epoch [6/50], Train Loss: 0.0115, Val Loss: 0.0058, Val MAE: 0.0833, Val RMSE: 0.1068, Val SMAPE: 46.83%\n",
      "Epoch [7/50], Train Loss: 0.0094, Val Loss: 0.0051, Val MAE: 0.0805, Val RMSE: 0.1000, Val SMAPE: 51.20%\n",
      "Epoch [8/50], Train Loss: 0.0065, Val Loss: 0.0033, Val MAE: 0.0640, Val RMSE: 0.0810, Val SMAPE: 39.05%\n",
      "Epoch [9/50], Train Loss: 0.0125, Val Loss: 0.0100, Val MAE: 0.1107, Val RMSE: 0.1414, Val SMAPE: 64.36%\n",
      "Epoch [10/50], Train Loss: 0.0110, Val Loss: 0.0040, Val MAE: 0.0706, Val RMSE: 0.0889, Val SMAPE: 49.44%\n",
      "Epoch [11/50], Train Loss: 0.0056, Val Loss: 0.0058, Val MAE: 0.0870, Val RMSE: 0.1076, Val SMAPE: 60.73%\n",
      "Epoch [12/50], Train Loss: 0.0115, Val Loss: 0.0051, Val MAE: 0.0780, Val RMSE: 0.1008, Val SMAPE: 44.36%\n",
      "Epoch [13/50], Train Loss: 0.0065, Val Loss: 0.0055, Val MAE: 0.0823, Val RMSE: 0.1045, Val SMAPE: 40.41%\n",
      "Epoch [14/50], Train Loss: 0.0042, Val Loss: 0.0034, Val MAE: 0.0630, Val RMSE: 0.0818, Val SMAPE: 30.47%\n",
      "Epoch [15/50], Train Loss: 0.0033, Val Loss: 0.0024, Val MAE: 0.0541, Val RMSE: 0.0694, Val SMAPE: 40.11%\n",
      "Epoch [16/50], Train Loss: 0.0131, Val Loss: 0.0058, Val MAE: 0.0800, Val RMSE: 0.1077, Val SMAPE: 41.54%\n",
      "Epoch [17/50], Train Loss: 0.0067, Val Loss: 0.0037, Val MAE: 0.0680, Val RMSE: 0.0860, Val SMAPE: 39.93%\n",
      "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0021, Val MAE: 0.0500, Val RMSE: 0.0652, Val SMAPE: 28.68%\n",
      "Epoch [19/50], Train Loss: 0.0024, Val Loss: 0.0016, Val MAE: 0.0414, Val RMSE: 0.0563, Val SMAPE: 22.25%\n",
      "Epoch [20/50], Train Loss: 0.0025, Val Loss: 0.0016, Val MAE: 0.0433, Val RMSE: 0.0566, Val SMAPE: 24.14%\n",
      "Epoch [21/50], Train Loss: 0.0022, Val Loss: 0.0023, Val MAE: 0.0546, Val RMSE: 0.0676, Val SMAPE: 28.40%\n",
      "Epoch [22/50], Train Loss: 0.0021, Val Loss: 0.0014, Val MAE: 0.0396, Val RMSE: 0.0521, Val SMAPE: 22.19%\n",
      "Epoch [23/50], Train Loss: 0.0018, Val Loss: 0.0013, Val MAE: 0.0393, Val RMSE: 0.0518, Val SMAPE: 21.09%\n",
      "Epoch [24/50], Train Loss: 0.0018, Val Loss: 0.0020, Val MAE: 0.0517, Val RMSE: 0.0635, Val SMAPE: 28.36%\n",
      "Epoch [25/50], Train Loss: 0.0019, Val Loss: 0.0012, Val MAE: 0.0364, Val RMSE: 0.0494, Val SMAPE: 20.21%\n",
      "Epoch [26/50], Train Loss: 0.0022, Val Loss: 0.0015, Val MAE: 0.0400, Val RMSE: 0.0547, Val SMAPE: 23.42%\n",
      "Epoch [27/50], Train Loss: 0.0024, Val Loss: 0.0015, Val MAE: 0.0421, Val RMSE: 0.0548, Val SMAPE: 22.77%\n",
      "Epoch [28/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0374, Val RMSE: 0.0505, Val SMAPE: 20.15%\n",
      "Epoch [29/50], Train Loss: 0.0016, Val Loss: 0.0012, Val MAE: 0.0370, Val RMSE: 0.0493, Val SMAPE: 21.55%\n",
      "Epoch [30/50], Train Loss: 0.0015, Val Loss: 0.0011, Val MAE: 0.0350, Val RMSE: 0.0476, Val SMAPE: 19.55%\n",
      "Epoch [31/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0369, Val RMSE: 0.0498, Val SMAPE: 23.08%\n",
      "Epoch [32/50], Train Loss: 0.0015, Val Loss: 0.0013, Val MAE: 0.0372, Val RMSE: 0.0516, Val SMAPE: 21.41%\n",
      "Epoch [33/50], Train Loss: 0.0013, Val Loss: 0.0012, Val MAE: 0.0358, Val RMSE: 0.0481, Val SMAPE: 19.28%\n",
      "Epoch [34/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0351, Val RMSE: 0.0472, Val SMAPE: 19.13%\n",
      "Epoch [35/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0338, Val RMSE: 0.0459, Val SMAPE: 17.59%\n",
      "Epoch [36/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0319, Val RMSE: 0.0449, Val SMAPE: 16.99%\n",
      "Epoch [37/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0321, Val RMSE: 0.0459, Val SMAPE: 17.32%\n",
      "Epoch [38/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0326, Val RMSE: 0.0454, Val SMAPE: 19.24%\n",
      "Epoch [39/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0316, Val RMSE: 0.0445, Val SMAPE: 17.13%\n",
      "Epoch [40/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0303, Val RMSE: 0.0427, Val SMAPE: 16.15%\n",
      "Epoch [41/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0307, Val RMSE: 0.0434, Val SMAPE: 16.25%\n",
      "Epoch [42/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0319, Val RMSE: 0.0441, Val SMAPE: 16.86%\n",
      "Epoch [43/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0304, Val RMSE: 0.0430, Val SMAPE: 15.89%\n",
      "Epoch [44/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0304, Val RMSE: 0.0433, Val SMAPE: 15.98%\n",
      "Epoch [45/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0303, Val RMSE: 0.0430, Val SMAPE: 15.83%\n",
      "Epoch [46/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0303, Val RMSE: 0.0430, Val SMAPE: 15.88%\n",
      "Epoch [47/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0303, Val RMSE: 0.0429, Val SMAPE: 15.90%\n",
      "Epoch [48/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0302, Val RMSE: 0.0428, Val SMAPE: 15.86%\n",
      "Epoch [49/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0302, Val RMSE: 0.0429, Val SMAPE: 15.86%\n",
      "Epoch [50/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0302, Val RMSE: 0.0429, Val SMAPE: 15.87%\n",
      "Early stopping triggered after 50 epochs\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m-0.03025 \u001b[39m | \u001b[35m195.4    \u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m5.457    \u001b[39m | \u001b[35m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1946, Val Loss: 0.1531, Val MAE: 0.4445, Val RMSE: 0.5555, Val SMAPE: 130.97%\n",
      "Epoch [2/50], Train Loss: 0.1357, Val Loss: 0.1009, Val MAE: 0.3552, Val RMSE: 0.4473, Val SMAPE: 111.28%\n",
      "Epoch [3/50], Train Loss: 0.0895, Val Loss: 0.0674, Val MAE: 0.2973, Val RMSE: 0.3651, Val SMAPE: 96.97%\n",
      "Epoch [4/50], Train Loss: 0.0707, Val Loss: 0.0496, Val MAE: 0.2568, Val RMSE: 0.3136, Val SMAPE: 87.00%\n",
      "Epoch [5/50], Train Loss: 0.0581, Val Loss: 0.0383, Val MAE: 0.2208, Val RMSE: 0.2749, Val SMAPE: 77.97%\n",
      "Epoch [6/50], Train Loss: 0.0501, Val Loss: 0.0362, Val MAE: 0.2228, Val RMSE: 0.2671, Val SMAPE: 76.44%\n",
      "Epoch [7/50], Train Loss: 0.0477, Val Loss: 0.0332, Val MAE: 0.2103, Val RMSE: 0.2552, Val SMAPE: 74.72%\n",
      "Epoch [8/50], Train Loss: 0.0458, Val Loss: 0.0333, Val MAE: 0.2140, Val RMSE: 0.2559, Val SMAPE: 75.23%\n",
      "Epoch [9/50], Train Loss: 0.0450, Val Loss: 0.0327, Val MAE: 0.2127, Val RMSE: 0.2534, Val SMAPE: 74.99%\n",
      "Epoch [10/50], Train Loss: 0.0440, Val Loss: 0.0350, Val MAE: 0.2242, Val RMSE: 0.2622, Val SMAPE: 76.80%\n",
      "Epoch [11/50], Train Loss: 0.0446, Val Loss: 0.0318, Val MAE: 0.2088, Val RMSE: 0.2500, Val SMAPE: 74.36%\n",
      "Epoch [12/50], Train Loss: 0.0431, Val Loss: 0.0313, Val MAE: 0.2072, Val RMSE: 0.2483, Val SMAPE: 73.79%\n",
      "Epoch [13/50], Train Loss: 0.0432, Val Loss: 0.0315, Val MAE: 0.2081, Val RMSE: 0.2491, Val SMAPE: 74.20%\n",
      "Epoch [14/50], Train Loss: 0.0438, Val Loss: 0.0311, Val MAE: 0.2065, Val RMSE: 0.2473, Val SMAPE: 73.94%\n",
      "Epoch [15/50], Train Loss: 0.0419, Val Loss: 0.0295, Val MAE: 0.1978, Val RMSE: 0.2410, Val SMAPE: 72.16%\n",
      "Epoch [16/50], Train Loss: 0.0424, Val Loss: 0.0323, Val MAE: 0.2168, Val RMSE: 0.2526, Val SMAPE: 75.06%\n",
      "Epoch [17/50], Train Loss: 0.0402, Val Loss: 0.0268, Val MAE: 0.1827, Val RMSE: 0.2303, Val SMAPE: 68.28%\n",
      "Epoch [18/50], Train Loss: 0.0383, Val Loss: 0.0236, Val MAE: 0.1770, Val RMSE: 0.2158, Val SMAPE: 66.99%\n",
      "Epoch [19/50], Train Loss: 0.0314, Val Loss: 0.0141, Val MAE: 0.1341, Val RMSE: 0.1670, Val SMAPE: 56.09%\n",
      "Epoch [20/50], Train Loss: 0.0193, Val Loss: 0.0061, Val MAE: 0.0871, Val RMSE: 0.1108, Val SMAPE: 61.56%\n",
      "Epoch [21/50], Train Loss: 0.0117, Val Loss: 0.0036, Val MAE: 0.0674, Val RMSE: 0.0857, Val SMAPE: 46.70%\n",
      "Epoch [22/50], Train Loss: 0.0095, Val Loss: 0.0035, Val MAE: 0.0663, Val RMSE: 0.0837, Val SMAPE: 48.89%\n",
      "Epoch [23/50], Train Loss: 0.0092, Val Loss: 0.0032, Val MAE: 0.0623, Val RMSE: 0.0802, Val SMAPE: 40.66%\n",
      "Epoch [24/50], Train Loss: 0.0084, Val Loss: 0.0026, Val MAE: 0.0565, Val RMSE: 0.0727, Val SMAPE: 32.04%\n",
      "Epoch [25/50], Train Loss: 0.0087, Val Loss: 0.0024, Val MAE: 0.0549, Val RMSE: 0.0703, Val SMAPE: 30.23%\n",
      "Epoch [26/50], Train Loss: 0.0083, Val Loss: 0.0027, Val MAE: 0.0577, Val RMSE: 0.0744, Val SMAPE: 30.88%\n",
      "Epoch [27/50], Train Loss: 0.0079, Val Loss: 0.0024, Val MAE: 0.0538, Val RMSE: 0.0695, Val SMAPE: 29.88%\n",
      "Epoch [28/50], Train Loss: 0.0075, Val Loss: 0.0023, Val MAE: 0.0538, Val RMSE: 0.0693, Val SMAPE: 28.77%\n",
      "Epoch [29/50], Train Loss: 0.0080, Val Loss: 0.0031, Val MAE: 0.0613, Val RMSE: 0.0785, Val SMAPE: 38.63%\n",
      "Epoch [30/50], Train Loss: 0.0076, Val Loss: 0.0029, Val MAE: 0.0592, Val RMSE: 0.0770, Val SMAPE: 35.04%\n",
      "Epoch [31/50], Train Loss: 0.0071, Val Loss: 0.0026, Val MAE: 0.0565, Val RMSE: 0.0727, Val SMAPE: 34.44%\n",
      "Epoch [32/50], Train Loss: 0.0072, Val Loss: 0.0029, Val MAE: 0.0595, Val RMSE: 0.0757, Val SMAPE: 37.41%\n",
      "Epoch [33/50], Train Loss: 0.0075, Val Loss: 0.0024, Val MAE: 0.0535, Val RMSE: 0.0693, Val SMAPE: 30.77%\n",
      "Epoch [34/50], Train Loss: 0.0072, Val Loss: 0.0024, Val MAE: 0.0544, Val RMSE: 0.0701, Val SMAPE: 31.32%\n",
      "Epoch [35/50], Train Loss: 0.0069, Val Loss: 0.0022, Val MAE: 0.0516, Val RMSE: 0.0667, Val SMAPE: 28.65%\n",
      "Epoch [36/50], Train Loss: 0.0071, Val Loss: 0.0021, Val MAE: 0.0500, Val RMSE: 0.0651, Val SMAPE: 28.06%\n",
      "Epoch [37/50], Train Loss: 0.0066, Val Loss: 0.0020, Val MAE: 0.0490, Val RMSE: 0.0642, Val SMAPE: 26.96%\n",
      "Epoch [38/50], Train Loss: 0.0067, Val Loss: 0.0021, Val MAE: 0.0501, Val RMSE: 0.0651, Val SMAPE: 27.88%\n",
      "Epoch [39/50], Train Loss: 0.0068, Val Loss: 0.0021, Val MAE: 0.0502, Val RMSE: 0.0655, Val SMAPE: 28.07%\n",
      "Epoch [40/50], Train Loss: 0.0067, Val Loss: 0.0021, Val MAE: 0.0499, Val RMSE: 0.0650, Val SMAPE: 27.61%\n",
      "Epoch [41/50], Train Loss: 0.0065, Val Loss: 0.0021, Val MAE: 0.0497, Val RMSE: 0.0646, Val SMAPE: 27.71%\n",
      "Epoch [42/50], Train Loss: 0.0066, Val Loss: 0.0020, Val MAE: 0.0490, Val RMSE: 0.0639, Val SMAPE: 26.98%\n",
      "Epoch [43/50], Train Loss: 0.0064, Val Loss: 0.0020, Val MAE: 0.0491, Val RMSE: 0.0642, Val SMAPE: 27.42%\n",
      "Epoch [44/50], Train Loss: 0.0066, Val Loss: 0.0020, Val MAE: 0.0483, Val RMSE: 0.0634, Val SMAPE: 26.75%\n",
      "Epoch [45/50], Train Loss: 0.0063, Val Loss: 0.0020, Val MAE: 0.0481, Val RMSE: 0.0631, Val SMAPE: 26.54%\n",
      "Epoch [46/50], Train Loss: 0.0065, Val Loss: 0.0020, Val MAE: 0.0483, Val RMSE: 0.0633, Val SMAPE: 26.77%\n",
      "Epoch [47/50], Train Loss: 0.0067, Val Loss: 0.0020, Val MAE: 0.0482, Val RMSE: 0.0632, Val SMAPE: 26.70%\n",
      "Epoch [48/50], Train Loss: 0.0064, Val Loss: 0.0020, Val MAE: 0.0482, Val RMSE: 0.0631, Val SMAPE: 26.68%\n",
      "Epoch [49/50], Train Loss: 0.0065, Val Loss: 0.0020, Val MAE: 0.0481, Val RMSE: 0.0630, Val SMAPE: 26.60%\n",
      "Epoch [50/50], Train Loss: 0.0066, Val Loss: 0.0020, Val MAE: 0.0481, Val RMSE: 0.0630, Val SMAPE: 26.60%\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-0.04807 \u001b[39m | \u001b[39m131.3    \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m8.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1151, Val Loss: 0.1127, Val MAE: 0.3770, Val RMSE: 0.4755, Val SMAPE: 120.90%\n",
      "Epoch [2/50], Train Loss: 0.1027, Val Loss: 0.0987, Val MAE: 0.3506, Val RMSE: 0.4446, Val SMAPE: 116.26%\n",
      "Epoch [3/50], Train Loss: 0.0893, Val Loss: 0.0803, Val MAE: 0.3140, Val RMSE: 0.4007, Val SMAPE: 109.12%\n",
      "Epoch [4/50], Train Loss: 0.0702, Val Loss: 0.0614, Val MAE: 0.2731, Val RMSE: 0.3502, Val SMAPE: 93.00%\n",
      "Epoch [5/50], Train Loss: 0.0562, Val Loss: 0.0480, Val MAE: 0.2420, Val RMSE: 0.3096, Val SMAPE: 77.99%\n",
      "Epoch [6/50], Train Loss: 0.0462, Val Loss: 0.0426, Val MAE: 0.2346, Val RMSE: 0.2917, Val SMAPE: 74.22%\n",
      "Epoch [7/50], Train Loss: 0.0434, Val Loss: 0.0410, Val MAE: 0.2365, Val RMSE: 0.2863, Val SMAPE: 73.66%\n",
      "Epoch [8/50], Train Loss: 0.0432, Val Loss: 0.0393, Val MAE: 0.2280, Val RMSE: 0.2800, Val SMAPE: 72.52%\n",
      "Epoch [9/50], Train Loss: 0.0409, Val Loss: 0.0388, Val MAE: 0.2239, Val RMSE: 0.2782, Val SMAPE: 71.93%\n",
      "Epoch [10/50], Train Loss: 0.0406, Val Loss: 0.0383, Val MAE: 0.2246, Val RMSE: 0.2765, Val SMAPE: 72.02%\n",
      "Epoch [11/50], Train Loss: 0.0409, Val Loss: 0.0381, Val MAE: 0.2236, Val RMSE: 0.2758, Val SMAPE: 71.80%\n",
      "Epoch [12/50], Train Loss: 0.0410, Val Loss: 0.0378, Val MAE: 0.2243, Val RMSE: 0.2746, Val SMAPE: 71.88%\n",
      "Epoch [13/50], Train Loss: 0.0404, Val Loss: 0.0378, Val MAE: 0.2235, Val RMSE: 0.2747, Val SMAPE: 71.74%\n",
      "Epoch [14/50], Train Loss: 0.0401, Val Loss: 0.0376, Val MAE: 0.2241, Val RMSE: 0.2740, Val SMAPE: 71.84%\n",
      "Epoch [15/50], Train Loss: 0.0396, Val Loss: 0.0376, Val MAE: 0.2244, Val RMSE: 0.2738, Val SMAPE: 71.86%\n",
      "Epoch [16/50], Train Loss: 0.0397, Val Loss: 0.0375, Val MAE: 0.2221, Val RMSE: 0.2733, Val SMAPE: 71.51%\n",
      "Epoch [17/50], Train Loss: 0.0394, Val Loss: 0.0374, Val MAE: 0.2219, Val RMSE: 0.2731, Val SMAPE: 71.46%\n",
      "Epoch [18/50], Train Loss: 0.0397, Val Loss: 0.0373, Val MAE: 0.2219, Val RMSE: 0.2728, Val SMAPE: 71.43%\n",
      "Epoch [19/50], Train Loss: 0.0393, Val Loss: 0.0373, Val MAE: 0.2200, Val RMSE: 0.2726, Val SMAPE: 71.19%\n",
      "Epoch [20/50], Train Loss: 0.0389, Val Loss: 0.0372, Val MAE: 0.2219, Val RMSE: 0.2723, Val SMAPE: 71.44%\n",
      "Epoch [21/50], Train Loss: 0.0393, Val Loss: 0.0372, Val MAE: 0.2201, Val RMSE: 0.2721, Val SMAPE: 71.19%\n",
      "Epoch [22/50], Train Loss: 0.0391, Val Loss: 0.0371, Val MAE: 0.2203, Val RMSE: 0.2718, Val SMAPE: 71.19%\n",
      "Epoch [23/50], Train Loss: 0.0388, Val Loss: 0.0371, Val MAE: 0.2179, Val RMSE: 0.2719, Val SMAPE: 70.84%\n",
      "Epoch [24/50], Train Loss: 0.0384, Val Loss: 0.0369, Val MAE: 0.2204, Val RMSE: 0.2711, Val SMAPE: 71.17%\n",
      "Epoch [25/50], Train Loss: 0.0378, Val Loss: 0.0367, Val MAE: 0.2194, Val RMSE: 0.2704, Val SMAPE: 70.99%\n",
      "Epoch [26/50], Train Loss: 0.0374, Val Loss: 0.0364, Val MAE: 0.2195, Val RMSE: 0.2695, Val SMAPE: 70.93%\n",
      "Epoch [27/50], Train Loss: 0.0374, Val Loss: 0.0360, Val MAE: 0.2163, Val RMSE: 0.2680, Val SMAPE: 70.32%\n",
      "Epoch [28/50], Train Loss: 0.0373, Val Loss: 0.0354, Val MAE: 0.2144, Val RMSE: 0.2657, Val SMAPE: 69.90%\n",
      "Epoch [29/50], Train Loss: 0.0366, Val Loss: 0.0342, Val MAE: 0.2130, Val RMSE: 0.2613, Val SMAPE: 69.42%\n",
      "Epoch [30/50], Train Loss: 0.0349, Val Loss: 0.0322, Val MAE: 0.2013, Val RMSE: 0.2534, Val SMAPE: 66.85%\n",
      "Epoch [31/50], Train Loss: 0.0318, Val Loss: 0.0282, Val MAE: 0.1866, Val RMSE: 0.2373, Val SMAPE: 63.20%\n",
      "Epoch [32/50], Train Loss: 0.0285, Val Loss: 0.0219, Val MAE: 0.1576, Val RMSE: 0.2083, Val SMAPE: 56.74%\n",
      "Epoch [33/50], Train Loss: 0.0220, Val Loss: 0.0147, Val MAE: 0.1291, Val RMSE: 0.1701, Val SMAPE: 51.22%\n",
      "Epoch [34/50], Train Loss: 0.0157, Val Loss: 0.0100, Val MAE: 0.1073, Val RMSE: 0.1400, Val SMAPE: 53.91%\n",
      "Epoch [35/50], Train Loss: 0.0134, Val Loss: 0.0076, Val MAE: 0.0954, Val RMSE: 0.1217, Val SMAPE: 41.80%\n",
      "Epoch [36/50], Train Loss: 0.0113, Val Loss: 0.0057, Val MAE: 0.0839, Val RMSE: 0.1050, Val SMAPE: 38.93%\n",
      "Epoch [37/50], Train Loss: 0.0103, Val Loss: 0.0052, Val MAE: 0.0777, Val RMSE: 0.0999, Val SMAPE: 38.62%\n",
      "Epoch [38/50], Train Loss: 0.0093, Val Loss: 0.0042, Val MAE: 0.0739, Val RMSE: 0.0910, Val SMAPE: 36.39%\n",
      "Epoch [39/50], Train Loss: 0.0085, Val Loss: 0.0038, Val MAE: 0.0695, Val RMSE: 0.0860, Val SMAPE: 35.22%\n",
      "Epoch [40/50], Train Loss: 0.0079, Val Loss: 0.0036, Val MAE: 0.0678, Val RMSE: 0.0835, Val SMAPE: 34.72%\n",
      "Epoch [41/50], Train Loss: 0.0077, Val Loss: 0.0034, Val MAE: 0.0661, Val RMSE: 0.0820, Val SMAPE: 34.48%\n",
      "Epoch [42/50], Train Loss: 0.0074, Val Loss: 0.0033, Val MAE: 0.0651, Val RMSE: 0.0809, Val SMAPE: 34.42%\n",
      "Epoch [43/50], Train Loss: 0.0071, Val Loss: 0.0033, Val MAE: 0.0654, Val RMSE: 0.0803, Val SMAPE: 34.32%\n",
      "Epoch [44/50], Train Loss: 0.0075, Val Loss: 0.0032, Val MAE: 0.0643, Val RMSE: 0.0791, Val SMAPE: 34.20%\n",
      "Epoch [45/50], Train Loss: 0.0074, Val Loss: 0.0032, Val MAE: 0.0638, Val RMSE: 0.0788, Val SMAPE: 34.16%\n",
      "Epoch [46/50], Train Loss: 0.0071, Val Loss: 0.0031, Val MAE: 0.0637, Val RMSE: 0.0784, Val SMAPE: 34.03%\n",
      "Epoch [47/50], Train Loss: 0.0072, Val Loss: 0.0031, Val MAE: 0.0637, Val RMSE: 0.0783, Val SMAPE: 33.99%\n",
      "Epoch [48/50], Train Loss: 0.0071, Val Loss: 0.0031, Val MAE: 0.0636, Val RMSE: 0.0782, Val SMAPE: 33.98%\n",
      "Epoch [49/50], Train Loss: 0.0073, Val Loss: 0.0031, Val MAE: 0.0636, Val RMSE: 0.0782, Val SMAPE: 33.97%\n",
      "Epoch [50/50], Train Loss: 0.0073, Val Loss: 0.0031, Val MAE: 0.0636, Val RMSE: 0.0782, Val SMAPE: 33.97%\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-0.06361 \u001b[39m | \u001b[39m75.5     \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m8.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 1.1170, Val Loss: 0.2287, Val MAE: 0.5669, Val RMSE: 0.6944, Val SMAPE: 118.47%\n",
      "Epoch [2/50], Train Loss: 0.3114, Val Loss: 0.1567, Val MAE: 0.4423, Val RMSE: 0.5660, Val SMAPE: 102.89%\n",
      "Epoch [3/50], Train Loss: 0.2274, Val Loss: 0.1672, Val MAE: 0.4571, Val RMSE: 0.5837, Val SMAPE: 119.54%\n",
      "Epoch [4/50], Train Loss: 0.1931, Val Loss: 0.0673, Val MAE: 0.3058, Val RMSE: 0.3668, Val SMAPE: 94.50%\n",
      "Epoch [5/50], Train Loss: 0.1428, Val Loss: 0.0645, Val MAE: 0.2930, Val RMSE: 0.3595, Val SMAPE: 93.27%\n",
      "Epoch [6/50], Train Loss: 0.1157, Val Loss: 0.0668, Val MAE: 0.2888, Val RMSE: 0.3667, Val SMAPE: 105.36%\n",
      "Epoch [7/50], Train Loss: 0.1009, Val Loss: 0.0739, Val MAE: 0.3040, Val RMSE: 0.3858, Val SMAPE: 105.99%\n",
      "Epoch [8/50], Train Loss: 0.0930, Val Loss: 0.0530, Val MAE: 0.2660, Val RMSE: 0.3256, Val SMAPE: 77.62%\n",
      "Epoch [9/50], Train Loss: 0.0784, Val Loss: 0.0686, Val MAE: 0.3109, Val RMSE: 0.3688, Val SMAPE: 80.10%\n",
      "Epoch [10/50], Train Loss: 0.0644, Val Loss: 0.0476, Val MAE: 0.2348, Val RMSE: 0.3100, Val SMAPE: 72.70%\n",
      "Epoch [11/50], Train Loss: 0.0799, Val Loss: 0.0663, Val MAE: 0.2721, Val RMSE: 0.3663, Val SMAPE: 98.58%\n",
      "Epoch [12/50], Train Loss: 0.0530, Val Loss: 0.0627, Val MAE: 0.2643, Val RMSE: 0.3561, Val SMAPE: 90.82%\n",
      "Epoch [13/50], Train Loss: 0.0565, Val Loss: 0.0638, Val MAE: 0.3081, Val RMSE: 0.3557, Val SMAPE: 80.42%\n",
      "Epoch [14/50], Train Loss: 0.0525, Val Loss: 0.0523, Val MAE: 0.2752, Val RMSE: 0.3223, Val SMAPE: 76.61%\n",
      "Epoch [15/50], Train Loss: 0.0462, Val Loss: 0.0473, Val MAE: 0.2351, Val RMSE: 0.3090, Val SMAPE: 72.72%\n",
      "Epoch [16/50], Train Loss: 0.0445, Val Loss: 0.0489, Val MAE: 0.2682, Val RMSE: 0.3119, Val SMAPE: 75.77%\n",
      "Epoch [17/50], Train Loss: 0.0429, Val Loss: 0.0451, Val MAE: 0.2593, Val RMSE: 0.2993, Val SMAPE: 74.54%\n",
      "Epoch [18/50], Train Loss: 0.0404, Val Loss: 0.0432, Val MAE: 0.2352, Val RMSE: 0.2946, Val SMAPE: 72.24%\n",
      "Epoch [19/50], Train Loss: 0.0419, Val Loss: 0.0420, Val MAE: 0.2386, Val RMSE: 0.2903, Val SMAPE: 72.46%\n",
      "Epoch [20/50], Train Loss: 0.0392, Val Loss: 0.0429, Val MAE: 0.2352, Val RMSE: 0.2938, Val SMAPE: 72.30%\n",
      "Epoch [21/50], Train Loss: 0.0388, Val Loss: 0.0448, Val MAE: 0.2330, Val RMSE: 0.3005, Val SMAPE: 72.07%\n",
      "Epoch [22/50], Train Loss: 0.0383, Val Loss: 0.0429, Val MAE: 0.2495, Val RMSE: 0.2923, Val SMAPE: 73.45%\n",
      "Epoch [23/50], Train Loss: 0.0384, Val Loss: 0.0463, Val MAE: 0.2323, Val RMSE: 0.3059, Val SMAPE: 72.08%\n",
      "Epoch [24/50], Train Loss: 0.0371, Val Loss: 0.0419, Val MAE: 0.2433, Val RMSE: 0.2894, Val SMAPE: 72.80%\n",
      "Epoch [25/50], Train Loss: 0.0380, Val Loss: 0.0433, Val MAE: 0.2348, Val RMSE: 0.2953, Val SMAPE: 72.23%\n",
      "Epoch [26/50], Train Loss: 0.0374, Val Loss: 0.0424, Val MAE: 0.2367, Val RMSE: 0.2916, Val SMAPE: 72.38%\n",
      "Epoch [27/50], Train Loss: 0.0367, Val Loss: 0.0438, Val MAE: 0.2341, Val RMSE: 0.2969, Val SMAPE: 72.22%\n",
      "Epoch [28/50], Train Loss: 0.0372, Val Loss: 0.0452, Val MAE: 0.2327, Val RMSE: 0.3019, Val SMAPE: 72.01%\n",
      "Epoch [29/50], Train Loss: 0.0368, Val Loss: 0.0422, Val MAE: 0.2369, Val RMSE: 0.2909, Val SMAPE: 72.37%\n",
      "Epoch [30/50], Train Loss: 0.0366, Val Loss: 0.0422, Val MAE: 0.2370, Val RMSE: 0.2911, Val SMAPE: 72.39%\n",
      "Epoch [31/50], Train Loss: 0.0376, Val Loss: 0.0433, Val MAE: 0.2346, Val RMSE: 0.2950, Val SMAPE: 72.28%\n",
      "Epoch [32/50], Train Loss: 0.0364, Val Loss: 0.0420, Val MAE: 0.2376, Val RMSE: 0.2901, Val SMAPE: 72.43%\n",
      "Epoch [33/50], Train Loss: 0.0360, Val Loss: 0.0430, Val MAE: 0.2350, Val RMSE: 0.2940, Val SMAPE: 72.30%\n",
      "Epoch [34/50], Train Loss: 0.0387, Val Loss: 0.0423, Val MAE: 0.2439, Val RMSE: 0.2907, Val SMAPE: 72.98%\n",
      "Early stopping triggered after 34 epochs\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-0.2439  \u001b[39m | \u001b[39m489.5    \u001b[39m | \u001b[39m0.3641   \u001b[39m | \u001b[39m0.007728 \u001b[39m | \u001b[39m15.37    \u001b[39m | \u001b[39m4.874    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1236, Val Loss: 0.0695, Val MAE: 0.2855, Val RMSE: 0.3707, Val SMAPE: 97.49%\n",
      "Epoch [2/50], Train Loss: 0.0749, Val Loss: 0.0488, Val MAE: 0.2453, Val RMSE: 0.3109, Val SMAPE: 77.90%\n",
      "Epoch [3/50], Train Loss: 0.0580, Val Loss: 0.0332, Val MAE: 0.2175, Val RMSE: 0.2565, Val SMAPE: 68.15%\n",
      "Epoch [4/50], Train Loss: 0.0262, Val Loss: 0.0177, Val MAE: 0.1533, Val RMSE: 0.1874, Val SMAPE: 74.53%\n",
      "Epoch [5/50], Train Loss: 0.0162, Val Loss: 0.0103, Val MAE: 0.1068, Val RMSE: 0.1430, Val SMAPE: 49.91%\n",
      "Epoch [6/50], Train Loss: 0.0109, Val Loss: 0.0074, Val MAE: 0.0960, Val RMSE: 0.1214, Val SMAPE: 46.97%\n",
      "Epoch [7/50], Train Loss: 0.0069, Val Loss: 0.0041, Val MAE: 0.0702, Val RMSE: 0.0904, Val SMAPE: 43.83%\n",
      "Epoch [8/50], Train Loss: 0.0054, Val Loss: 0.0088, Val MAE: 0.1076, Val RMSE: 0.1327, Val SMAPE: 41.66%\n",
      "Epoch [9/50], Train Loss: 0.0052, Val Loss: 0.0035, Val MAE: 0.0651, Val RMSE: 0.0828, Val SMAPE: 48.03%\n",
      "Epoch [10/50], Train Loss: 0.0058, Val Loss: 0.0077, Val MAE: 0.0974, Val RMSE: 0.1237, Val SMAPE: 62.40%\n",
      "Epoch [11/50], Train Loss: 0.0061, Val Loss: 0.0034, Val MAE: 0.0651, Val RMSE: 0.0828, Val SMAPE: 37.44%\n",
      "Epoch [12/50], Train Loss: 0.0052, Val Loss: 0.0046, Val MAE: 0.0794, Val RMSE: 0.0961, Val SMAPE: 35.43%\n",
      "Epoch [13/50], Train Loss: 0.0065, Val Loss: 0.0041, Val MAE: 0.0717, Val RMSE: 0.0905, Val SMAPE: 35.98%\n",
      "Epoch [14/50], Train Loss: 0.0044, Val Loss: 0.0051, Val MAE: 0.0847, Val RMSE: 0.1010, Val SMAPE: 39.04%\n",
      "Epoch [15/50], Train Loss: 0.0042, Val Loss: 0.0034, Val MAE: 0.0682, Val RMSE: 0.0826, Val SMAPE: 33.68%\n",
      "Epoch [16/50], Train Loss: 0.0043, Val Loss: 0.0030, Val MAE: 0.0591, Val RMSE: 0.0775, Val SMAPE: 30.68%\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0023, Val MAE: 0.0553, Val RMSE: 0.0683, Val SMAPE: 27.73%\n",
      "Epoch [18/50], Train Loss: 0.0028, Val Loss: 0.0028, Val MAE: 0.0554, Val RMSE: 0.0749, Val SMAPE: 28.48%\n",
      "Epoch [19/50], Train Loss: 0.0025, Val Loss: 0.0020, Val MAE: 0.0484, Val RMSE: 0.0636, Val SMAPE: 24.81%\n",
      "Epoch [20/50], Train Loss: 0.0030, Val Loss: 0.0041, Val MAE: 0.0768, Val RMSE: 0.0905, Val SMAPE: 36.66%\n",
      "Epoch [21/50], Train Loss: 0.0026, Val Loss: 0.0031, Val MAE: 0.0644, Val RMSE: 0.0788, Val SMAPE: 30.57%\n",
      "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0020, Val MAE: 0.0512, Val RMSE: 0.0634, Val SMAPE: 26.62%\n",
      "Epoch [23/50], Train Loss: 0.0030, Val Loss: 0.0027, Val MAE: 0.0557, Val RMSE: 0.0738, Val SMAPE: 28.81%\n",
      "Epoch [24/50], Train Loss: 0.0020, Val Loss: 0.0025, Val MAE: 0.0520, Val RMSE: 0.0711, Val SMAPE: 24.76%\n",
      "Epoch [25/50], Train Loss: 0.0020, Val Loss: 0.0018, Val MAE: 0.0475, Val RMSE: 0.0595, Val SMAPE: 25.67%\n",
      "Epoch [26/50], Train Loss: 0.0024, Val Loss: 0.0020, Val MAE: 0.0486, Val RMSE: 0.0628, Val SMAPE: 25.93%\n",
      "Epoch [27/50], Train Loss: 0.0022, Val Loss: 0.0022, Val MAE: 0.0555, Val RMSE: 0.0671, Val SMAPE: 29.13%\n",
      "Epoch [28/50], Train Loss: 0.0021, Val Loss: 0.0016, Val MAE: 0.0437, Val RMSE: 0.0565, Val SMAPE: 24.66%\n",
      "Epoch [29/50], Train Loss: 0.0017, Val Loss: 0.0015, Val MAE: 0.0418, Val RMSE: 0.0539, Val SMAPE: 22.58%\n",
      "Epoch [30/50], Train Loss: 0.0016, Val Loss: 0.0015, Val MAE: 0.0419, Val RMSE: 0.0551, Val SMAPE: 22.06%\n",
      "Epoch [31/50], Train Loss: 0.0016, Val Loss: 0.0014, Val MAE: 0.0419, Val RMSE: 0.0533, Val SMAPE: 21.19%\n",
      "Epoch [32/50], Train Loss: 0.0015, Val Loss: 0.0013, Val MAE: 0.0406, Val RMSE: 0.0513, Val SMAPE: 21.64%\n",
      "Epoch [33/50], Train Loss: 0.0015, Val Loss: 0.0016, Val MAE: 0.0450, Val RMSE: 0.0561, Val SMAPE: 23.75%\n",
      "Epoch [34/50], Train Loss: 0.0016, Val Loss: 0.0015, Val MAE: 0.0430, Val RMSE: 0.0551, Val SMAPE: 23.55%\n",
      "Epoch [35/50], Train Loss: 0.0015, Val Loss: 0.0017, Val MAE: 0.0480, Val RMSE: 0.0577, Val SMAPE: 25.67%\n",
      "Epoch [36/50], Train Loss: 0.0015, Val Loss: 0.0014, Val MAE: 0.0402, Val RMSE: 0.0519, Val SMAPE: 21.66%\n",
      "Epoch [37/50], Train Loss: 0.0013, Val Loss: 0.0014, Val MAE: 0.0429, Val RMSE: 0.0535, Val SMAPE: 23.32%\n",
      "Epoch [38/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0396, Val RMSE: 0.0504, Val SMAPE: 21.64%\n",
      "Epoch [39/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0391, Val RMSE: 0.0506, Val SMAPE: 21.51%\n",
      "Epoch [40/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0390, Val RMSE: 0.0511, Val SMAPE: 21.19%\n",
      "Epoch [41/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0385, Val RMSE: 0.0504, Val SMAPE: 20.58%\n",
      "Epoch [42/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0409, Val RMSE: 0.0513, Val SMAPE: 22.60%\n",
      "Epoch [43/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0395, Val RMSE: 0.0507, Val SMAPE: 21.76%\n",
      "Epoch [44/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0386, Val RMSE: 0.0497, Val SMAPE: 20.93%\n",
      "Epoch [45/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0398, Val RMSE: 0.0502, Val SMAPE: 21.76%\n",
      "Epoch [46/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0389, Val RMSE: 0.0498, Val SMAPE: 21.24%\n",
      "Epoch [47/50], Train Loss: 0.0011, Val Loss: 0.0012, Val MAE: 0.0389, Val RMSE: 0.0499, Val SMAPE: 21.29%\n",
      "Epoch [48/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0396, Val RMSE: 0.0502, Val SMAPE: 21.80%\n",
      "Epoch [49/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0397, Val RMSE: 0.0503, Val SMAPE: 21.91%\n",
      "Epoch [50/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0397, Val RMSE: 0.0503, Val SMAPE: 21.91%\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-0.03974 \u001b[39m | \u001b[39m150.1    \u001b[39m | \u001b[39m0.4624   \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.4924, Val Loss: 0.1483, Val MAE: 0.4441, Val RMSE: 0.5451, Val SMAPE: 104.40%\n",
      "Epoch [2/50], Train Loss: 0.1722, Val Loss: 0.1552, Val MAE: 0.4584, Val RMSE: 0.5632, Val SMAPE: 141.22%\n",
      "Epoch [3/50], Train Loss: 0.1004, Val Loss: 0.0605, Val MAE: 0.2707, Val RMSE: 0.3503, Val SMAPE: 87.01%\n",
      "Epoch [4/50], Train Loss: 0.0746, Val Loss: 0.0394, Val MAE: 0.2391, Val RMSE: 0.2811, Val SMAPE: 73.25%\n",
      "Epoch [5/50], Train Loss: 0.0371, Val Loss: 0.0167, Val MAE: 0.1488, Val RMSE: 0.1829, Val SMAPE: 67.92%\n",
      "Epoch [6/50], Train Loss: 0.0201, Val Loss: 0.0107, Val MAE: 0.1192, Val RMSE: 0.1467, Val SMAPE: 54.21%\n",
      "Epoch [7/50], Train Loss: 0.0148, Val Loss: 0.0067, Val MAE: 0.0905, Val RMSE: 0.1162, Val SMAPE: 38.13%\n",
      "Epoch [8/50], Train Loss: 0.0331, Val Loss: 0.0482, Val MAE: 0.2615, Val RMSE: 0.3108, Val SMAPE: 93.37%\n",
      "Epoch [9/50], Train Loss: 0.0345, Val Loss: 0.0313, Val MAE: 0.1787, Val RMSE: 0.2504, Val SMAPE: 75.83%\n",
      "Epoch [10/50], Train Loss: 0.0493, Val Loss: 0.0348, Val MAE: 0.2269, Val RMSE: 0.2642, Val SMAPE: 89.04%\n",
      "Epoch [11/50], Train Loss: 0.0172, Val Loss: 0.0103, Val MAE: 0.1144, Val RMSE: 0.1442, Val SMAPE: 61.65%\n",
      "Epoch [12/50], Train Loss: 0.0079, Val Loss: 0.0035, Val MAE: 0.0672, Val RMSE: 0.0844, Val SMAPE: 37.77%\n",
      "Epoch [13/50], Train Loss: 0.0107, Val Loss: 0.0052, Val MAE: 0.0846, Val RMSE: 0.1025, Val SMAPE: 49.78%\n",
      "Epoch [14/50], Train Loss: 0.0102, Val Loss: 0.0626, Val MAE: 0.3120, Val RMSE: 0.3545, Val SMAPE: 75.00%\n",
      "Epoch [15/50], Train Loss: 0.0226, Val Loss: 0.0061, Val MAE: 0.0885, Val RMSE: 0.1111, Val SMAPE: 36.55%\n",
      "Epoch [16/50], Train Loss: 0.0143, Val Loss: 0.0170, Val MAE: 0.1437, Val RMSE: 0.1849, Val SMAPE: 75.19%\n",
      "Epoch [17/50], Train Loss: 0.0070, Val Loss: 0.0037, Val MAE: 0.0658, Val RMSE: 0.0865, Val SMAPE: 33.18%\n",
      "Epoch [18/50], Train Loss: 0.0039, Val Loss: 0.0034, Val MAE: 0.0660, Val RMSE: 0.0827, Val SMAPE: 29.68%\n",
      "Epoch [19/50], Train Loss: 0.0034, Val Loss: 0.0130, Val MAE: 0.1344, Val RMSE: 0.1611, Val SMAPE: 48.60%\n",
      "Epoch [20/50], Train Loss: 0.0076, Val Loss: 0.0037, Val MAE: 0.0669, Val RMSE: 0.0860, Val SMAPE: 32.74%\n",
      "Epoch [21/50], Train Loss: 0.0046, Val Loss: 0.0114, Val MAE: 0.1191, Val RMSE: 0.1512, Val SMAPE: 64.55%\n",
      "Epoch [22/50], Train Loss: 0.0054, Val Loss: 0.0027, Val MAE: 0.0554, Val RMSE: 0.0732, Val SMAPE: 26.61%\n",
      "Epoch [23/50], Train Loss: 0.0034, Val Loss: 0.0052, Val MAE: 0.0803, Val RMSE: 0.1022, Val SMAPE: 36.42%\n",
      "Epoch [24/50], Train Loss: 0.0033, Val Loss: 0.0023, Val MAE: 0.0505, Val RMSE: 0.0677, Val SMAPE: 23.53%\n",
      "Epoch [25/50], Train Loss: 0.0048, Val Loss: 0.0086, Val MAE: 0.0962, Val RMSE: 0.1324, Val SMAPE: 47.60%\n",
      "Epoch [26/50], Train Loss: 0.0095, Val Loss: 0.0086, Val MAE: 0.1010, Val RMSE: 0.1320, Val SMAPE: 52.14%\n",
      "Epoch [27/50], Train Loss: 0.0064, Val Loss: 0.0051, Val MAE: 0.0763, Val RMSE: 0.1020, Val SMAPE: 35.15%\n",
      "Epoch [28/50], Train Loss: 0.0040, Val Loss: 0.0034, Val MAE: 0.0617, Val RMSE: 0.0823, Val SMAPE: 27.00%\n",
      "Epoch [29/50], Train Loss: 0.0033, Val Loss: 0.0042, Val MAE: 0.0726, Val RMSE: 0.0913, Val SMAPE: 33.96%\n",
      "Epoch [30/50], Train Loss: 0.0031, Val Loss: 0.0021, Val MAE: 0.0499, Val RMSE: 0.0651, Val SMAPE: 30.61%\n",
      "Epoch [31/50], Train Loss: 0.0024, Val Loss: 0.0017, Val MAE: 0.0443, Val RMSE: 0.0582, Val SMAPE: 22.84%\n",
      "Epoch [32/50], Train Loss: 0.0021, Val Loss: 0.0018, Val MAE: 0.0461, Val RMSE: 0.0601, Val SMAPE: 22.41%\n",
      "Epoch [33/50], Train Loss: 0.0019, Val Loss: 0.0018, Val MAE: 0.0463, Val RMSE: 0.0601, Val SMAPE: 21.98%\n",
      "Epoch [34/50], Train Loss: 0.0018, Val Loss: 0.0014, Val MAE: 0.0400, Val RMSE: 0.0529, Val SMAPE: 20.33%\n",
      "Epoch [35/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0383, Val RMSE: 0.0507, Val SMAPE: 20.03%\n",
      "Epoch [36/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0400, Val RMSE: 0.0522, Val SMAPE: 20.08%\n",
      "Epoch [37/50], Train Loss: 0.0015, Val Loss: 0.0013, Val MAE: 0.0402, Val RMSE: 0.0521, Val SMAPE: 19.71%\n",
      "Epoch [38/50], Train Loss: 0.0014, Val Loss: 0.0014, Val MAE: 0.0407, Val RMSE: 0.0532, Val SMAPE: 20.16%\n",
      "Epoch [39/50], Train Loss: 0.0015, Val Loss: 0.0013, Val MAE: 0.0393, Val RMSE: 0.0515, Val SMAPE: 19.65%\n",
      "Epoch [40/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0390, Val RMSE: 0.0513, Val SMAPE: 19.46%\n",
      "Epoch [41/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0391, Val RMSE: 0.0511, Val SMAPE: 19.68%\n",
      "Epoch [42/50], Train Loss: 0.0014, Val Loss: 0.0011, Val MAE: 0.0365, Val RMSE: 0.0478, Val SMAPE: 18.51%\n",
      "Epoch [43/50], Train Loss: 0.0013, Val Loss: 0.0012, Val MAE: 0.0378, Val RMSE: 0.0500, Val SMAPE: 18.83%\n",
      "Epoch [44/50], Train Loss: 0.0013, Val Loss: 0.0011, Val MAE: 0.0363, Val RMSE: 0.0476, Val SMAPE: 18.41%\n",
      "Epoch [45/50], Train Loss: 0.0013, Val Loss: 0.0011, Val MAE: 0.0365, Val RMSE: 0.0480, Val SMAPE: 18.59%\n",
      "Epoch [46/50], Train Loss: 0.0013, Val Loss: 0.0012, Val MAE: 0.0367, Val RMSE: 0.0483, Val SMAPE: 18.64%\n",
      "Epoch [47/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0365, Val RMSE: 0.0478, Val SMAPE: 18.58%\n",
      "Epoch [48/50], Train Loss: 0.0013, Val Loss: 0.0011, Val MAE: 0.0364, Val RMSE: 0.0477, Val SMAPE: 18.57%\n",
      "Epoch [49/50], Train Loss: 0.0013, Val Loss: 0.0011, Val MAE: 0.0364, Val RMSE: 0.0478, Val SMAPE: 18.56%\n",
      "Epoch [50/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0364, Val RMSE: 0.0478, Val SMAPE: 18.56%\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-0.03642 \u001b[39m | \u001b[39m380.2    \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.6288, Val Loss: 0.2140, Val MAE: 0.5014, Val RMSE: 0.6741, Val SMAPE: 102.95%\n",
      "Epoch [2/50], Train Loss: 0.2639, Val Loss: 0.0736, Val MAE: 0.3266, Val RMSE: 0.3847, Val SMAPE: 84.26%\n",
      "Epoch [3/50], Train Loss: 0.2293, Val Loss: 0.0655, Val MAE: 0.3018, Val RMSE: 0.3630, Val SMAPE: 82.70%\n",
      "Epoch [4/50], Train Loss: 0.1932, Val Loss: 0.0463, Val MAE: 0.2347, Val RMSE: 0.3066, Val SMAPE: 87.30%\n",
      "Epoch [5/50], Train Loss: 0.1448, Val Loss: 0.0463, Val MAE: 0.2536, Val RMSE: 0.3059, Val SMAPE: 75.17%\n",
      "Epoch [6/50], Train Loss: 0.1123, Val Loss: 0.0567, Val MAE: 0.2788, Val RMSE: 0.3383, Val SMAPE: 95.86%\n",
      "Epoch [7/50], Train Loss: 0.0967, Val Loss: 0.0323, Val MAE: 0.2061, Val RMSE: 0.2563, Val SMAPE: 68.18%\n",
      "Epoch [8/50], Train Loss: 0.0819, Val Loss: 0.0381, Val MAE: 0.1992, Val RMSE: 0.2785, Val SMAPE: 77.12%\n",
      "Epoch [9/50], Train Loss: 0.0737, Val Loss: 0.0354, Val MAE: 0.2122, Val RMSE: 0.2683, Val SMAPE: 69.81%\n",
      "Epoch [10/50], Train Loss: 0.0619, Val Loss: 0.0451, Val MAE: 0.2608, Val RMSE: 0.3019, Val SMAPE: 77.04%\n",
      "Epoch [11/50], Train Loss: 0.0574, Val Loss: 0.0292, Val MAE: 0.1792, Val RMSE: 0.2444, Val SMAPE: 63.20%\n",
      "Epoch [12/50], Train Loss: 0.0527, Val Loss: 0.0309, Val MAE: 0.1999, Val RMSE: 0.2511, Val SMAPE: 67.46%\n",
      "Epoch [13/50], Train Loss: 0.0501, Val Loss: 0.0324, Val MAE: 0.2203, Val RMSE: 0.2566, Val SMAPE: 71.61%\n",
      "Epoch [14/50], Train Loss: 0.0471, Val Loss: 0.0380, Val MAE: 0.2442, Val RMSE: 0.2775, Val SMAPE: 75.46%\n",
      "Epoch [15/50], Train Loss: 0.0442, Val Loss: 0.0343, Val MAE: 0.2288, Val RMSE: 0.2638, Val SMAPE: 73.01%\n",
      "Epoch [16/50], Train Loss: 0.0437, Val Loss: 0.0307, Val MAE: 0.2121, Val RMSE: 0.2500, Val SMAPE: 70.21%\n",
      "Epoch [17/50], Train Loss: 0.0438, Val Loss: 0.0403, Val MAE: 0.2534, Val RMSE: 0.2857, Val SMAPE: 76.90%\n",
      "Epoch [18/50], Train Loss: 0.0436, Val Loss: 0.0365, Val MAE: 0.2370, Val RMSE: 0.2719, Val SMAPE: 74.25%\n",
      "Epoch [19/50], Train Loss: 0.0437, Val Loss: 0.0379, Val MAE: 0.2446, Val RMSE: 0.2772, Val SMAPE: 75.59%\n",
      "Epoch [20/50], Train Loss: 0.0413, Val Loss: 0.0446, Val MAE: 0.2690, Val RMSE: 0.3000, Val SMAPE: 79.22%\n",
      "Epoch [21/50], Train Loss: 0.0429, Val Loss: 0.0328, Val MAE: 0.2229, Val RMSE: 0.2583, Val SMAPE: 72.09%\n",
      "Early stopping triggered after 21 epochs\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-0.2229  \u001b[39m | \u001b[39m383.1    \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m8.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.2185, Val Loss: 0.0814, Val MAE: 0.3436, Val RMSE: 0.4025, Val SMAPE: 102.58%\n",
      "Epoch [2/50], Train Loss: 0.1280, Val Loss: 0.0728, Val MAE: 0.2884, Val RMSE: 0.3793, Val SMAPE: 111.67%\n",
      "Epoch [3/50], Train Loss: 0.1026, Val Loss: 0.0430, Val MAE: 0.2523, Val RMSE: 0.2917, Val SMAPE: 77.40%\n",
      "Epoch [4/50], Train Loss: 0.0880, Val Loss: 0.0541, Val MAE: 0.2836, Val RMSE: 0.3278, Val SMAPE: 81.20%\n",
      "Epoch [5/50], Train Loss: 0.0746, Val Loss: 0.0645, Val MAE: 0.3171, Val RMSE: 0.3586, Val SMAPE: 85.01%\n",
      "Epoch [6/50], Train Loss: 0.0676, Val Loss: 0.0485, Val MAE: 0.2562, Val RMSE: 0.3100, Val SMAPE: 77.30%\n",
      "Epoch [7/50], Train Loss: 0.0583, Val Loss: 0.0487, Val MAE: 0.2404, Val RMSE: 0.3100, Val SMAPE: 77.73%\n",
      "Epoch [8/50], Train Loss: 0.0534, Val Loss: 0.0437, Val MAE: 0.2481, Val RMSE: 0.2940, Val SMAPE: 76.61%\n",
      "Epoch [9/50], Train Loss: 0.0499, Val Loss: 0.0428, Val MAE: 0.2161, Val RMSE: 0.2902, Val SMAPE: 71.66%\n",
      "Epoch [10/50], Train Loss: 0.0474, Val Loss: 0.0428, Val MAE: 0.2483, Val RMSE: 0.2912, Val SMAPE: 76.74%\n",
      "Epoch [11/50], Train Loss: 0.0420, Val Loss: 0.0490, Val MAE: 0.2733, Val RMSE: 0.3125, Val SMAPE: 80.16%\n",
      "Epoch [12/50], Train Loss: 0.0368, Val Loss: 0.0404, Val MAE: 0.2245, Val RMSE: 0.2821, Val SMAPE: 73.13%\n",
      "Epoch [13/50], Train Loss: 0.0423, Val Loss: 0.0408, Val MAE: 0.2265, Val RMSE: 0.2835, Val SMAPE: 73.39%\n",
      "Epoch [14/50], Train Loss: 0.0401, Val Loss: 0.0409, Val MAE: 0.2412, Val RMSE: 0.2842, Val SMAPE: 75.87%\n",
      "Epoch [15/50], Train Loss: 0.0390, Val Loss: 0.0398, Val MAE: 0.2263, Val RMSE: 0.2800, Val SMAPE: 73.58%\n",
      "Epoch [16/50], Train Loss: 0.0390, Val Loss: 0.0479, Val MAE: 0.2761, Val RMSE: 0.3086, Val SMAPE: 80.64%\n",
      "Epoch [17/50], Train Loss: 0.0399, Val Loss: 0.0416, Val MAE: 0.2120, Val RMSE: 0.2860, Val SMAPE: 70.73%\n",
      "Epoch [18/50], Train Loss: 0.0392, Val Loss: 0.0403, Val MAE: 0.2411, Val RMSE: 0.2822, Val SMAPE: 75.96%\n",
      "Epoch [19/50], Train Loss: 0.0369, Val Loss: 0.0417, Val MAE: 0.2497, Val RMSE: 0.2874, Val SMAPE: 77.13%\n",
      "Epoch [20/50], Train Loss: 0.0387, Val Loss: 0.0420, Val MAE: 0.2508, Val RMSE: 0.2883, Val SMAPE: 77.28%\n",
      "Epoch [21/50], Train Loss: 0.0368, Val Loss: 0.0413, Val MAE: 0.2466, Val RMSE: 0.2857, Val SMAPE: 76.70%\n",
      "Epoch [22/50], Train Loss: 0.0369, Val Loss: 0.0394, Val MAE: 0.2334, Val RMSE: 0.2788, Val SMAPE: 74.84%\n",
      "Epoch [23/50], Train Loss: 0.0378, Val Loss: 0.0393, Val MAE: 0.2295, Val RMSE: 0.2783, Val SMAPE: 74.20%\n",
      "Epoch [24/50], Train Loss: 0.0376, Val Loss: 0.0394, Val MAE: 0.2243, Val RMSE: 0.2784, Val SMAPE: 73.33%\n",
      "Epoch [25/50], Train Loss: 0.0370, Val Loss: 0.0431, Val MAE: 0.2564, Val RMSE: 0.2923, Val SMAPE: 78.04%\n",
      "Epoch [26/50], Train Loss: 0.0373, Val Loss: 0.0439, Val MAE: 0.2598, Val RMSE: 0.2951, Val SMAPE: 78.48%\n",
      "Epoch [27/50], Train Loss: 0.0376, Val Loss: 0.0453, Val MAE: 0.2659, Val RMSE: 0.2999, Val SMAPE: 79.30%\n",
      "Epoch [28/50], Train Loss: 0.0366, Val Loss: 0.0407, Val MAE: 0.2442, Val RMSE: 0.2837, Val SMAPE: 76.39%\n",
      "Epoch [29/50], Train Loss: 0.0369, Val Loss: 0.0403, Val MAE: 0.2416, Val RMSE: 0.2823, Val SMAPE: 76.03%\n",
      "Epoch [30/50], Train Loss: 0.0368, Val Loss: 0.0404, Val MAE: 0.2414, Val RMSE: 0.2825, Val SMAPE: 75.98%\n",
      "Epoch [31/50], Train Loss: 0.0363, Val Loss: 0.0398, Val MAE: 0.2367, Val RMSE: 0.2802, Val SMAPE: 75.31%\n",
      "Epoch [32/50], Train Loss: 0.0364, Val Loss: 0.0427, Val MAE: 0.2546, Val RMSE: 0.2910, Val SMAPE: 77.79%\n",
      "Epoch [33/50], Train Loss: 0.0364, Val Loss: 0.0395, Val MAE: 0.2352, Val RMSE: 0.2793, Val SMAPE: 75.12%\n",
      "Early stopping triggered after 33 epochs\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-0.2352  \u001b[39m | \u001b[39m199.4    \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.007161 \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m8.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1155, Val Loss: 0.0692, Val MAE: 0.3180, Val RMSE: 0.3725, Val SMAPE: 88.43%\n",
      "Epoch [2/50], Train Loss: 0.0797, Val Loss: 0.0563, Val MAE: 0.2872, Val RMSE: 0.3358, Val SMAPE: 83.22%\n",
      "Epoch [3/50], Train Loss: 0.0573, Val Loss: 0.0432, Val MAE: 0.2250, Val RMSE: 0.2938, Val SMAPE: 79.38%\n",
      "Epoch [4/50], Train Loss: 0.0497, Val Loss: 0.0346, Val MAE: 0.2237, Val RMSE: 0.2631, Val SMAPE: 74.61%\n",
      "Epoch [5/50], Train Loss: 0.0414, Val Loss: 0.0213, Val MAE: 0.1562, Val RMSE: 0.2059, Val SMAPE: 57.37%\n",
      "Epoch [6/50], Train Loss: 0.0203, Val Loss: 0.0097, Val MAE: 0.1113, Val RMSE: 0.1383, Val SMAPE: 64.28%\n",
      "Epoch [7/50], Train Loss: 0.0111, Val Loss: 0.0052, Val MAE: 0.0816, Val RMSE: 0.1019, Val SMAPE: 56.60%\n",
      "Epoch [8/50], Train Loss: 0.0065, Val Loss: 0.0028, Val MAE: 0.0581, Val RMSE: 0.0744, Val SMAPE: 38.48%\n",
      "Epoch [9/50], Train Loss: 0.0048, Val Loss: 0.0022, Val MAE: 0.0501, Val RMSE: 0.0661, Val SMAPE: 28.18%\n",
      "Epoch [10/50], Train Loss: 0.0041, Val Loss: 0.0031, Val MAE: 0.0581, Val RMSE: 0.0784, Val SMAPE: 33.27%\n",
      "Epoch [11/50], Train Loss: 0.0036, Val Loss: 0.0023, Val MAE: 0.0543, Val RMSE: 0.0672, Val SMAPE: 30.53%\n",
      "Epoch [12/50], Train Loss: 0.0033, Val Loss: 0.0029, Val MAE: 0.0599, Val RMSE: 0.0755, Val SMAPE: 39.96%\n",
      "Epoch [13/50], Train Loss: 0.0028, Val Loss: 0.0016, Val MAE: 0.0417, Val RMSE: 0.0559, Val SMAPE: 23.92%\n",
      "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0021, Val MAE: 0.0531, Val RMSE: 0.0650, Val SMAPE: 30.69%\n",
      "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0015, Val MAE: 0.0434, Val RMSE: 0.0552, Val SMAPE: 25.60%\n",
      "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0014, Val MAE: 0.0401, Val RMSE: 0.0518, Val SMAPE: 24.37%\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0012, Val MAE: 0.0372, Val RMSE: 0.0486, Val SMAPE: 23.71%\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0014, Val MAE: 0.0402, Val RMSE: 0.0522, Val SMAPE: 24.85%\n",
      "Epoch [19/50], Train Loss: 0.0020, Val Loss: 0.0013, Val MAE: 0.0388, Val RMSE: 0.0511, Val SMAPE: 23.71%\n",
      "Epoch [20/50], Train Loss: 0.0018, Val Loss: 0.0012, Val MAE: 0.0370, Val RMSE: 0.0481, Val SMAPE: 23.09%\n",
      "Epoch [21/50], Train Loss: 0.0019, Val Loss: 0.0019, Val MAE: 0.0438, Val RMSE: 0.0609, Val SMAPE: 23.83%\n",
      "Epoch [22/50], Train Loss: 0.0019, Val Loss: 0.0018, Val MAE: 0.0465, Val RMSE: 0.0604, Val SMAPE: 25.65%\n",
      "Epoch [23/50], Train Loss: 0.0019, Val Loss: 0.0012, Val MAE: 0.0373, Val RMSE: 0.0488, Val SMAPE: 22.96%\n",
      "Epoch [24/50], Train Loss: 0.0017, Val Loss: 0.0012, Val MAE: 0.0376, Val RMSE: 0.0489, Val SMAPE: 23.22%\n",
      "Epoch [25/50], Train Loss: 0.0016, Val Loss: 0.0017, Val MAE: 0.0441, Val RMSE: 0.0583, Val SMAPE: 24.89%\n",
      "Epoch [26/50], Train Loss: 0.0016, Val Loss: 0.0014, Val MAE: 0.0404, Val RMSE: 0.0519, Val SMAPE: 28.88%\n",
      "Epoch [27/50], Train Loss: 0.0016, Val Loss: 0.0011, Val MAE: 0.0364, Val RMSE: 0.0472, Val SMAPE: 23.14%\n",
      "Epoch [28/50], Train Loss: 0.0016, Val Loss: 0.0011, Val MAE: 0.0351, Val RMSE: 0.0459, Val SMAPE: 21.62%\n",
      "Epoch [29/50], Train Loss: 0.0016, Val Loss: 0.0011, Val MAE: 0.0354, Val RMSE: 0.0467, Val SMAPE: 21.82%\n",
      "Epoch [30/50], Train Loss: 0.0015, Val Loss: 0.0011, Val MAE: 0.0353, Val RMSE: 0.0464, Val SMAPE: 22.50%\n",
      "Epoch [31/50], Train Loss: 0.0014, Val Loss: 0.0010, Val MAE: 0.0344, Val RMSE: 0.0447, Val SMAPE: 20.71%\n",
      "Epoch [32/50], Train Loss: 0.0015, Val Loss: 0.0014, Val MAE: 0.0424, Val RMSE: 0.0528, Val SMAPE: 25.95%\n",
      "Epoch [33/50], Train Loss: 0.0014, Val Loss: 0.0010, Val MAE: 0.0341, Val RMSE: 0.0446, Val SMAPE: 21.00%\n",
      "Epoch [34/50], Train Loss: 0.0013, Val Loss: 0.0015, Val MAE: 0.0410, Val RMSE: 0.0543, Val SMAPE: 23.23%\n",
      "Epoch [35/50], Train Loss: 0.0015, Val Loss: 0.0010, Val MAE: 0.0342, Val RMSE: 0.0444, Val SMAPE: 21.33%\n",
      "Epoch [36/50], Train Loss: 0.0014, Val Loss: 0.0011, Val MAE: 0.0351, Val RMSE: 0.0465, Val SMAPE: 21.25%\n",
      "Epoch [37/50], Train Loss: 0.0014, Val Loss: 0.0009, Val MAE: 0.0329, Val RMSE: 0.0429, Val SMAPE: 20.39%\n",
      "Epoch [38/50], Train Loss: 0.0013, Val Loss: 0.0010, Val MAE: 0.0350, Val RMSE: 0.0450, Val SMAPE: 21.52%\n",
      "Epoch [39/50], Train Loss: 0.0012, Val Loss: 0.0009, Val MAE: 0.0328, Val RMSE: 0.0432, Val SMAPE: 20.20%\n",
      "Epoch [40/50], Train Loss: 0.0012, Val Loss: 0.0009, Val MAE: 0.0324, Val RMSE: 0.0425, Val SMAPE: 19.84%\n",
      "Epoch [41/50], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0345, Val RMSE: 0.0443, Val SMAPE: 21.59%\n",
      "Epoch [42/50], Train Loss: 0.0012, Val Loss: 0.0009, Val MAE: 0.0325, Val RMSE: 0.0425, Val SMAPE: 19.87%\n",
      "Epoch [43/50], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0342, Val RMSE: 0.0442, Val SMAPE: 21.00%\n",
      "Epoch [44/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0325, Val RMSE: 0.0424, Val SMAPE: 20.02%\n",
      "Epoch [45/50], Train Loss: 0.0012, Val Loss: 0.0009, Val MAE: 0.0329, Val RMSE: 0.0429, Val SMAPE: 20.13%\n",
      "Epoch [46/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0332, Val RMSE: 0.0433, Val SMAPE: 20.21%\n",
      "Epoch [47/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0325, Val RMSE: 0.0425, Val SMAPE: 19.88%\n",
      "Epoch [48/50], Train Loss: 0.0012, Val Loss: 0.0009, Val MAE: 0.0324, Val RMSE: 0.0424, Val SMAPE: 19.91%\n",
      "Epoch [49/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0326, Val RMSE: 0.0425, Val SMAPE: 19.99%\n",
      "Epoch [50/50], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0326, Val RMSE: 0.0426, Val SMAPE: 20.01%\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-0.03261 \u001b[39m | \u001b[39m74.64    \u001b[39m | \u001b[39m0.4081   \u001b[39m | \u001b[39m0.006857 \u001b[39m | \u001b[39m3.501    \u001b[39m | \u001b[39m1.713    \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1461, Val Loss: 0.0917, Val MAE: 0.3559, Val RMSE: 0.4263, Val SMAPE: 87.54%\n",
      "Epoch [2/50], Train Loss: 0.0803, Val Loss: 0.0617, Val MAE: 0.2645, Val RMSE: 0.3485, Val SMAPE: 100.13%\n",
      "Epoch [3/50], Train Loss: 0.0651, Val Loss: 0.0446, Val MAE: 0.2540, Val RMSE: 0.2964, Val SMAPE: 74.96%\n",
      "Epoch [4/50], Train Loss: 0.0517, Val Loss: 0.0310, Val MAE: 0.1933, Val RMSE: 0.2467, Val SMAPE: 65.84%\n",
      "Epoch [5/50], Train Loss: 0.0252, Val Loss: 0.0111, Val MAE: 0.1193, Val RMSE: 0.1480, Val SMAPE: 71.65%\n",
      "Epoch [6/50], Train Loss: 0.0117, Val Loss: 0.0063, Val MAE: 0.0854, Val RMSE: 0.1117, Val SMAPE: 38.25%\n",
      "Epoch [7/50], Train Loss: 0.0073, Val Loss: 0.0045, Val MAE: 0.0726, Val RMSE: 0.0945, Val SMAPE: 41.81%\n",
      "Epoch [8/50], Train Loss: 0.0055, Val Loss: 0.0031, Val MAE: 0.0600, Val RMSE: 0.0790, Val SMAPE: 34.63%\n",
      "Epoch [9/50], Train Loss: 0.0044, Val Loss: 0.0024, Val MAE: 0.0518, Val RMSE: 0.0688, Val SMAPE: 27.45%\n",
      "Epoch [10/50], Train Loss: 0.0044, Val Loss: 0.0038, Val MAE: 0.0633, Val RMSE: 0.0856, Val SMAPE: 27.51%\n",
      "Epoch [11/50], Train Loss: 0.0044, Val Loss: 0.0033, Val MAE: 0.0595, Val RMSE: 0.0803, Val SMAPE: 27.59%\n",
      "Epoch [12/50], Train Loss: 0.0034, Val Loss: 0.0037, Val MAE: 0.0698, Val RMSE: 0.0855, Val SMAPE: 33.90%\n",
      "Epoch [13/50], Train Loss: 0.0035, Val Loss: 0.0032, Val MAE: 0.0637, Val RMSE: 0.0791, Val SMAPE: 31.03%\n",
      "Epoch [14/50], Train Loss: 0.0031, Val Loss: 0.0022, Val MAE: 0.0512, Val RMSE: 0.0660, Val SMAPE: 29.84%\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0020, Val MAE: 0.0481, Val RMSE: 0.0627, Val SMAPE: 24.46%\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0023, Val MAE: 0.0554, Val RMSE: 0.0680, Val SMAPE: 27.83%\n",
      "Epoch [17/50], Train Loss: 0.0037, Val Loss: 0.0035, Val MAE: 0.0648, Val RMSE: 0.0828, Val SMAPE: 38.33%\n",
      "Epoch [18/50], Train Loss: 0.0037, Val Loss: 0.0021, Val MAE: 0.0509, Val RMSE: 0.0648, Val SMAPE: 29.77%\n",
      "Epoch [19/50], Train Loss: 0.0032, Val Loss: 0.0025, Val MAE: 0.0531, Val RMSE: 0.0702, Val SMAPE: 25.71%\n",
      "Epoch [20/50], Train Loss: 0.0025, Val Loss: 0.0019, Val MAE: 0.0499, Val RMSE: 0.0618, Val SMAPE: 26.27%\n",
      "Epoch [21/50], Train Loss: 0.0019, Val Loss: 0.0016, Val MAE: 0.0446, Val RMSE: 0.0569, Val SMAPE: 23.44%\n",
      "Epoch [22/50], Train Loss: 0.0018, Val Loss: 0.0019, Val MAE: 0.0459, Val RMSE: 0.0611, Val SMAPE: 23.87%\n",
      "Epoch [23/50], Train Loss: 0.0019, Val Loss: 0.0019, Val MAE: 0.0482, Val RMSE: 0.0614, Val SMAPE: 23.76%\n",
      "Epoch [24/50], Train Loss: 0.0020, Val Loss: 0.0019, Val MAE: 0.0504, Val RMSE: 0.0619, Val SMAPE: 26.16%\n",
      "Epoch [25/50], Train Loss: 0.0022, Val Loss: 0.0019, Val MAE: 0.0493, Val RMSE: 0.0613, Val SMAPE: 26.01%\n",
      "Epoch [26/50], Train Loss: 0.0021, Val Loss: 0.0016, Val MAE: 0.0442, Val RMSE: 0.0570, Val SMAPE: 23.05%\n",
      "Epoch [27/50], Train Loss: 0.0019, Val Loss: 0.0016, Val MAE: 0.0438, Val RMSE: 0.0557, Val SMAPE: 23.33%\n",
      "Epoch [28/50], Train Loss: 0.0016, Val Loss: 0.0017, Val MAE: 0.0443, Val RMSE: 0.0582, Val SMAPE: 23.20%\n",
      "Epoch [29/50], Train Loss: 0.0016, Val Loss: 0.0015, Val MAE: 0.0427, Val RMSE: 0.0550, Val SMAPE: 22.40%\n",
      "Epoch [30/50], Train Loss: 0.0015, Val Loss: 0.0014, Val MAE: 0.0420, Val RMSE: 0.0531, Val SMAPE: 22.00%\n",
      "Epoch [31/50], Train Loss: 0.0014, Val Loss: 0.0014, Val MAE: 0.0400, Val RMSE: 0.0526, Val SMAPE: 20.86%\n",
      "Epoch [32/50], Train Loss: 0.0013, Val Loss: 0.0014, Val MAE: 0.0411, Val RMSE: 0.0534, Val SMAPE: 21.90%\n",
      "Epoch [33/50], Train Loss: 0.0014, Val Loss: 0.0014, Val MAE: 0.0412, Val RMSE: 0.0530, Val SMAPE: 22.10%\n",
      "Epoch [34/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0395, Val RMSE: 0.0516, Val SMAPE: 20.65%\n",
      "Epoch [35/50], Train Loss: 0.0013, Val Loss: 0.0014, Val MAE: 0.0411, Val RMSE: 0.0529, Val SMAPE: 21.25%\n",
      "Epoch [36/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0393, Val RMSE: 0.0514, Val SMAPE: 20.60%\n",
      "Epoch [37/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0393, Val RMSE: 0.0511, Val SMAPE: 20.86%\n",
      "Epoch [38/50], Train Loss: 0.0014, Val Loss: 0.0015, Val MAE: 0.0438, Val RMSE: 0.0545, Val SMAPE: 23.09%\n",
      "Epoch [39/50], Train Loss: 0.0014, Val Loss: 0.0014, Val MAE: 0.0412, Val RMSE: 0.0524, Val SMAPE: 21.55%\n",
      "Epoch [40/50], Train Loss: 0.0012, Val Loss: 0.0014, Val MAE: 0.0403, Val RMSE: 0.0519, Val SMAPE: 21.01%\n",
      "Epoch [41/50], Train Loss: 0.0012, Val Loss: 0.0014, Val MAE: 0.0408, Val RMSE: 0.0522, Val SMAPE: 21.14%\n",
      "Epoch [42/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0405, Val RMSE: 0.0518, Val SMAPE: 20.95%\n",
      "Epoch [43/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0384, Val RMSE: 0.0501, Val SMAPE: 20.04%\n",
      "Epoch [44/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0399, Val RMSE: 0.0511, Val SMAPE: 20.64%\n",
      "Epoch [45/50], Train Loss: 0.0013, Val Loss: 0.0013, Val MAE: 0.0388, Val RMSE: 0.0502, Val SMAPE: 20.24%\n",
      "Epoch [46/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0399, Val RMSE: 0.0511, Val SMAPE: 20.71%\n",
      "Epoch [47/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0388, Val RMSE: 0.0502, Val SMAPE: 20.18%\n",
      "Epoch [48/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0387, Val RMSE: 0.0501, Val SMAPE: 20.14%\n",
      "Epoch [49/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0388, Val RMSE: 0.0502, Val SMAPE: 20.21%\n",
      "Epoch [50/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0389, Val RMSE: 0.0502, Val SMAPE: 20.23%\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-0.03886 \u001b[39m | \u001b[39m94.39    \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m9.031    \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.0991, Val Loss: 0.0640, Val MAE: 0.3001, Val RMSE: 0.3577, Val SMAPE: 81.42%\n",
      "Epoch [2/50], Train Loss: 0.0524, Val Loss: 0.0306, Val MAE: 0.2068, Val RMSE: 0.2480, Val SMAPE: 67.30%\n",
      "Epoch [3/50], Train Loss: 0.0382, Val Loss: 0.0136, Val MAE: 0.1316, Val RMSE: 0.1664, Val SMAPE: 60.97%\n",
      "Epoch [4/50], Train Loss: 0.0168, Val Loss: 0.0086, Val MAE: 0.1055, Val RMSE: 0.1304, Val SMAPE: 67.77%\n",
      "Epoch [5/50], Train Loss: 0.0109, Val Loss: 0.0037, Val MAE: 0.0664, Val RMSE: 0.0863, Val SMAPE: 40.25%\n",
      "Epoch [6/50], Train Loss: 0.0090, Val Loss: 0.0061, Val MAE: 0.0892, Val RMSE: 0.1111, Val SMAPE: 61.02%\n",
      "Epoch [7/50], Train Loss: 0.0080, Val Loss: 0.0040, Val MAE: 0.0725, Val RMSE: 0.0898, Val SMAPE: 41.96%\n",
      "Epoch [8/50], Train Loss: 0.0075, Val Loss: 0.0130, Val MAE: 0.1254, Val RMSE: 0.1619, Val SMAPE: 47.37%\n",
      "Epoch [9/50], Train Loss: 0.0141, Val Loss: 0.0067, Val MAE: 0.0903, Val RMSE: 0.1158, Val SMAPE: 44.71%\n",
      "Epoch [10/50], Train Loss: 0.0137, Val Loss: 0.0112, Val MAE: 0.1224, Val RMSE: 0.1505, Val SMAPE: 73.97%\n",
      "Epoch [11/50], Train Loss: 0.0081, Val Loss: 0.0083, Val MAE: 0.1063, Val RMSE: 0.1291, Val SMAPE: 48.84%\n",
      "Epoch [12/50], Train Loss: 0.0050, Val Loss: 0.0033, Val MAE: 0.0642, Val RMSE: 0.0816, Val SMAPE: 42.97%\n",
      "Epoch [13/50], Train Loss: 0.0049, Val Loss: 0.0049, Val MAE: 0.0808, Val RMSE: 0.0994, Val SMAPE: 44.58%\n",
      "Epoch [14/50], Train Loss: 0.0055, Val Loss: 0.0039, Val MAE: 0.0689, Val RMSE: 0.0887, Val SMAPE: 42.29%\n",
      "Epoch [15/50], Train Loss: 0.0042, Val Loss: 0.0024, Val MAE: 0.0537, Val RMSE: 0.0690, Val SMAPE: 28.71%\n",
      "Epoch [16/50], Train Loss: 0.0026, Val Loss: 0.0026, Val MAE: 0.0556, Val RMSE: 0.0720, Val SMAPE: 27.29%\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0035, Val MAE: 0.0686, Val RMSE: 0.0832, Val SMAPE: 32.10%\n",
      "Epoch [18/50], Train Loss: 0.0029, Val Loss: 0.0022, Val MAE: 0.0525, Val RMSE: 0.0655, Val SMAPE: 40.08%\n",
      "Epoch [19/50], Train Loss: 0.0022, Val Loss: 0.0018, Val MAE: 0.0468, Val RMSE: 0.0594, Val SMAPE: 23.93%\n",
      "Epoch [20/50], Train Loss: 0.0024, Val Loss: 0.0031, Val MAE: 0.0629, Val RMSE: 0.0782, Val SMAPE: 29.66%\n",
      "Epoch [21/50], Train Loss: 0.0022, Val Loss: 0.0016, Val MAE: 0.0429, Val RMSE: 0.0561, Val SMAPE: 21.29%\n",
      "Epoch [22/50], Train Loss: 0.0018, Val Loss: 0.0017, Val MAE: 0.0444, Val RMSE: 0.0576, Val SMAPE: 22.68%\n",
      "Epoch [23/50], Train Loss: 0.0022, Val Loss: 0.0021, Val MAE: 0.0499, Val RMSE: 0.0655, Val SMAPE: 22.36%\n",
      "Epoch [24/50], Train Loss: 0.0021, Val Loss: 0.0019, Val MAE: 0.0448, Val RMSE: 0.0620, Val SMAPE: 21.28%\n",
      "Epoch [25/50], Train Loss: 0.0022, Val Loss: 0.0013, Val MAE: 0.0390, Val RMSE: 0.0502, Val SMAPE: 20.79%\n",
      "Epoch [26/50], Train Loss: 0.0021, Val Loss: 0.0028, Val MAE: 0.0567, Val RMSE: 0.0748, Val SMAPE: 24.24%\n",
      "Epoch [27/50], Train Loss: 0.0021, Val Loss: 0.0013, Val MAE: 0.0391, Val RMSE: 0.0514, Val SMAPE: 23.58%\n",
      "Epoch [28/50], Train Loss: 0.0016, Val Loss: 0.0016, Val MAE: 0.0446, Val RMSE: 0.0568, Val SMAPE: 22.44%\n",
      "Epoch [29/50], Train Loss: 0.0016, Val Loss: 0.0011, Val MAE: 0.0353, Val RMSE: 0.0469, Val SMAPE: 18.88%\n",
      "Epoch [30/50], Train Loss: 0.0013, Val Loss: 0.0011, Val MAE: 0.0358, Val RMSE: 0.0466, Val SMAPE: 18.92%\n",
      "Epoch [31/50], Train Loss: 0.0014, Val Loss: 0.0011, Val MAE: 0.0345, Val RMSE: 0.0457, Val SMAPE: 17.56%\n",
      "Epoch [32/50], Train Loss: 0.0015, Val Loss: 0.0012, Val MAE: 0.0359, Val RMSE: 0.0478, Val SMAPE: 19.05%\n",
      "Epoch [33/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0379, Val RMSE: 0.0500, Val SMAPE: 23.15%\n",
      "Epoch [34/50], Train Loss: 0.0014, Val Loss: 0.0010, Val MAE: 0.0340, Val RMSE: 0.0450, Val SMAPE: 17.40%\n",
      "Epoch [35/50], Train Loss: 0.0013, Val Loss: 0.0010, Val MAE: 0.0328, Val RMSE: 0.0442, Val SMAPE: 17.15%\n",
      "Epoch [36/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0346, Val RMSE: 0.0461, Val SMAPE: 18.09%\n",
      "Epoch [37/50], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0323, Val RMSE: 0.0437, Val SMAPE: 16.46%\n",
      "Epoch [38/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0366, Val RMSE: 0.0495, Val SMAPE: 18.09%\n",
      "Epoch [39/50], Train Loss: 0.0013, Val Loss: 0.0009, Val MAE: 0.0314, Val RMSE: 0.0425, Val SMAPE: 16.37%\n",
      "Epoch [40/50], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0318, Val RMSE: 0.0438, Val SMAPE: 15.92%\n",
      "Epoch [41/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0323, Val RMSE: 0.0438, Val SMAPE: 16.60%\n",
      "Epoch [42/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0313, Val RMSE: 0.0426, Val SMAPE: 15.59%\n",
      "Epoch [43/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0305, Val RMSE: 0.0415, Val SMAPE: 15.76%\n",
      "Epoch [44/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0320, Val RMSE: 0.0433, Val SMAPE: 16.28%\n",
      "Epoch [45/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0306, Val RMSE: 0.0418, Val SMAPE: 15.45%\n",
      "Epoch [46/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0309, Val RMSE: 0.0422, Val SMAPE: 15.29%\n",
      "Epoch [47/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0303, Val RMSE: 0.0415, Val SMAPE: 15.17%\n",
      "Epoch [48/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0301, Val RMSE: 0.0412, Val SMAPE: 15.13%\n",
      "Epoch [49/50], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0302, Val RMSE: 0.0414, Val SMAPE: 15.09%\n",
      "Epoch [50/50], Train Loss: 0.0009, Val Loss: 0.0009, Val MAE: 0.0302, Val RMSE: 0.0414, Val SMAPE: 15.09%\n",
      "| \u001b[35m26       \u001b[39m | \u001b[35m-0.03019 \u001b[39m | \u001b[35m185.1    \u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m0.008917 \u001b[39m | \u001b[35m2.0      \u001b[39m | \u001b[35m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1565, Val Loss: 0.0491, Val MAE: 0.2405, Val RMSE: 0.3132, Val SMAPE: 79.99%\n",
      "Epoch [2/50], Train Loss: 0.0636, Val Loss: 0.0600, Val MAE: 0.2610, Val RMSE: 0.3455, Val SMAPE: 107.51%\n",
      "Epoch [3/50], Train Loss: 0.0431, Val Loss: 0.0194, Val MAE: 0.1459, Val RMSE: 0.1960, Val SMAPE: 52.15%\n",
      "Epoch [4/50], Train Loss: 0.0231, Val Loss: 0.0136, Val MAE: 0.1242, Val RMSE: 0.1655, Val SMAPE: 66.83%\n",
      "Epoch [5/50], Train Loss: 0.0142, Val Loss: 0.0086, Val MAE: 0.0983, Val RMSE: 0.1305, Val SMAPE: 39.15%\n",
      "Epoch [6/50], Train Loss: 0.0109, Val Loss: 0.0051, Val MAE: 0.0810, Val RMSE: 0.1012, Val SMAPE: 53.75%\n",
      "Epoch [7/50], Train Loss: 0.0091, Val Loss: 0.0044, Val MAE: 0.0763, Val RMSE: 0.0949, Val SMAPE: 48.61%\n",
      "Epoch [8/50], Train Loss: 0.0187, Val Loss: 0.0282, Val MAE: 0.1844, Val RMSE: 0.2372, Val SMAPE: 67.67%\n",
      "Epoch [9/50], Train Loss: 0.0137, Val Loss: 0.0057, Val MAE: 0.0855, Val RMSE: 0.1066, Val SMAPE: 40.87%\n",
      "Epoch [10/50], Train Loss: 0.0092, Val Loss: 0.0160, Val MAE: 0.1439, Val RMSE: 0.1786, Val SMAPE: 59.87%\n",
      "Epoch [11/50], Train Loss: 0.0094, Val Loss: 0.0088, Val MAE: 0.1162, Val RMSE: 0.1322, Val SMAPE: 44.50%\n",
      "Epoch [12/50], Train Loss: 0.0142, Val Loss: 0.0356, Val MAE: 0.2358, Val RMSE: 0.2667, Val SMAPE: 88.40%\n",
      "Epoch [13/50], Train Loss: 0.0722, Val Loss: 0.0140, Val MAE: 0.1361, Val RMSE: 0.1676, Val SMAPE: 61.97%\n",
      "Epoch [14/50], Train Loss: 0.0191, Val Loss: 0.0141, Val MAE: 0.1283, Val RMSE: 0.1673, Val SMAPE: 44.62%\n",
      "Epoch [15/50], Train Loss: 0.0103, Val Loss: 0.0025, Val MAE: 0.0552, Val RMSE: 0.0723, Val SMAPE: 34.02%\n",
      "Epoch [16/50], Train Loss: 0.0053, Val Loss: 0.0043, Val MAE: 0.0757, Val RMSE: 0.0931, Val SMAPE: 40.70%\n",
      "Epoch [17/50], Train Loss: 0.0053, Val Loss: 0.0024, Val MAE: 0.0551, Val RMSE: 0.0695, Val SMAPE: 33.80%\n",
      "Epoch [18/50], Train Loss: 0.0041, Val Loss: 0.0028, Val MAE: 0.0625, Val RMSE: 0.0754, Val SMAPE: 29.05%\n",
      "Epoch [19/50], Train Loss: 0.0038, Val Loss: 0.0026, Val MAE: 0.0561, Val RMSE: 0.0714, Val SMAPE: 33.34%\n",
      "Epoch [20/50], Train Loss: 0.0051, Val Loss: 0.0029, Val MAE: 0.0614, Val RMSE: 0.0771, Val SMAPE: 44.13%\n",
      "Epoch [21/50], Train Loss: 0.0033, Val Loss: 0.0017, Val MAE: 0.0448, Val RMSE: 0.0584, Val SMAPE: 23.85%\n",
      "Epoch [22/50], Train Loss: 0.0029, Val Loss: 0.0017, Val MAE: 0.0461, Val RMSE: 0.0581, Val SMAPE: 24.64%\n",
      "Epoch [23/50], Train Loss: 0.0026, Val Loss: 0.0022, Val MAE: 0.0523, Val RMSE: 0.0671, Val SMAPE: 27.29%\n",
      "Epoch [24/50], Train Loss: 0.0034, Val Loss: 0.0028, Val MAE: 0.0558, Val RMSE: 0.0747, Val SMAPE: 25.39%\n",
      "Epoch [25/50], Train Loss: 0.0028, Val Loss: 0.0015, Val MAE: 0.0430, Val RMSE: 0.0553, Val SMAPE: 23.13%\n",
      "Epoch [26/50], Train Loss: 0.0031, Val Loss: 0.0024, Val MAE: 0.0542, Val RMSE: 0.0698, Val SMAPE: 42.14%\n",
      "Epoch [27/50], Train Loss: 0.0026, Val Loss: 0.0021, Val MAE: 0.0500, Val RMSE: 0.0657, Val SMAPE: 24.80%\n",
      "Epoch [28/50], Train Loss: 0.0022, Val Loss: 0.0013, Val MAE: 0.0406, Val RMSE: 0.0512, Val SMAPE: 22.98%\n",
      "Epoch [29/50], Train Loss: 0.0020, Val Loss: 0.0013, Val MAE: 0.0413, Val RMSE: 0.0511, Val SMAPE: 22.71%\n",
      "Epoch [30/50], Train Loss: 0.0023, Val Loss: 0.0021, Val MAE: 0.0474, Val RMSE: 0.0637, Val SMAPE: 22.94%\n",
      "Epoch [31/50], Train Loss: 0.0022, Val Loss: 0.0017, Val MAE: 0.0438, Val RMSE: 0.0573, Val SMAPE: 21.63%\n",
      "Epoch [32/50], Train Loss: 0.0027, Val Loss: 0.0017, Val MAE: 0.0446, Val RMSE: 0.0586, Val SMAPE: 20.47%\n",
      "Epoch [33/50], Train Loss: 0.0022, Val Loss: 0.0013, Val MAE: 0.0395, Val RMSE: 0.0516, Val SMAPE: 19.93%\n",
      "Epoch [34/50], Train Loss: 0.0019, Val Loss: 0.0014, Val MAE: 0.0391, Val RMSE: 0.0529, Val SMAPE: 18.61%\n",
      "Epoch [35/50], Train Loss: 0.0019, Val Loss: 0.0016, Val MAE: 0.0440, Val RMSE: 0.0567, Val SMAPE: 23.68%\n",
      "Epoch [36/50], Train Loss: 0.0018, Val Loss: 0.0014, Val MAE: 0.0407, Val RMSE: 0.0530, Val SMAPE: 22.64%\n",
      "Epoch [37/50], Train Loss: 0.0017, Val Loss: 0.0015, Val MAE: 0.0429, Val RMSE: 0.0555, Val SMAPE: 27.65%\n",
      "Epoch [38/50], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0388, Val RMSE: 0.0502, Val SMAPE: 20.51%\n",
      "Epoch [39/50], Train Loss: 0.0016, Val Loss: 0.0016, Val MAE: 0.0431, Val RMSE: 0.0577, Val SMAPE: 21.34%\n",
      "Epoch [40/50], Train Loss: 0.0016, Val Loss: 0.0014, Val MAE: 0.0396, Val RMSE: 0.0529, Val SMAPE: 20.85%\n",
      "Epoch [41/50], Train Loss: 0.0016, Val Loss: 0.0011, Val MAE: 0.0363, Val RMSE: 0.0471, Val SMAPE: 19.25%\n",
      "Epoch [42/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0411, Val RMSE: 0.0524, Val SMAPE: 25.81%\n",
      "Epoch [43/50], Train Loss: 0.0014, Val Loss: 0.0011, Val MAE: 0.0363, Val RMSE: 0.0476, Val SMAPE: 18.64%\n",
      "Epoch [44/50], Train Loss: 0.0014, Val Loss: 0.0011, Val MAE: 0.0361, Val RMSE: 0.0470, Val SMAPE: 19.22%\n",
      "Epoch [45/50], Train Loss: 0.0016, Val Loss: 0.0011, Val MAE: 0.0364, Val RMSE: 0.0479, Val SMAPE: 18.93%\n",
      "Epoch [46/50], Train Loss: 0.0015, Val Loss: 0.0011, Val MAE: 0.0365, Val RMSE: 0.0480, Val SMAPE: 18.72%\n",
      "Epoch [47/50], Train Loss: 0.0015, Val Loss: 0.0011, Val MAE: 0.0365, Val RMSE: 0.0475, Val SMAPE: 19.03%\n",
      "Epoch [48/50], Train Loss: 0.0013, Val Loss: 0.0011, Val MAE: 0.0361, Val RMSE: 0.0472, Val SMAPE: 18.66%\n",
      "Epoch [49/50], Train Loss: 0.0014, Val Loss: 0.0011, Val MAE: 0.0360, Val RMSE: 0.0470, Val SMAPE: 18.50%\n",
      "Epoch [50/50], Train Loss: 0.0015, Val Loss: 0.0011, Val MAE: 0.0360, Val RMSE: 0.0470, Val SMAPE: 18.48%\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-0.03597 \u001b[39m | \u001b[39m215.7    \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.00992  \u001b[39m | \u001b[39m3.664    \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.1350, Val Loss: 0.0514, Val MAE: 0.2538, Val RMSE: 0.3203, Val SMAPE: 81.54%\n",
      "Epoch [2/50], Train Loss: 0.0778, Val Loss: 0.0555, Val MAE: 0.2725, Val RMSE: 0.3335, Val SMAPE: 78.47%\n",
      "Epoch [3/50], Train Loss: 0.0635, Val Loss: 0.0395, Val MAE: 0.2212, Val RMSE: 0.2815, Val SMAPE: 70.39%\n",
      "Epoch [4/50], Train Loss: 0.0501, Val Loss: 0.0230, Val MAE: 0.1778, Val RMSE: 0.2151, Val SMAPE: 60.54%\n",
      "Epoch [5/50], Train Loss: 0.0236, Val Loss: 0.0126, Val MAE: 0.1301, Val RMSE: 0.1587, Val SMAPE: 58.36%\n",
      "Epoch [6/50], Train Loss: 0.0147, Val Loss: 0.0065, Val MAE: 0.0909, Val RMSE: 0.1151, Val SMAPE: 44.73%\n",
      "Epoch [7/50], Train Loss: 0.0076, Val Loss: 0.0047, Val MAE: 0.0753, Val RMSE: 0.0970, Val SMAPE: 41.14%\n",
      "Epoch [8/50], Train Loss: 0.0059, Val Loss: 0.0070, Val MAE: 0.0956, Val RMSE: 0.1188, Val SMAPE: 48.14%\n",
      "Epoch [9/50], Train Loss: 0.0073, Val Loss: 0.0044, Val MAE: 0.0754, Val RMSE: 0.0940, Val SMAPE: 46.07%\n",
      "Epoch [10/50], Train Loss: 0.0078, Val Loss: 0.0062, Val MAE: 0.0939, Val RMSE: 0.1117, Val SMAPE: 64.40%\n",
      "Epoch [11/50], Train Loss: 0.0053, Val Loss: 0.0038, Val MAE: 0.0707, Val RMSE: 0.0875, Val SMAPE: 50.12%\n",
      "Epoch [12/50], Train Loss: 0.0072, Val Loss: 0.0092, Val MAE: 0.1109, Val RMSE: 0.1357, Val SMAPE: 66.58%\n",
      "Epoch [13/50], Train Loss: 0.0099, Val Loss: 0.0076, Val MAE: 0.1038, Val RMSE: 0.1235, Val SMAPE: 51.98%\n",
      "Epoch [14/50], Train Loss: 0.0062, Val Loss: 0.0036, Val MAE: 0.0664, Val RMSE: 0.0848, Val SMAPE: 32.87%\n",
      "Epoch [15/50], Train Loss: 0.0038, Val Loss: 0.0035, Val MAE: 0.0668, Val RMSE: 0.0837, Val SMAPE: 33.80%\n",
      "Epoch [16/50], Train Loss: 0.0038, Val Loss: 0.0022, Val MAE: 0.0526, Val RMSE: 0.0669, Val SMAPE: 32.14%\n",
      "Epoch [17/50], Train Loss: 0.0034, Val Loss: 0.0024, Val MAE: 0.0544, Val RMSE: 0.0703, Val SMAPE: 30.94%\n",
      "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0021, Val MAE: 0.0521, Val RMSE: 0.0646, Val SMAPE: 28.02%\n",
      "Epoch [19/50], Train Loss: 0.0030, Val Loss: 0.0019, Val MAE: 0.0476, Val RMSE: 0.0613, Val SMAPE: 25.80%\n",
      "Epoch [20/50], Train Loss: 0.0026, Val Loss: 0.0045, Val MAE: 0.0778, Val RMSE: 0.0944, Val SMAPE: 34.53%\n",
      "Epoch [21/50], Train Loss: 0.0030, Val Loss: 0.0018, Val MAE: 0.0468, Val RMSE: 0.0598, Val SMAPE: 26.07%\n",
      "Epoch [22/50], Train Loss: 0.0024, Val Loss: 0.0020, Val MAE: 0.0501, Val RMSE: 0.0630, Val SMAPE: 26.71%\n",
      "Epoch [23/50], Train Loss: 0.0022, Val Loss: 0.0015, Val MAE: 0.0421, Val RMSE: 0.0541, Val SMAPE: 23.80%\n",
      "Epoch [24/50], Train Loss: 0.0026, Val Loss: 0.0017, Val MAE: 0.0460, Val RMSE: 0.0580, Val SMAPE: 29.88%\n",
      "Epoch [25/50], Train Loss: 0.0018, Val Loss: 0.0014, Val MAE: 0.0408, Val RMSE: 0.0523, Val SMAPE: 23.41%\n",
      "Epoch [26/50], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0410, Val RMSE: 0.0516, Val SMAPE: 23.08%\n",
      "Epoch [27/50], Train Loss: 0.0015, Val Loss: 0.0012, Val MAE: 0.0396, Val RMSE: 0.0498, Val SMAPE: 21.78%\n",
      "Epoch [28/50], Train Loss: 0.0016, Val Loss: 0.0012, Val MAE: 0.0395, Val RMSE: 0.0497, Val SMAPE: 23.30%\n",
      "Epoch [29/50], Train Loss: 0.0014, Val Loss: 0.0014, Val MAE: 0.0405, Val RMSE: 0.0532, Val SMAPE: 22.99%\n",
      "Epoch [30/50], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0390, Val RMSE: 0.0511, Val SMAPE: 21.73%\n",
      "Epoch [31/50], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0385, Val RMSE: 0.0493, Val SMAPE: 21.19%\n",
      "Epoch [32/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0371, Val RMSE: 0.0472, Val SMAPE: 21.11%\n",
      "Epoch [33/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0376, Val RMSE: 0.0493, Val SMAPE: 21.04%\n",
      "Epoch [34/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0364, Val RMSE: 0.0469, Val SMAPE: 20.61%\n",
      "Epoch [35/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0368, Val RMSE: 0.0470, Val SMAPE: 20.75%\n",
      "Epoch [36/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0359, Val RMSE: 0.0465, Val SMAPE: 20.70%\n",
      "Epoch [37/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0362, Val RMSE: 0.0466, Val SMAPE: 20.62%\n",
      "Epoch [38/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0343, Val RMSE: 0.0457, Val SMAPE: 19.11%\n",
      "Epoch [39/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0351, Val RMSE: 0.0451, Val SMAPE: 20.07%\n",
      "Epoch [40/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0344, Val RMSE: 0.0444, Val SMAPE: 19.97%\n",
      "Epoch [41/50], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0340, Val RMSE: 0.0440, Val SMAPE: 19.57%\n",
      "Epoch [42/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0343, Val RMSE: 0.0442, Val SMAPE: 19.73%\n",
      "Epoch [43/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0344, Val RMSE: 0.0446, Val SMAPE: 20.10%\n",
      "Epoch [44/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0337, Val RMSE: 0.0438, Val SMAPE: 19.36%\n",
      "Epoch [45/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0338, Val RMSE: 0.0438, Val SMAPE: 19.36%\n",
      "Epoch [46/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0339, Val RMSE: 0.0440, Val SMAPE: 19.56%\n",
      "Epoch [47/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0335, Val RMSE: 0.0440, Val SMAPE: 19.14%\n",
      "Epoch [48/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0335, Val RMSE: 0.0438, Val SMAPE: 19.18%\n",
      "Epoch [49/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0438, Val SMAPE: 19.29%\n",
      "Epoch [50/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0438, Val SMAPE: 19.30%\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-0.03365 \u001b[39m | \u001b[39m166.5    \u001b[39m | \u001b[39m0.3608   \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.0961, Val Loss: 0.0472, Val MAE: 0.2460, Val RMSE: 0.3097, Val SMAPE: 77.87%\n",
      "Epoch [2/50], Train Loss: 0.0507, Val Loss: 0.0396, Val MAE: 0.2216, Val RMSE: 0.2834, Val SMAPE: 71.97%\n",
      "Epoch [3/50], Train Loss: 0.0412, Val Loss: 0.0257, Val MAE: 0.1935, Val RMSE: 0.2275, Val SMAPE: 63.00%\n",
      "Epoch [4/50], Train Loss: 0.0198, Val Loss: 0.0158, Val MAE: 0.1449, Val RMSE: 0.1776, Val SMAPE: 78.41%\n",
      "Epoch [5/50], Train Loss: 0.0143, Val Loss: 0.0102, Val MAE: 0.1049, Val RMSE: 0.1438, Val SMAPE: 45.14%\n",
      "Epoch [6/50], Train Loss: 0.0093, Val Loss: 0.0055, Val MAE: 0.0848, Val RMSE: 0.1045, Val SMAPE: 58.55%\n",
      "Epoch [7/50], Train Loss: 0.0061, Val Loss: 0.0045, Val MAE: 0.0750, Val RMSE: 0.0956, Val SMAPE: 36.07%\n",
      "Epoch [8/50], Train Loss: 0.0053, Val Loss: 0.0051, Val MAE: 0.0798, Val RMSE: 0.1015, Val SMAPE: 37.21%\n",
      "Epoch [9/50], Train Loss: 0.0100, Val Loss: 0.0072, Val MAE: 0.1001, Val RMSE: 0.1200, Val SMAPE: 59.27%\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0031, Val MAE: 0.0610, Val RMSE: 0.0780, Val SMAPE: 37.05%\n",
      "Epoch [11/50], Train Loss: 0.0070, Val Loss: 0.0043, Val MAE: 0.0747, Val RMSE: 0.0924, Val SMAPE: 52.36%\n",
      "Epoch [12/50], Train Loss: 0.0048, Val Loss: 0.0041, Val MAE: 0.0683, Val RMSE: 0.0909, Val SMAPE: 38.27%\n",
      "Epoch [13/50], Train Loss: 0.0047, Val Loss: 0.0034, Val MAE: 0.0639, Val RMSE: 0.0821, Val SMAPE: 38.49%\n",
      "Epoch [14/50], Train Loss: 0.0039, Val Loss: 0.0025, Val MAE: 0.0561, Val RMSE: 0.0714, Val SMAPE: 27.18%\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0026, Val MAE: 0.0550, Val RMSE: 0.0718, Val SMAPE: 30.53%\n",
      "Epoch [16/50], Train Loss: 0.0021, Val Loss: 0.0029, Val MAE: 0.0594, Val RMSE: 0.0763, Val SMAPE: 25.58%\n",
      "Epoch [17/50], Train Loss: 0.0025, Val Loss: 0.0031, Val MAE: 0.0655, Val RMSE: 0.0792, Val SMAPE: 32.33%\n",
      "Epoch [18/50], Train Loss: 0.0027, Val Loss: 0.0021, Val MAE: 0.0519, Val RMSE: 0.0646, Val SMAPE: 25.43%\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0019, Val MAE: 0.0445, Val RMSE: 0.0610, Val SMAPE: 21.31%\n",
      "Epoch [20/50], Train Loss: 0.0022, Val Loss: 0.0018, Val MAE: 0.0473, Val RMSE: 0.0597, Val SMAPE: 23.34%\n",
      "Epoch [21/50], Train Loss: 0.0017, Val Loss: 0.0021, Val MAE: 0.0504, Val RMSE: 0.0650, Val SMAPE: 24.04%\n",
      "Epoch [22/50], Train Loss: 0.0016, Val Loss: 0.0014, Val MAE: 0.0421, Val RMSE: 0.0541, Val SMAPE: 21.01%\n",
      "Epoch [23/50], Train Loss: 0.0017, Val Loss: 0.0016, Val MAE: 0.0433, Val RMSE: 0.0569, Val SMAPE: 22.60%\n",
      "Epoch [24/50], Train Loss: 0.0014, Val Loss: 0.0014, Val MAE: 0.0410, Val RMSE: 0.0532, Val SMAPE: 21.14%\n",
      "Epoch [25/50], Train Loss: 0.0014, Val Loss: 0.0019, Val MAE: 0.0510, Val RMSE: 0.0625, Val SMAPE: 25.95%\n",
      "Epoch [26/50], Train Loss: 0.0014, Val Loss: 0.0014, Val MAE: 0.0424, Val RMSE: 0.0542, Val SMAPE: 22.16%\n",
      "Epoch [27/50], Train Loss: 0.0013, Val Loss: 0.0019, Val MAE: 0.0468, Val RMSE: 0.0622, Val SMAPE: 25.83%\n",
      "Epoch [28/50], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0387, Val RMSE: 0.0511, Val SMAPE: 19.76%\n",
      "Epoch [29/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0394, Val RMSE: 0.0518, Val SMAPE: 21.40%\n",
      "Epoch [30/50], Train Loss: 0.0012, Val Loss: 0.0013, Val MAE: 0.0395, Val RMSE: 0.0520, Val SMAPE: 18.80%\n",
      "Epoch [31/50], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0360, Val RMSE: 0.0483, Val SMAPE: 18.95%\n",
      "Epoch [32/50], Train Loss: 0.0011, Val Loss: 0.0012, Val MAE: 0.0370, Val RMSE: 0.0496, Val SMAPE: 18.47%\n",
      "Epoch [33/50], Train Loss: 0.0012, Val Loss: 0.0012, Val MAE: 0.0361, Val RMSE: 0.0485, Val SMAPE: 18.31%\n",
      "Epoch [34/50], Train Loss: 0.0011, Val Loss: 0.0014, Val MAE: 0.0393, Val RMSE: 0.0530, Val SMAPE: 19.96%\n",
      "Epoch [35/50], Train Loss: 0.0011, Val Loss: 0.0012, Val MAE: 0.0377, Val RMSE: 0.0504, Val SMAPE: 18.10%\n",
      "Epoch [36/50], Train Loss: 0.0011, Val Loss: 0.0011, Val MAE: 0.0355, Val RMSE: 0.0482, Val SMAPE: 17.24%\n",
      "Epoch [37/50], Train Loss: 0.0010, Val Loss: 0.0010, Val MAE: 0.0330, Val RMSE: 0.0459, Val SMAPE: 15.95%\n",
      "Epoch [38/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0347, Val RMSE: 0.0469, Val SMAPE: 17.35%\n",
      "Epoch [39/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0336, Val RMSE: 0.0460, Val SMAPE: 16.65%\n",
      "Epoch [40/50], Train Loss: 0.0009, Val Loss: 0.0011, Val MAE: 0.0341, Val RMSE: 0.0474, Val SMAPE: 17.46%\n",
      "Epoch [41/50], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0339, Val RMSE: 0.0476, Val SMAPE: 16.82%\n",
      "Epoch [42/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0335, Val RMSE: 0.0459, Val SMAPE: 16.20%\n",
      "Epoch [43/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0333, Val RMSE: 0.0459, Val SMAPE: 15.34%\n",
      "Epoch [44/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0321, Val RMSE: 0.0452, Val SMAPE: 14.83%\n",
      "Epoch [45/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0318, Val RMSE: 0.0447, Val SMAPE: 14.73%\n",
      "Epoch [46/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0317, Val RMSE: 0.0446, Val SMAPE: 14.44%\n",
      "Epoch [47/50], Train Loss: 0.0008, Val Loss: 0.0010, Val MAE: 0.0317, Val RMSE: 0.0445, Val SMAPE: 14.46%\n",
      "Epoch [48/50], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0318, Val RMSE: 0.0446, Val SMAPE: 14.51%\n",
      "Epoch [49/50], Train Loss: 0.0008, Val Loss: 0.0010, Val MAE: 0.0318, Val RMSE: 0.0446, Val SMAPE: 14.49%\n",
      "Epoch [50/50], Train Loss: 0.0008, Val Loss: 0.0010, Val MAE: 0.0318, Val RMSE: 0.0445, Val SMAPE: 14.49%\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-0.03177 \u001b[39m | \u001b[39m155.7    \u001b[39m | \u001b[39m0.1021   \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m15.7     \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "Epoch [1/50], Train Loss: 0.4672, Val Loss: 0.3495, Val MAE: 0.7153, Val RMSE: 0.8871, Val SMAPE: 152.36%\n",
      "Epoch [2/50], Train Loss: 0.2857, Val Loss: 0.1851, Val MAE: 0.4950, Val RMSE: 0.6238, Val SMAPE: 129.01%\n",
      "Epoch [3/50], Train Loss: 0.1707, Val Loss: 0.1547, Val MAE: 0.4539, Val RMSE: 0.5593, Val SMAPE: 120.36%\n",
      "Epoch [4/50], Train Loss: 0.1599, Val Loss: 0.1224, Val MAE: 0.4027, Val RMSE: 0.4948, Val SMAPE: 107.94%\n",
      "Epoch [5/50], Train Loss: 0.1022, Val Loss: 0.0718, Val MAE: 0.2927, Val RMSE: 0.3788, Val SMAPE: 102.96%\n",
      "Epoch [6/50], Train Loss: 0.0803, Val Loss: 0.0507, Val MAE: 0.2475, Val RMSE: 0.3169, Val SMAPE: 82.69%\n",
      "Epoch [7/50], Train Loss: 0.0652, Val Loss: 0.0428, Val MAE: 0.2310, Val RMSE: 0.2910, Val SMAPE: 77.01%\n",
      "Epoch [8/50], Train Loss: 0.0575, Val Loss: 0.0392, Val MAE: 0.2222, Val RMSE: 0.2788, Val SMAPE: 74.77%\n",
      "Epoch [9/50], Train Loss: 0.0514, Val Loss: 0.0397, Val MAE: 0.2352, Val RMSE: 0.2808, Val SMAPE: 76.24%\n",
      "Epoch [10/50], Train Loss: 0.0530, Val Loss: 0.0362, Val MAE: 0.2218, Val RMSE: 0.2686, Val SMAPE: 73.98%\n",
      "Epoch [11/50], Train Loss: 0.0477, Val Loss: 0.0364, Val MAE: 0.2259, Val RMSE: 0.2686, Val SMAPE: 74.20%\n",
      "Epoch [12/50], Train Loss: 0.0467, Val Loss: 0.0317, Val MAE: 0.2040, Val RMSE: 0.2508, Val SMAPE: 70.82%\n",
      "Epoch [13/50], Train Loss: 0.0426, Val Loss: 0.0263, Val MAE: 0.1836, Val RMSE: 0.2284, Val SMAPE: 65.35%\n",
      "Epoch [14/50], Train Loss: 0.0364, Val Loss: 0.0201, Val MAE: 0.1561, Val RMSE: 0.2002, Val SMAPE: 58.17%\n",
      "Epoch [15/50], Train Loss: 0.0293, Val Loss: 0.0131, Val MAE: 0.1300, Val RMSE: 0.1614, Val SMAPE: 53.39%\n",
      "Epoch [16/50], Train Loss: 0.0208, Val Loss: 0.0078, Val MAE: 0.0951, Val RMSE: 0.1246, Val SMAPE: 50.24%\n",
      "Epoch [17/50], Train Loss: 0.0179, Val Loss: 0.0046, Val MAE: 0.0747, Val RMSE: 0.0962, Val SMAPE: 43.05%\n",
      "Epoch [18/50], Train Loss: 0.0152, Val Loss: 0.0049, Val MAE: 0.0771, Val RMSE: 0.0992, Val SMAPE: 43.12%\n",
      "Epoch [19/50], Train Loss: 0.0148, Val Loss: 0.0040, Val MAE: 0.0698, Val RMSE: 0.0894, Val SMAPE: 41.38%\n",
      "Epoch [20/50], Train Loss: 0.0140, Val Loss: 0.0038, Val MAE: 0.0672, Val RMSE: 0.0869, Val SMAPE: 39.89%\n",
      "Epoch [21/50], Train Loss: 0.0128, Val Loss: 0.0035, Val MAE: 0.0650, Val RMSE: 0.0839, Val SMAPE: 37.87%\n",
      "Epoch [22/50], Train Loss: 0.0128, Val Loss: 0.0037, Val MAE: 0.0686, Val RMSE: 0.0863, Val SMAPE: 44.46%\n",
      "Epoch [23/50], Train Loss: 0.0125, Val Loss: 0.0033, Val MAE: 0.0633, Val RMSE: 0.0816, Val SMAPE: 35.39%\n",
      "Epoch [24/50], Train Loss: 0.0126, Val Loss: 0.0029, Val MAE: 0.0596, Val RMSE: 0.0767, Val SMAPE: 35.57%\n",
      "Epoch [25/50], Train Loss: 0.0119, Val Loss: 0.0034, Val MAE: 0.0645, Val RMSE: 0.0826, Val SMAPE: 36.29%\n",
      "Epoch [26/50], Train Loss: 0.0120, Val Loss: 0.0034, Val MAE: 0.0638, Val RMSE: 0.0820, Val SMAPE: 40.05%\n",
      "Epoch [27/50], Train Loss: 0.0112, Val Loss: 0.0035, Val MAE: 0.0661, Val RMSE: 0.0841, Val SMAPE: 36.39%\n",
      "Epoch [28/50], Train Loss: 0.0110, Val Loss: 0.0033, Val MAE: 0.0623, Val RMSE: 0.0813, Val SMAPE: 33.47%\n",
      "Epoch [29/50], Train Loss: 0.0107, Val Loss: 0.0027, Val MAE: 0.0577, Val RMSE: 0.0742, Val SMAPE: 35.71%\n",
      "Epoch [30/50], Train Loss: 0.0107, Val Loss: 0.0032, Val MAE: 0.0630, Val RMSE: 0.0800, Val SMAPE: 43.61%\n",
      "Epoch [31/50], Train Loss: 0.0106, Val Loss: 0.0029, Val MAE: 0.0589, Val RMSE: 0.0758, Val SMAPE: 39.47%\n",
      "Epoch [32/50], Train Loss: 0.0106, Val Loss: 0.0026, Val MAE: 0.0568, Val RMSE: 0.0725, Val SMAPE: 34.91%\n",
      "Epoch [33/50], Train Loss: 0.0102, Val Loss: 0.0026, Val MAE: 0.0567, Val RMSE: 0.0718, Val SMAPE: 36.49%\n",
      "Epoch [34/50], Train Loss: 0.0100, Val Loss: 0.0025, Val MAE: 0.0560, Val RMSE: 0.0711, Val SMAPE: 36.56%\n",
      "Epoch [35/50], Train Loss: 0.0100, Val Loss: 0.0025, Val MAE: 0.0551, Val RMSE: 0.0707, Val SMAPE: 32.65%\n",
      "Epoch [36/50], Train Loss: 0.0094, Val Loss: 0.0024, Val MAE: 0.0536, Val RMSE: 0.0688, Val SMAPE: 33.63%\n",
      "Epoch [37/50], Train Loss: 0.0098, Val Loss: 0.0023, Val MAE: 0.0532, Val RMSE: 0.0681, Val SMAPE: 35.29%\n",
      "Epoch [38/50], Train Loss: 0.0094, Val Loss: 0.0024, Val MAE: 0.0530, Val RMSE: 0.0693, Val SMAPE: 29.94%\n",
      "Epoch [39/50], Train Loss: 0.0094, Val Loss: 0.0022, Val MAE: 0.0509, Val RMSE: 0.0664, Val SMAPE: 29.05%\n",
      "Epoch [40/50], Train Loss: 0.0095, Val Loss: 0.0023, Val MAE: 0.0528, Val RMSE: 0.0678, Val SMAPE: 34.14%\n",
      "Epoch [41/50], Train Loss: 0.0094, Val Loss: 0.0023, Val MAE: 0.0525, Val RMSE: 0.0688, Val SMAPE: 29.23%\n",
      "Epoch [42/50], Train Loss: 0.0093, Val Loss: 0.0022, Val MAE: 0.0516, Val RMSE: 0.0667, Val SMAPE: 32.26%\n",
      "Epoch [43/50], Train Loss: 0.0093, Val Loss: 0.0021, Val MAE: 0.0503, Val RMSE: 0.0657, Val SMAPE: 28.97%\n",
      "Epoch [44/50], Train Loss: 0.0091, Val Loss: 0.0020, Val MAE: 0.0494, Val RMSE: 0.0641, Val SMAPE: 29.37%\n",
      "Epoch [45/50], Train Loss: 0.0090, Val Loss: 0.0021, Val MAE: 0.0497, Val RMSE: 0.0645, Val SMAPE: 29.33%\n",
      "Epoch [46/50], Train Loss: 0.0088, Val Loss: 0.0020, Val MAE: 0.0494, Val RMSE: 0.0641, Val SMAPE: 28.47%\n",
      "Epoch [47/50], Train Loss: 0.0090, Val Loss: 0.0021, Val MAE: 0.0499, Val RMSE: 0.0647, Val SMAPE: 28.63%\n",
      "Epoch [48/50], Train Loss: 0.0087, Val Loss: 0.0021, Val MAE: 0.0497, Val RMSE: 0.0645, Val SMAPE: 28.35%\n",
      "Epoch [49/50], Train Loss: 0.0086, Val Loss: 0.0020, Val MAE: 0.0496, Val RMSE: 0.0643, Val SMAPE: 28.21%\n",
      "Epoch [50/50], Train Loss: 0.0088, Val Loss: 0.0020, Val MAE: 0.0496, Val RMSE: 0.0643, Val SMAPE: 28.19%\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-0.04955 \u001b[39m | \u001b[39m365.8    \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m16.0     \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "=====================================================================================\n",
      "Best hyperparameters: {'target': -0.030187275260686874, 'params': {'d_model': 185.10680065792664, 'dropout': 0.1, 'learning_rate': 0.008917098355999721, 'nhead': 2.0, 'num_layers': 1.0}}\n",
      "Epoch [1/200], Train Loss: 0.1066, Val Loss: 0.0690, Val MAE: 0.2958, Val RMSE: 0.3669, Val SMAPE: 90.59%\n",
      "Epoch [2/200], Train Loss: 0.0574, Val Loss: 0.0408, Val MAE: 0.2341, Val RMSE: 0.2800, Val SMAPE: 73.24%\n",
      "Epoch [3/200], Train Loss: 0.0457, Val Loss: 0.0395, Val MAE: 0.2262, Val RMSE: 0.2756, Val SMAPE: 71.29%\n",
      "Epoch [4/200], Train Loss: 0.0363, Val Loss: 0.0268, Val MAE: 0.1881, Val RMSE: 0.2274, Val SMAPE: 64.04%\n",
      "Epoch [5/200], Train Loss: 0.0232, Val Loss: 0.0093, Val MAE: 0.1093, Val RMSE: 0.1352, Val SMAPE: 46.09%\n",
      "Epoch [6/200], Train Loss: 0.0125, Val Loss: 0.0084, Val MAE: 0.1039, Val RMSE: 0.1289, Val SMAPE: 62.81%\n",
      "Epoch [7/200], Train Loss: 0.0089, Val Loss: 0.0053, Val MAE: 0.0773, Val RMSE: 0.1019, Val SMAPE: 37.73%\n",
      "Epoch [8/200], Train Loss: 0.0082, Val Loss: 0.0041, Val MAE: 0.0710, Val RMSE: 0.0906, Val SMAPE: 48.20%\n",
      "Epoch [9/200], Train Loss: 0.0065, Val Loss: 0.0037, Val MAE: 0.0657, Val RMSE: 0.0856, Val SMAPE: 35.32%\n",
      "Epoch [10/200], Train Loss: 0.0057, Val Loss: 0.0031, Val MAE: 0.0604, Val RMSE: 0.0781, Val SMAPE: 30.47%\n",
      "Epoch [11/200], Train Loss: 0.0056, Val Loss: 0.0025, Val MAE: 0.0534, Val RMSE: 0.0703, Val SMAPE: 30.22%\n",
      "Epoch [12/200], Train Loss: 0.0047, Val Loss: 0.0035, Val MAE: 0.0660, Val RMSE: 0.0836, Val SMAPE: 32.61%\n",
      "Epoch [13/200], Train Loss: 0.0051, Val Loss: 0.0031, Val MAE: 0.0623, Val RMSE: 0.0789, Val SMAPE: 33.73%\n",
      "Epoch [14/200], Train Loss: 0.0043, Val Loss: 0.0034, Val MAE: 0.0645, Val RMSE: 0.0820, Val SMAPE: 43.28%\n",
      "Epoch [15/200], Train Loss: 0.0040, Val Loss: 0.0030, Val MAE: 0.0591, Val RMSE: 0.0773, Val SMAPE: 32.33%\n",
      "Epoch [16/200], Train Loss: 0.0037, Val Loss: 0.0026, Val MAE: 0.0574, Val RMSE: 0.0723, Val SMAPE: 38.43%\n",
      "Epoch [17/200], Train Loss: 0.0034, Val Loss: 0.0032, Val MAE: 0.0635, Val RMSE: 0.0798, Val SMAPE: 35.21%\n",
      "Epoch [18/200], Train Loss: 0.0042, Val Loss: 0.0026, Val MAE: 0.0574, Val RMSE: 0.0724, Val SMAPE: 37.17%\n",
      "Epoch [19/200], Train Loss: 0.0033, Val Loss: 0.0025, Val MAE: 0.0551, Val RMSE: 0.0705, Val SMAPE: 30.57%\n",
      "Epoch [20/200], Train Loss: 0.0030, Val Loss: 0.0020, Val MAE: 0.0487, Val RMSE: 0.0626, Val SMAPE: 28.67%\n",
      "Epoch [21/200], Train Loss: 0.0030, Val Loss: 0.0023, Val MAE: 0.0526, Val RMSE: 0.0676, Val SMAPE: 35.12%\n",
      "Epoch [22/200], Train Loss: 0.0032, Val Loss: 0.0046, Val MAE: 0.0805, Val RMSE: 0.0962, Val SMAPE: 37.47%\n",
      "Epoch [23/200], Train Loss: 0.0033, Val Loss: 0.0024, Val MAE: 0.0553, Val RMSE: 0.0686, Val SMAPE: 29.80%\n",
      "Epoch [24/200], Train Loss: 0.0026, Val Loss: 0.0018, Val MAE: 0.0461, Val RMSE: 0.0595, Val SMAPE: 28.12%\n",
      "Epoch [25/200], Train Loss: 0.0029, Val Loss: 0.0032, Val MAE: 0.0626, Val RMSE: 0.0788, Val SMAPE: 30.45%\n",
      "Epoch [26/200], Train Loss: 0.0028, Val Loss: 0.0023, Val MAE: 0.0539, Val RMSE: 0.0680, Val SMAPE: 39.08%\n",
      "Epoch [27/200], Train Loss: 0.0024, Val Loss: 0.0024, Val MAE: 0.0541, Val RMSE: 0.0690, Val SMAPE: 35.34%\n",
      "Epoch [28/200], Train Loss: 0.0027, Val Loss: 0.0026, Val MAE: 0.0591, Val RMSE: 0.0718, Val SMAPE: 32.98%\n",
      "Epoch [29/200], Train Loss: 0.0028, Val Loss: 0.0019, Val MAE: 0.0487, Val RMSE: 0.0613, Val SMAPE: 27.47%\n",
      "Epoch [30/200], Train Loss: 0.0024, Val Loss: 0.0018, Val MAE: 0.0471, Val RMSE: 0.0600, Val SMAPE: 27.70%\n",
      "Epoch [31/200], Train Loss: 0.0021, Val Loss: 0.0032, Val MAE: 0.0667, Val RMSE: 0.0793, Val SMAPE: 33.86%\n",
      "Epoch [32/200], Train Loss: 0.0028, Val Loss: 0.0021, Val MAE: 0.0511, Val RMSE: 0.0643, Val SMAPE: 27.33%\n",
      "Epoch [33/200], Train Loss: 0.0025, Val Loss: 0.0015, Val MAE: 0.0408, Val RMSE: 0.0545, Val SMAPE: 22.71%\n",
      "Epoch [34/200], Train Loss: 0.0018, Val Loss: 0.0018, Val MAE: 0.0449, Val RMSE: 0.0587, Val SMAPE: 23.80%\n",
      "Epoch [35/200], Train Loss: 0.0019, Val Loss: 0.0011, Val MAE: 0.0362, Val RMSE: 0.0474, Val SMAPE: 20.87%\n",
      "Epoch [36/200], Train Loss: 0.0015, Val Loss: 0.0015, Val MAE: 0.0414, Val RMSE: 0.0543, Val SMAPE: 25.82%\n",
      "Epoch [37/200], Train Loss: 0.0017, Val Loss: 0.0015, Val MAE: 0.0425, Val RMSE: 0.0541, Val SMAPE: 24.59%\n",
      "Epoch [38/200], Train Loss: 0.0021, Val Loss: 0.0016, Val MAE: 0.0437, Val RMSE: 0.0563, Val SMAPE: 24.11%\n",
      "Epoch [39/200], Train Loss: 0.0021, Val Loss: 0.0019, Val MAE: 0.0465, Val RMSE: 0.0611, Val SMAPE: 25.57%\n",
      "Epoch [40/200], Train Loss: 0.0026, Val Loss: 0.0018, Val MAE: 0.0486, Val RMSE: 0.0599, Val SMAPE: 28.31%\n",
      "Epoch [41/200], Train Loss: 0.0017, Val Loss: 0.0013, Val MAE: 0.0381, Val RMSE: 0.0497, Val SMAPE: 22.02%\n",
      "Epoch [42/200], Train Loss: 0.0016, Val Loss: 0.0019, Val MAE: 0.0481, Val RMSE: 0.0610, Val SMAPE: 27.78%\n",
      "Epoch [43/200], Train Loss: 0.0018, Val Loss: 0.0013, Val MAE: 0.0405, Val RMSE: 0.0518, Val SMAPE: 28.96%\n",
      "Epoch [44/200], Train Loss: 0.0021, Val Loss: 0.0021, Val MAE: 0.0528, Val RMSE: 0.0645, Val SMAPE: 28.00%\n",
      "Epoch [45/200], Train Loss: 0.0021, Val Loss: 0.0013, Val MAE: 0.0385, Val RMSE: 0.0503, Val SMAPE: 22.70%\n",
      "Epoch [46/200], Train Loss: 0.0016, Val Loss: 0.0016, Val MAE: 0.0402, Val RMSE: 0.0544, Val SMAPE: 21.60%\n",
      "Epoch [47/200], Train Loss: 0.0017, Val Loss: 0.0011, Val MAE: 0.0358, Val RMSE: 0.0467, Val SMAPE: 20.99%\n",
      "Epoch [48/200], Train Loss: 0.0016, Val Loss: 0.0014, Val MAE: 0.0416, Val RMSE: 0.0535, Val SMAPE: 28.98%\n",
      "Epoch [49/200], Train Loss: 0.0015, Val Loss: 0.0010, Val MAE: 0.0341, Val RMSE: 0.0448, Val SMAPE: 20.22%\n",
      "Epoch [50/200], Train Loss: 0.0014, Val Loss: 0.0013, Val MAE: 0.0378, Val RMSE: 0.0501, Val SMAPE: 23.65%\n",
      "Epoch [51/200], Train Loss: 0.0013, Val Loss: 0.0008, Val MAE: 0.0301, Val RMSE: 0.0408, Val SMAPE: 16.53%\n",
      "Epoch [52/200], Train Loss: 0.0013, Val Loss: 0.0010, Val MAE: 0.0324, Val RMSE: 0.0437, Val SMAPE: 18.04%\n",
      "Epoch [53/200], Train Loss: 0.0011, Val Loss: 0.0016, Val MAE: 0.0459, Val RMSE: 0.0560, Val SMAPE: 26.91%\n",
      "Epoch [54/200], Train Loss: 0.0012, Val Loss: 0.0016, Val MAE: 0.0438, Val RMSE: 0.0564, Val SMAPE: 28.19%\n",
      "Epoch [55/200], Train Loss: 0.0016, Val Loss: 0.0013, Val MAE: 0.0391, Val RMSE: 0.0514, Val SMAPE: 21.58%\n",
      "Epoch [56/200], Train Loss: 0.0028, Val Loss: 0.0021, Val MAE: 0.0487, Val RMSE: 0.0643, Val SMAPE: 23.38%\n",
      "Epoch [57/200], Train Loss: 0.0024, Val Loss: 0.0016, Val MAE: 0.0449, Val RMSE: 0.0569, Val SMAPE: 25.26%\n",
      "Epoch [58/200], Train Loss: 0.0014, Val Loss: 0.0012, Val MAE: 0.0348, Val RMSE: 0.0476, Val SMAPE: 18.88%\n",
      "Epoch [59/200], Train Loss: 0.0013, Val Loss: 0.0010, Val MAE: 0.0322, Val RMSE: 0.0444, Val SMAPE: 17.83%\n",
      "Epoch [60/200], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0331, Val RMSE: 0.0443, Val SMAPE: 19.29%\n",
      "Epoch [61/200], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0338, Val RMSE: 0.0448, Val SMAPE: 19.96%\n",
      "Epoch [62/200], Train Loss: 0.0011, Val Loss: 0.0008, Val MAE: 0.0290, Val RMSE: 0.0399, Val SMAPE: 16.78%\n",
      "Epoch [63/200], Train Loss: 0.0010, Val Loss: 0.0013, Val MAE: 0.0397, Val RMSE: 0.0505, Val SMAPE: 21.60%\n",
      "Epoch [64/200], Train Loss: 0.0015, Val Loss: 0.0011, Val MAE: 0.0361, Val RMSE: 0.0463, Val SMAPE: 22.50%\n",
      "Epoch [65/200], Train Loss: 0.0014, Val Loss: 0.0015, Val MAE: 0.0433, Val RMSE: 0.0551, Val SMAPE: 23.57%\n",
      "Epoch [66/200], Train Loss: 0.0012, Val Loss: 0.0010, Val MAE: 0.0313, Val RMSE: 0.0442, Val SMAPE: 16.54%\n",
      "Epoch [67/200], Train Loss: 0.0010, Val Loss: 0.0012, Val MAE: 0.0332, Val RMSE: 0.0475, Val SMAPE: 17.34%\n",
      "Epoch [68/200], Train Loss: 0.0011, Val Loss: 0.0010, Val MAE: 0.0338, Val RMSE: 0.0438, Val SMAPE: 21.15%\n",
      "Epoch [69/200], Train Loss: 0.0012, Val Loss: 0.0011, Val MAE: 0.0373, Val RMSE: 0.0467, Val SMAPE: 22.45%\n",
      "Epoch [70/200], Train Loss: 0.0012, Val Loss: 0.0009, Val MAE: 0.0319, Val RMSE: 0.0417, Val SMAPE: 22.89%\n",
      "Epoch [71/200], Train Loss: 0.0011, Val Loss: 0.0013, Val MAE: 0.0366, Val RMSE: 0.0497, Val SMAPE: 20.56%\n",
      "Epoch [72/200], Train Loss: 0.0011, Val Loss: 0.0009, Val MAE: 0.0312, Val RMSE: 0.0420, Val SMAPE: 19.88%\n",
      "Epoch [73/200], Train Loss: 0.0009, Val Loss: 0.0007, Val MAE: 0.0276, Val RMSE: 0.0378, Val SMAPE: 16.36%\n",
      "Epoch [74/200], Train Loss: 0.0009, Val Loss: 0.0007, Val MAE: 0.0263, Val RMSE: 0.0370, Val SMAPE: 14.78%\n",
      "Epoch [75/200], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0359, Val RMSE: 0.0449, Val SMAPE: 23.17%\n",
      "Epoch [76/200], Train Loss: 0.0009, Val Loss: 0.0008, Val MAE: 0.0291, Val RMSE: 0.0393, Val SMAPE: 16.72%\n",
      "Epoch [77/200], Train Loss: 0.0008, Val Loss: 0.0008, Val MAE: 0.0298, Val RMSE: 0.0398, Val SMAPE: 18.03%\n",
      "Epoch [78/200], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0310, Val RMSE: 0.0441, Val SMAPE: 15.70%\n",
      "Epoch [79/200], Train Loss: 0.0010, Val Loss: 0.0011, Val MAE: 0.0375, Val RMSE: 0.0467, Val SMAPE: 21.92%\n",
      "Epoch [80/200], Train Loss: 0.0009, Val Loss: 0.0008, Val MAE: 0.0283, Val RMSE: 0.0392, Val SMAPE: 17.71%\n",
      "Epoch [81/200], Train Loss: 0.0009, Val Loss: 0.0006, Val MAE: 0.0262, Val RMSE: 0.0359, Val SMAPE: 16.41%\n",
      "Epoch [82/200], Train Loss: 0.0008, Val Loss: 0.0008, Val MAE: 0.0283, Val RMSE: 0.0393, Val SMAPE: 19.93%\n",
      "Epoch [83/200], Train Loss: 0.0009, Val Loss: 0.0008, Val MAE: 0.0290, Val RMSE: 0.0390, Val SMAPE: 19.29%\n",
      "Epoch [84/200], Train Loss: 0.0008, Val Loss: 0.0007, Val MAE: 0.0268, Val RMSE: 0.0366, Val SMAPE: 15.68%\n",
      "Epoch [85/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0257, Val RMSE: 0.0355, Val SMAPE: 14.86%\n",
      "Epoch [86/200], Train Loss: 0.0008, Val Loss: 0.0009, Val MAE: 0.0321, Val RMSE: 0.0413, Val SMAPE: 18.73%\n",
      "Epoch [87/200], Train Loss: 0.0008, Val Loss: 0.0014, Val MAE: 0.0425, Val RMSE: 0.0531, Val SMAPE: 28.00%\n",
      "Epoch [88/200], Train Loss: 0.0010, Val Loss: 0.0007, Val MAE: 0.0257, Val RMSE: 0.0362, Val SMAPE: 14.48%\n",
      "Epoch [89/200], Train Loss: 0.0008, Val Loss: 0.0010, Val MAE: 0.0340, Val RMSE: 0.0447, Val SMAPE: 18.85%\n",
      "Epoch [90/200], Train Loss: 0.0009, Val Loss: 0.0010, Val MAE: 0.0345, Val RMSE: 0.0444, Val SMAPE: 25.86%\n",
      "Epoch [91/200], Train Loss: 0.0008, Val Loss: 0.0006, Val MAE: 0.0250, Val RMSE: 0.0353, Val SMAPE: 14.31%\n",
      "Epoch [92/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0249, Val RMSE: 0.0344, Val SMAPE: 14.93%\n",
      "Epoch [93/200], Train Loss: 0.0008, Val Loss: 0.0006, Val MAE: 0.0246, Val RMSE: 0.0343, Val SMAPE: 13.52%\n",
      "Epoch [94/200], Train Loss: 0.0010, Val Loss: 0.0008, Val MAE: 0.0277, Val RMSE: 0.0392, Val SMAPE: 14.43%\n",
      "Epoch [95/200], Train Loss: 0.0009, Val Loss: 0.0008, Val MAE: 0.0288, Val RMSE: 0.0389, Val SMAPE: 19.21%\n",
      "Epoch [96/200], Train Loss: 0.0009, Val Loss: 0.0006, Val MAE: 0.0244, Val RMSE: 0.0344, Val SMAPE: 16.05%\n",
      "Epoch [97/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0247, Val RMSE: 0.0346, Val SMAPE: 14.20%\n",
      "Epoch [98/200], Train Loss: 0.0010, Val Loss: 0.0009, Val MAE: 0.0328, Val RMSE: 0.0420, Val SMAPE: 19.04%\n",
      "Epoch [99/200], Train Loss: 0.0009, Val Loss: 0.0008, Val MAE: 0.0281, Val RMSE: 0.0385, Val SMAPE: 16.30%\n",
      "Epoch [100/200], Train Loss: 0.0007, Val Loss: 0.0007, Val MAE: 0.0275, Val RMSE: 0.0381, Val SMAPE: 14.13%\n",
      "Epoch [101/200], Train Loss: 0.0007, Val Loss: 0.0007, Val MAE: 0.0275, Val RMSE: 0.0371, Val SMAPE: 15.46%\n",
      "Epoch [102/200], Train Loss: 0.0007, Val Loss: 0.0007, Val MAE: 0.0268, Val RMSE: 0.0373, Val SMAPE: 13.66%\n",
      "Epoch [103/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0244, Val RMSE: 0.0343, Val SMAPE: 14.85%\n",
      "Epoch [104/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0260, Val RMSE: 0.0356, Val SMAPE: 14.85%\n",
      "Epoch [105/200], Train Loss: 0.0007, Val Loss: 0.0007, Val MAE: 0.0277, Val RMSE: 0.0375, Val SMAPE: 16.43%\n",
      "Epoch [106/200], Train Loss: 0.0007, Val Loss: 0.0009, Val MAE: 0.0326, Val RMSE: 0.0423, Val SMAPE: 21.79%\n",
      "Epoch [107/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0240, Val RMSE: 0.0336, Val SMAPE: 13.78%\n",
      "Epoch [108/200], Train Loss: 0.0007, Val Loss: 0.0007, Val MAE: 0.0265, Val RMSE: 0.0370, Val SMAPE: 14.96%\n",
      "Epoch [109/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0244, Val RMSE: 0.0337, Val SMAPE: 14.18%\n",
      "Epoch [110/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0243, Val RMSE: 0.0334, Val SMAPE: 15.11%\n",
      "Epoch [111/200], Train Loss: 0.0006, Val Loss: 0.0006, Val MAE: 0.0247, Val RMSE: 0.0342, Val SMAPE: 14.81%\n",
      "Epoch [112/200], Train Loss: 0.0006, Val Loss: 0.0006, Val MAE: 0.0250, Val RMSE: 0.0346, Val SMAPE: 14.04%\n",
      "Epoch [113/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0228, Val RMSE: 0.0321, Val SMAPE: 13.90%\n",
      "Epoch [114/200], Train Loss: 0.0007, Val Loss: 0.0006, Val MAE: 0.0242, Val RMSE: 0.0334, Val SMAPE: 15.94%\n",
      "Epoch [115/200], Train Loss: 0.0007, Val Loss: 0.0007, Val MAE: 0.0285, Val RMSE: 0.0377, Val SMAPE: 16.10%\n",
      "Epoch [116/200], Train Loss: 0.0006, Val Loss: 0.0006, Val MAE: 0.0249, Val RMSE: 0.0355, Val SMAPE: 13.57%\n",
      "Epoch [117/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0227, Val RMSE: 0.0323, Val SMAPE: 12.96%\n",
      "Epoch [118/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0225, Val RMSE: 0.0315, Val SMAPE: 12.83%\n",
      "Epoch [119/200], Train Loss: 0.0005, Val Loss: 0.0006, Val MAE: 0.0242, Val RMSE: 0.0331, Val SMAPE: 13.41%\n",
      "Epoch [120/200], Train Loss: 0.0006, Val Loss: 0.0007, Val MAE: 0.0273, Val RMSE: 0.0380, Val SMAPE: 13.54%\n",
      "Epoch [121/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0224, Val RMSE: 0.0312, Val SMAPE: 14.68%\n",
      "Epoch [122/200], Train Loss: 0.0006, Val Loss: 0.0006, Val MAE: 0.0244, Val RMSE: 0.0333, Val SMAPE: 14.93%\n",
      "Epoch [123/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0222, Val RMSE: 0.0310, Val SMAPE: 14.12%\n",
      "Epoch [124/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0225, Val RMSE: 0.0315, Val SMAPE: 13.80%\n",
      "Epoch [125/200], Train Loss: 0.0006, Val Loss: 0.0006, Val MAE: 0.0249, Val RMSE: 0.0344, Val SMAPE: 13.87%\n",
      "Epoch [126/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0222, Val RMSE: 0.0312, Val SMAPE: 12.65%\n",
      "Epoch [127/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0217, Val RMSE: 0.0302, Val SMAPE: 12.81%\n",
      "Epoch [128/200], Train Loss: 0.0006, Val Loss: 0.0006, Val MAE: 0.0255, Val RMSE: 0.0346, Val SMAPE: 14.12%\n",
      "Epoch [129/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0215, Val RMSE: 0.0308, Val SMAPE: 11.90%\n",
      "Epoch [130/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0234, Val RMSE: 0.0318, Val SMAPE: 14.22%\n",
      "Epoch [131/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0240, Val RMSE: 0.0319, Val SMAPE: 14.80%\n",
      "Epoch [132/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0227, Val RMSE: 0.0322, Val SMAPE: 13.52%\n",
      "Epoch [133/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0226, Val RMSE: 0.0314, Val SMAPE: 14.49%\n",
      "Epoch [134/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0211, Val RMSE: 0.0298, Val SMAPE: 12.38%\n",
      "Epoch [135/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0223, Val RMSE: 0.0305, Val SMAPE: 16.45%\n",
      "Epoch [136/200], Train Loss: 0.0006, Val Loss: 0.0005, Val MAE: 0.0222, Val RMSE: 0.0307, Val SMAPE: 13.20%\n",
      "Epoch [137/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0207, Val RMSE: 0.0291, Val SMAPE: 12.09%\n",
      "Epoch [138/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0213, Val RMSE: 0.0296, Val SMAPE: 12.53%\n",
      "Epoch [139/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0206, Val RMSE: 0.0292, Val SMAPE: 11.99%\n",
      "Epoch [140/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0199, Val RMSE: 0.0282, Val SMAPE: 11.48%\n",
      "Epoch [141/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0200, Val RMSE: 0.0281, Val SMAPE: 11.49%\n",
      "Epoch [142/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0208, Val RMSE: 0.0290, Val SMAPE: 12.46%\n",
      "Epoch [143/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0205, Val RMSE: 0.0289, Val SMAPE: 12.13%\n",
      "Epoch [144/200], Train Loss: 0.0005, Val Loss: 0.0005, Val MAE: 0.0230, Val RMSE: 0.0316, Val SMAPE: 13.19%\n",
      "Epoch [145/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0196, Val RMSE: 0.0278, Val SMAPE: 11.55%\n",
      "Epoch [146/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0208, Val RMSE: 0.0288, Val SMAPE: 14.33%\n",
      "Epoch [147/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0204, Val RMSE: 0.0284, Val SMAPE: 13.43%\n",
      "Epoch [148/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0199, Val RMSE: 0.0281, Val SMAPE: 11.74%\n",
      "Epoch [149/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0192, Val RMSE: 0.0270, Val SMAPE: 11.25%\n",
      "Epoch [150/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0200, Val RMSE: 0.0279, Val SMAPE: 11.58%\n",
      "Epoch [151/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0212, Val RMSE: 0.0293, Val SMAPE: 12.56%\n",
      "Epoch [152/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0201, Val RMSE: 0.0282, Val SMAPE: 11.78%\n",
      "Epoch [153/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0192, Val RMSE: 0.0274, Val SMAPE: 11.22%\n",
      "Epoch [154/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0193, Val RMSE: 0.0272, Val SMAPE: 11.50%\n",
      "Epoch [155/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0197, Val RMSE: 0.0278, Val SMAPE: 11.61%\n",
      "Epoch [156/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0209, Val RMSE: 0.0286, Val SMAPE: 13.18%\n",
      "Epoch [157/200], Train Loss: 0.0005, Val Loss: 0.0004, Val MAE: 0.0195, Val RMSE: 0.0273, Val SMAPE: 11.45%\n",
      "Epoch [158/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0193, Val RMSE: 0.0272, Val SMAPE: 11.91%\n",
      "Epoch [159/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0193, Val RMSE: 0.0271, Val SMAPE: 11.50%\n",
      "Epoch [160/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0193, Val RMSE: 0.0273, Val SMAPE: 11.54%\n",
      "Epoch [161/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0190, Val RMSE: 0.0268, Val SMAPE: 11.56%\n",
      "Epoch [162/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0195, Val RMSE: 0.0272, Val SMAPE: 11.58%\n",
      "Epoch [163/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0192, Val RMSE: 0.0272, Val SMAPE: 11.56%\n",
      "Epoch [164/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0192, Val RMSE: 0.0268, Val SMAPE: 11.95%\n",
      "Epoch [165/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0189, Val RMSE: 0.0266, Val SMAPE: 11.05%\n",
      "Epoch [166/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0190, Val RMSE: 0.0270, Val SMAPE: 11.08%\n",
      "Epoch [167/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0201, Val RMSE: 0.0274, Val SMAPE: 13.75%\n",
      "Epoch [168/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0195, Val RMSE: 0.0275, Val SMAPE: 11.28%\n",
      "Epoch [169/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0189, Val RMSE: 0.0265, Val SMAPE: 11.16%\n",
      "Epoch [170/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0189, Val RMSE: 0.0267, Val SMAPE: 10.97%\n",
      "Epoch [171/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0198, Val RMSE: 0.0277, Val SMAPE: 11.55%\n",
      "Epoch [172/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0193, Val RMSE: 0.0270, Val SMAPE: 11.57%\n",
      "Epoch [173/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0192, Val RMSE: 0.0269, Val SMAPE: 11.27%\n",
      "Epoch [174/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0187, Val RMSE: 0.0264, Val SMAPE: 10.92%\n",
      "Epoch [175/200], Train Loss: 0.0004, Val Loss: 0.0004, Val MAE: 0.0188, Val RMSE: 0.0264, Val SMAPE: 11.02%\n",
      "Epoch [176/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0186, Val RMSE: 0.0263, Val SMAPE: 10.89%\n",
      "Epoch [177/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0186, Val RMSE: 0.0262, Val SMAPE: 10.81%\n",
      "Epoch [178/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0187, Val RMSE: 0.0263, Val SMAPE: 10.87%\n",
      "Epoch [179/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0185, Val RMSE: 0.0261, Val SMAPE: 10.78%\n",
      "Epoch [180/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0185, Val RMSE: 0.0261, Val SMAPE: 10.82%\n",
      "Epoch [181/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0185, Val RMSE: 0.0261, Val SMAPE: 10.75%\n",
      "Epoch [182/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0184, Val RMSE: 0.0260, Val SMAPE: 10.71%\n",
      "Epoch [183/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0184, Val RMSE: 0.0261, Val SMAPE: 10.72%\n",
      "Epoch [184/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0184, Val RMSE: 0.0260, Val SMAPE: 10.71%\n",
      "Epoch [185/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.73%\n",
      "Epoch [186/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.73%\n",
      "Epoch [187/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.77%\n",
      "Epoch [188/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0184, Val RMSE: 0.0260, Val SMAPE: 10.81%\n",
      "Epoch [189/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.71%\n",
      "Epoch [190/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.73%\n",
      "Epoch [191/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.75%\n",
      "Epoch [192/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.75%\n",
      "Epoch [193/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Epoch [194/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Epoch [195/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Epoch [196/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Epoch [197/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Epoch [198/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Epoch [199/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Epoch [200/200], Train Loss: 0.0004, Val Loss: 0.0003, Val MAE: 0.0183, Val RMSE: 0.0260, Val SMAPE: 10.74%\n",
      "Final Model - MAE: 0.0183, RMSE: 0.0260, SMAPE: 10.74%\n"
     ]
    }
   ],
   "source": [
    "file_path = r'C:\\Users\\Administrator\\Desktop\\DSP391m_cryptocurrency-price-prediction\\data\\dataset\\processed_dataset.csv'\n",
    "sequence_length = 30\n",
    "prediction_window = 7\n",
    "step = 1\n",
    "batch_size = 64\n",
    "\n",
    "X, y, scaler_features, scaler_target = prepare_data(file_path, sequence_length, prediction_window, step)\n",
    "\n",
    "# Optimize hyperparameters and train the best model\n",
    "best_params = optimize_hyperparameters()\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Ensure d_model and nhead are appropriate\n",
    "d_model = int(best_params['params']['d_model'])\n",
    "nhead = int(best_params['params']['nhead'])\n",
    "d_model = ((d_model + 1) // 2) * 2  # Make d_model even\n",
    "nhead = nhead if nhead % 2 == 0 else nhead + 1\n",
    "d_model = (d_model // nhead) * nhead\n",
    "\n",
    "best_model = TimeSeriesTransformer(input_dim=X.shape[2], \n",
    "                                   d_model=d_model,\n",
    "                                   nhead=nhead,\n",
    "                                   num_layers=int(best_params['params']['num_layers']),\n",
    "                                   dropout=best_params['params']['dropout'],\n",
    "                                   output_dim=prediction_window)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "train_model(best_model, train_loader, val_loader, scaler_target, \n",
    "            epochs=200, lr=best_params['params']['learning_rate'], patience=20)\n",
    "\n",
    "# Evaluate the model\n",
    "_, mae, rmse, smape_value = evaluate_model(best_model, val_loader, scaler_target, nn.HuberLoss())\n",
    "print(f\"Final Model - MAE: {mae:.4f}, RMSE: {rmse:.4f}, SMAPE: {smape_value:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open: -0.0939\n",
      "Close_rolling_14: -0.0535\n",
      "Ensemble_Sentiment: -0.0242\n",
      "Close_rolling_7: -0.0226\n",
      "Sentiment_Volatility: -0.0058\n",
      "RSI: 0.0066\n",
      "MA(25): 0.0149\n",
      "Sentiment_Change: 0.0236\n",
      "Sentiment_7day_MA: 0.0472\n",
      "MA(7): 0.0547\n",
      "Vol(USDT): 0.0696\n",
      "High: 0.0988\n",
      "Low: 0.1017\n",
      "Close_lag_7: 0.1019\n",
      "Close_lag_14: 0.1209\n",
      "MA(99): 0.1601\n",
      "Some predictions: [[0.81795543 0.8146064  0.81142396 0.81018656 0.8092912  0.8116218\n",
      "  0.8141206 ]\n",
      " [0.81979936 0.81687284 0.8135631  0.8125992  0.81216884 0.81416976\n",
      "  0.8180412 ]\n",
      " [0.8203413  0.8183392  0.81593317 0.8152982  0.81543887 0.817348\n",
      "  0.82122004]\n",
      " [0.8180528  0.817024   0.81574833 0.8152231  0.81580603 0.8178921\n",
      "  0.82131654]\n",
      " [0.80498576 0.80503404 0.80433446 0.80431765 0.8049242  0.8072448\n",
      "  0.8105135 ]\n",
      " [0.81217927 0.81156325 0.8107412  0.8099718  0.81029785 0.8124312\n",
      "  0.81523544]\n",
      " [0.809506   0.80933595 0.80832505 0.8076197  0.8082373  0.8102116\n",
      "  0.8128418 ]]\n",
      "Corresponding actual prices: [[0.82785463 0.77694434 0.80334824 0.786923   0.80547756 0.8017237\n",
      "  0.80161077]\n",
      " [0.77694434 0.80334824 0.786923   0.80547756 0.8017237  0.80161077\n",
      "  0.77929264]\n",
      " [0.80334824 0.786923   0.80547756 0.8017237  0.80161077 0.77929264\n",
      "  0.78904706]\n",
      " [0.786923   0.80547756 0.8017237  0.80161077 0.77929264 0.78904706\n",
      "  0.8202095 ]\n",
      " [0.80547756 0.8017237  0.80161077 0.77929264 0.78904706 0.8202095\n",
      "  0.8202095 ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Close_lag_7', 'Close_lag_14',\n",
    "            'Close_rolling_7', 'Close_rolling_14', 'MA(7)', 'MA(25)', 'MA(99)',\n",
    "            'RSI', 'Vol(USDT)', 'Ensemble_Sentiment', 'Sentiment_7day_MA',\n",
    "            'Sentiment_Change', 'Sentiment_Volatility']\n",
    "\n",
    "\n",
    "def interpret_model(model, X, y, features):\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert tensors to numpy arrays\n",
    "    X_np = X.numpy()\n",
    "    y_np = y.numpy()\n",
    "    \n",
    "    # Calculate baseline performance\n",
    "    with torch.no_grad():\n",
    "        baseline_preds = model(X).numpy()\n",
    "    baseline_mae = mean_absolute_error(y_np, baseline_preds)\n",
    "    \n",
    "    # Calculate feature importance\n",
    "    feature_importance = []\n",
    "    for i in range(X.shape[2]):  # Iterate over features\n",
    "        X_permuted = X_np.copy()\n",
    "        X_permuted[:, :, i] = np.random.permutation(X_permuted[:, :, i])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            permuted_preds = model(torch.FloatTensor(X_permuted)).numpy()\n",
    "        permuted_mae = mean_absolute_error(y_np, permuted_preds)\n",
    "        \n",
    "        importance = permuted_mae - baseline_mae\n",
    "        feature_importance.append(importance)\n",
    "    \n",
    "    # Normalize feature importance\n",
    "    feature_importance = np.array(feature_importance)\n",
    "    feature_importance = feature_importance / np.sum(np.abs(feature_importance))\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    sorted_features = [features[i] for i in sorted_idx]\n",
    "    sorted_importance = feature_importance[sorted_idx]\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(sorted_features)), sorted_importance)\n",
    "    plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "    plt.xlabel('Normalized Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print feature importance\n",
    "    for feature, importance in zip(sorted_features, sorted_importance):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "interpret_model(best_model, X, y, features)\n",
    "\n",
    "# Make predictions\n",
    "sample_X = X[-7:] \n",
    "mask = create_attention_mask(sample_X.size(1)).to(sample_X.device)\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(sample_X, mask)\n",
    "\n",
    "original_predictions = scaler_target.inverse_transform(predictions.numpy())\n",
    "original_y = scaler_target.inverse_transform(y[-5:].numpy())\n",
    "\n",
    "print(\"Some predictions:\", original_predictions)\n",
    "print(\"Corresponding actual prices:\", original_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_model(model, directory, model_name, val_loader, scaler_target):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    _, mae, rmse, smape_value = evaluate_model(model, val_loader, scaler_target, nn.HuberLoss())\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': float(mae),\n",
    "        'RMSE': float(rmse),\n",
    "        'SMAPE': float(smape_value)\n",
    "    }\n",
    "\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    metrics_str = '_'.join([f\"{k}_{v:.4f}\" for k, v in metrics.items()])\n",
    "    filename = f\"{model_name}_{current_date}_{metrics_str}.pth\"\n",
    "\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "\n",
    "    model_info = {\n",
    "        'input_dim': model.embedding.in_features,\n",
    "        'd_model': model.embedding.out_features,\n",
    "        'nhead': model.transformer_encoder.layers[0].self_attn.num_heads,\n",
    "        'num_layers': len(model.transformer_encoder.layers),\n",
    "        'dropout': float(model.transformer_encoder.layers[0].dropout.p),  # Chuyn i sang float\n",
    "        'output_dim': model.decoder.out_features,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "    info_filepath = filepath.replace('.pth', '_info.json')\n",
    "    with open(info_filepath, 'w') as f:\n",
    "        json.dump(model_info, f, indent=4)\n",
    "\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "    print(f\"Model info saved to {info_filepath}\")\n",
    "    print(f\"Final Model - MAE: {metrics['MAE']:.4f}, RMSE: {metrics['RMSE']:.4f}, SMAPE: {metrics['SMAPE']:.2f}%\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def save_scalers(scaler_features, scaler_target, directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(scaler_features, os.path.join(directory, 'scaler_features.joblib'))\n",
    "    joblib.dump(scaler_target, os.path.join(directory, 'scaler_target.joblib'))\n",
    "    \n",
    "    print(f\"Scalers saved to {directory}\")\n",
    "\n",
    "def load_scalers(directory):\n",
    "    scaler_features = joblib.load(os.path.join(directory, 'scaler_features.joblib'))\n",
    "    scaler_target = joblib.load(os.path.join(directory, 'scaler_target.joblib'))\n",
    "    \n",
    "    print(f\"Scalers loaded from {directory}\")\n",
    "    return scaler_features, scaler_target\n",
    "\n",
    "def load_model(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    info_filepath = filepath.replace('.pth', '_info.json')\n",
    "    \n",
    "    with open(info_filepath, 'r') as f:\n",
    "        model_info = json.load(f)\n",
    "    \n",
    "    # Khi to m hnh vi cc siu tham s  lu\n",
    "    model = TimeSeriesTransformer(\n",
    "        input_dim=model_info['input_dim'],\n",
    "        d_model=model_info['d_model'],\n",
    "        nhead=model_info['nhead'],\n",
    "        num_layers=model_info['num_layers'],\n",
    "        dropout=model_info['dropout'],\n",
    "        output_dim=model_info['output_dim']\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    \n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    print(f\"Model metrics: {model_info['metrics']}\")\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\TimeSeriesTransformer_20240717_MAE_0.0183_RMSE_0.0260_SMAPE_10.7383.pth\n",
      "Model info saved to models\\TimeSeriesTransformer_20240717_MAE_0.0183_RMSE_0.0260_SMAPE_10.7383_info.json\n",
      "Final Model - MAE: 0.0183, RMSE: 0.0260, SMAPE: 10.74%\n",
      "Scalers saved to models\n",
      "Scalers loaded from models\n",
      "Model loaded from models\\TimeSeriesTransformer_20240717_MAE_0.0183_RMSE_0.0260_SMAPE_10.7383.pth\n",
      "Model metrics: {'MAE': 0.018323447555303574, 'RMSE': 0.025956351310014725, 'SMAPE': 10.738314688205719}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "saved_filename = save_model(best_model, directory='models', model_name='TimeSeriesTransformer', val_loader=val_loader, scaler_target=scaler_target)\n",
    "save_scalers(scaler_features, scaler_target, directory='models')\n",
    "\n",
    "scaler_features, scaler_target = load_scalers(directory='models')\n",
    "loaded_model = load_model(directory='models', filename=saved_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
